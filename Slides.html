<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="generator" content="pandoc" />

        <meta name="author" content="Cesar Ojeda/Omar Rios – Univalle" />
    
    
    <title>Regresión de series de tiempo</title>

        <script src="Slides_files/header-attrs-2.30/header-attrs.js"></script>
        <script src="Slides_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link href="Slides_files/bootstrap-3.3.7/css/bootstrap.min.css" rel="stylesheet" />
        <script src="Slides_files/bootstrap-3.3.7/js/bootstrap.min.js"></script>
        <script src="Slides_files/navigation-1.1/tabsets.js"></script>
        <script src="Slides_files/navigation-1.1/codefolding.js"></script>
        <link href="Slides_files/readthedown-0.1/readthedown.css" rel="stylesheet" />
        <link href="Slides_files/readthedown-0.1/readthedown_fonts_embed.css" rel="stylesheet" />
        <script src="Slides_files/readthedown-0.1/readthedown.js"></script>
    
    
        <style type="text/css">code{white-space: pre;}</style>
    <style type="text/css">
      html { -webkit-text-size-adjust: 100%; }
      pre > code.sourceCode { white-space: pre; position: relative; }
      pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
      pre > code.sourceCode > span:empty { height: 1.2em; }
      .sourceCode { overflow: visible; }
      code.sourceCode > span { color: inherit; text-decoration: inherit; }
      div.sourceCode { margin: 1em 0; }
      pre.sourceCode { margin: 0; }
      @media screen {
      div.sourceCode { overflow: auto; }
      }
      @media print {
      pre > code.sourceCode { white-space: pre-wrap; }
      pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
      }
      pre.numberSource code
        { counter-reset: source-line 0; }
      pre.numberSource code > span
        { position: relative; left: -4em; counter-increment: source-line; }
      pre.numberSource code > span > a:first-child::before
        { content: counter(source-line);
          position: relative; left: -1em; text-align: right; vertical-align: baseline;
          border: none; display: inline-block;
          -webkit-touch-callout: none; -webkit-user-select: none;
          -khtml-user-select: none; -moz-user-select: none;
          -ms-user-select: none; user-select: none;
          padding: 0 4px; width: 4em;
          background-color: #ffffff;
          color: #a0a0a0;
        }
      pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
      div.sourceCode
        { color: #1f1c1b; background-color: #ffffff; }
      @media screen {
      pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
      }
      code span { color: #1f1c1b; } /* Normal */
      code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
      code span.an { color: #ca60ca; } /* Annotation */
      code span.at { color: #0057ae; } /* Attribute */
      code span.bn { color: #b08000; } /* BaseN */
      code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
      code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
      code span.ch { color: #924c9d; } /* Char */
      code span.cn { color: #aa5500; } /* Constant */
      code span.co { color: #898887; } /* Comment */
      code span.cv { color: #0095ff; } /* CommentVar */
      code span.do { color: #607880; } /* Documentation */
      code span.dt { color: #0057ae; } /* DataType */
      code span.dv { color: #b08000; } /* DecVal */
      code span.er { color: #bf0303; text-decoration: underline; } /* Error */
      code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
      code span.fl { color: #b08000; } /* Float */
      code span.fu { color: #644a9b; } /* Function */
      code span.im { color: #ff5500; } /* Import */
      code span.in { color: #b08000; } /* Information */
      code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
      code span.op { color: #1f1c1b; } /* Operator */
      code span.ot { color: #006e28; } /* Other */
      code span.pp { color: #006e28; } /* Preprocessor */
      code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
      code span.sc { color: #3daee9; } /* SpecialChar */
      code span.ss { color: #ff5500; } /* SpecialString */
      code span.st { color: #bf0303; } /* String */
      code span.va { color: #0057ae; } /* Variable */
      code span.vs { color: #bf0303; } /* VerbatimString */
      code span.wa { color: #bf0303; } /* Warning */
    </style>
    
    
    <!-- tabsets -->
    <script>
      $(document).ready(function () {
	  window.buildTabsets("toc");
      });
      $(document).ready(function () {
	  $('.tabset-dropdown > .nav-tabs > li').click(function () {
	      $(this).parent().toggleClass('nav-tabs-open')
	  });
      });
    </script>

    <!-- code folding -->
        <script>
      $(document).ready(function () {
	  	  	  window.initializeCodeFolding("hide" === "show");
	        });
    </script>
    
    <!-- code download -->
    
    <!-- tabsets dropdown -->

    <style type="text/css">
      .tabset-dropdown > .nav-tabs {
	  display: inline-table;
	  max-height: 500px;
	  min-height: 44px;
	  overflow-y: auto;
	  background: white;
	  border: 1px solid #ddd;
	  border-radius: 4px;
      }
      
      .tabset-dropdown > .nav-tabs > li.active:before {
	  content: "";
	  font-family: 'Glyphicons Halflings';
	  display: inline-block;
	  padding: 10px;
	  border-right: 1px solid #ddd;
      }
      
      .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
	  content: "&#xe258;";
	  border: none;
      }
      
      .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
	  content: "";
	  font-family: 'Glyphicons Halflings';
	  display: inline-block;
	  padding: 10px;
	  border-right: 1px solid #ddd;
      }
      
      .tabset-dropdown > .nav-tabs > li.active {
	  display: block;
      }

      .tabset-dropdown > .nav-tabs > li.active a {
  	  padding: 0 15px !important;
      }

      .tabset-dropdown > .nav-tabs > li > a,
      .tabset-dropdown > .nav-tabs > li > a:focus,
      .tabset-dropdown > .nav-tabs > li > a:hover {
	  border: none;
	  display: inline-block;
	  border-radius: 4px;
	  background-color: transparent;
      }
      
      .tabset-dropdown > .nav-tabs.nav-tabs-open > li {
	  display: block;
	  float: none;
      }
      
      .tabset-dropdown > .nav-tabs > li {
	  display: none;
	  margin-left: 0 !important;
      }
    </style>
    
</head>

<body class="preload">

   	
         <!-- readthedown start -->   
   <div id="content" data-toggle="wy-nav-shift">
     <nav id="nav-top" role="navigation" aria-label="top navigation">
       <a role="button" href="#" data-toggle="wy-nav-top"><span class="glyphicon glyphicon-menu-hamburger"></span></a>
     </nav>
         
      <div class="btn-group pull-right">
     <button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
     <ul class="dropdown-menu" style="min-width: 50px;">
	    	    <li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
	    <li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
	    	    	         </ul>
   </div>
   
        
      <h1 class="title">Regresión de series de tiempo</h1>
      
         <!-- readthedown authors -->
   <div id="sidebar">
    <h2><a href="#content">Regresión de series de tiempo</a></h2>
    <div id="toc">
      <ul>
      <li><a href="#course-outline" id="toc-course-outline">Course
      Outline</a>
      <ul>
      <li><a href="#text" id="toc-text">Text</a></li>
      <li><a href="#others" id="toc-others">Others</a></li>
      <li><a href="#statistical-software"
      id="toc-statistical-software">Statistical Software</a></li>
      <li><a href="#assessment" id="toc-assessment">Assessment</a></li>
      <li><a href="#schedule" id="toc-schedule">Schedule</a></li>
      </ul></li>
      <li><a href="#characteristics-of-time-series"
      id="toc-characteristics-of-time-series">Characteristics of time
      series</a>
      <ul>
      <li><a href="#approaches" id="toc-approaches">Approaches</a></li>
      <li><a href="#examples" id="toc-examples">Examples</a></li>
      <li><a href="#time-series-statistical-models"
      id="toc-time-series-statistical-models">Time Series Statistical
      Models</a></li>
      <li><a href="#measures-of-dependence"
      id="toc-measures-of-dependence">Measures of Dependence</a></li>
      <li><a href="#stationary-time-series"
      id="toc-stationary-time-series">Stationary Time Series</a></li>
      <li><a href="#estimation-of-correlation"
      id="toc-estimation-of-correlation">Estimation of
      Correlation</a></li>
      </ul></li>
      <li><a
      href="#time-series-regression-and-exploratory-data-analysis"
      id="toc-time-series-regression-and-exploratory-data-analysis">Time
      series regression and exploratory data analysis</a>
      <ul>
      <li><a href="#classical-regression-in-the-time-series-context"
      id="toc-classical-regression-in-the-time-series-context">Classical
      Regression in the Time Series Context</a></li>
      <li><a href="#exploratory-data-analysis"
      id="toc-exploratory-data-analysis">Exploratory Data
      Analysis</a></li>
      <li><a href="#smoothing-in-the-time-series-context"
      id="toc-smoothing-in-the-time-series-context">Smoothing in the
      Time Series Context</a></li>
      </ul></li>
      <li><a href="#arima-models" id="toc-arima-models">ARIMA Models</a>
      <ul>
      <li><a href="#autoregressive-models"
      id="toc-autoregressive-models">Autoregressive Models</a></li>
      <li><a href="#moving-average-models"
      id="toc-moving-average-models">Moving Average Models</a></li>
      <li><a href="#autoregressive-moving-average-models"
      id="toc-autoregressive-moving-average-models">Autoregressive
      Moving Average Models</a></li>
      <li><a href="#autocorrelation-and-partial-autocorrelation"
      id="toc-autocorrelation-and-partial-autocorrelation">Autocorrelation
      and Partial Autocorrelation</a></li>
      <li><a href="#forecasting"
      id="toc-forecasting">Forecasting</a></li>
      </ul></li>
      </ul>
    </div>
    <div id="postamble" data-toggle="wy-nav-shift" class="status">
                  <p class="author"><span class="glyphicon glyphicon-user"></span> Cesar
Ojeda/Omar Rios – Univalle</p>
                        <p class="date"><span class="glyphicon glyphicon-calendar"></span> Semester
I, 2026</p>
          </div>
   </div>
     

   
      
   
<!-- Don't indent these lines or it will mess pre blocks indentation --> 
<div id="main">
<div id="course-outline" class="section level1">
<h1>Course Outline</h1>
<ul>
<li>The goal is to learn modern time series models on either time or
frequency domain. Students shall be able to understand the underlying
mathematical structure as well as to make practical applications of
these models.</li>
</ul>
<div id="text" class="section level2">
<h2>Text</h2>
<ul>
<li>Shumway, R. H. and Stoffer, D. S. (2017), Time Series Analysis and
its Applications: With R Examples (Fourth ed.), Springer.</li>
</ul>
</div>
<div id="others" class="section level2">
<h2>Others</h2>
<ul>
<li>Box, G. E. P., Jenkins, G. M., Reinsel, G. C., and Ljung, G. M.
(2016), Time Series Analysis: Forecasting and Control (Fifth ed.),
Wiley.</li>
<li>Brockwell, P. J. and Davis, R. A. (1991), Time Series: Theory and
Methods (Second ed.), Springer.</li>
<li>Wei, W. W. S. (2006), Time Series Analysis: Univariate and
Multivariate Methods (Second ed.), Pearson Addison Wesley.</li>
<li>Cryer, J. D. and Chan, K.-S. (2008), Time Series Analysis with
Applications in R (Second ed.), Springer.</li>
<li>Palma, W. (2016), Time Series Analysis, Wiley.</li>
</ul>
</div>
<div id="statistical-software" class="section level2">
<h2>Statistical Software</h2>
<ul>
<li>R, <a href="https://www.r-project.org/"
class="uri">https://www.r-project.org/</a>.</li>
<li>Time Series Modelling (TSM), <a
href="http://www.timeseriesmodelling.com/"
class="uri">http://www.timeseriesmodelling.com/</a>.</li>
<li>Scientific Computing Associates (SCA), <a
href="http://www.scausa.com/"
class="uri">http://www.scausa.com/</a>.</li>
</ul>
</div>
<div id="assessment" class="section level2">
<h2>Assessment</h2>
<ul>
<li>Midterm Exam (40%).</li>
<li>Presentation I (15%).</li>
<li>Presentations II (15%).</li>
<li>Project (30%).</li>
</ul>
</div>
<div id="schedule" class="section level2">
<h2>Schedule</h2>
<table>
<thead>
<tr class="header">
<th align="left">Fecha</th>
<th align="left">Tema</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Vie Feb 6</td>
<td align="left">Characteristics of time series</td>
</tr>
<tr class="even">
<td align="left">Sab Feb 7</td>
<td align="left">Time series regression and exploratory data
analysis</td>
</tr>
<tr class="odd">
<td align="left">Vie Feb 13</td>
<td align="left">ARIMA models: AR, MA models</td>
</tr>
<tr class="even">
<td align="left">Sáb Feb 14</td>
<td align="left">ARIMA models: ARMA, ARIMA models</td>
</tr>
<tr class="odd">
<td align="left">Vie Feb 20</td>
<td align="left">ARIMA models: Estimation and forecasting</td>
</tr>
<tr class="even">
<td align="left">Sab Feb 21</td>
<td align="left">ARIMA models: Estimation and forecasting</td>
</tr>
<tr class="odd">
<td align="left">Sab Feb 21</td>
<td align="left"><strong>Actividad evaluativa 1</strong></td>
</tr>
<tr class="even">
<td align="left">Vie Feb 27</td>
<td align="left">Spectral analysis and filtering</td>
</tr>
<tr class="odd">
<td align="left">Sab Feb 28</td>
<td align="left">Spectral analysis and filtering</td>
</tr>
<tr class="even">
<td align="left">Vie Mar 06</td>
<td align="left">Spectral analysis and filtering</td>
</tr>
<tr class="odd">
<td align="left">Sab Mar 07</td>
<td align="left">State Space Models</td>
</tr>
<tr class="even">
<td align="left">Vie Mar 13</td>
<td align="left">State Space Models</td>
</tr>
<tr class="odd">
<td align="left">Sab Mar 14</td>
<td align="left">State Space Models</td>
</tr>
<tr class="even">
<td align="left">Sab Mar 14</td>
<td align="left"><strong>Actividad evaluativa 2</strong></td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="10%" />
<col width="89%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Jan 24</td>
<td align="left">Characteristics of time series</td>
</tr>
<tr class="even">
<td align="left">Jan 31</td>
<td align="left">Time series regression and exploratory data
analysis</td>
</tr>
<tr class="odd">
<td align="left">Feb 07</td>
<td align="left">ARIMA models</td>
</tr>
<tr class="even">
<td align="left">Feb 14</td>
<td align="left">ARIMA models</td>
</tr>
<tr class="odd">
<td align="left">Feb 21</td>
<td align="left">Spectral analysis and filtering</td>
</tr>
<tr class="even">
<td align="left">Feb 28</td>
<td align="left">Spectral analysis and filtering</td>
</tr>
<tr class="odd">
<td align="left">Mar 06</td>
<td align="left"><strong>Midterm Exam</strong></td>
</tr>
<tr class="even">
<td align="left">Mar 13</td>
<td align="left">Additional time domain topics (<strong>presentation
I</strong>)</td>
</tr>
<tr class="odd">
<td align="left">Mar 20</td>
<td align="left">State space models</td>
</tr>
<tr class="even">
<td align="left">Mar 27</td>
<td align="left">State space models</td>
</tr>
<tr class="odd">
<td align="left">Apr 03</td>
<td align="left">Statistical methods in the frequency domain
(<strong>presentation II</strong>)</td>
</tr>
<tr class="even">
<td align="left">Apr 17</td>
<td align="left"><strong>Projects</strong></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="characteristics-of-time-series" class="section level1">
<h1>Characteristics of time series</h1>
<ul>
<li><p>The first step in any time series investigation always involves
careful examination of the recorded data plotted over time.</p></li>
<li><p>This scrutiny often suggests the method of analysis as well as
statistics that will be of use in summarizing the information in the
data.</p></li>
</ul>
<div id="approaches" class="section level2">
<h2>Approaches</h2>
<ul>
<li><p>Time domain: views the investigation of lagged relationships as
most important (e.g., how does what happened today affect what will
happen tomorrow).</p></li>
<li><p>Frequency domain: views the investigation of cycles as most
important (e.g., what is the economic cycle through periods of expansion
and recession).</p></li>
</ul>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<div id="johnson-johnson-quarterly-earnings" class="section level3">
<h3>Johnson &amp; Johnson Quarterly Earnings</h3>
<ul>
<li>Quarterly earnings per share for the U.S. company Johnson &amp;
Johnson. There are 84 quarters (21 years) measured from the first
quarter of 1960 to the last quarter of 1980.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">tsplot</span>(jj, <span class="at">type=</span><span class="st">&quot;o&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Quarterly Earnings per Share&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-1-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>Modeling such series begins by observing the primary patterns in the
time history. In this case, note
<ul>
<li>the gradually increasing underlying trend and</li>
<li>the rather regular variation superimposed on the trend that seems to
repeat over quarters.</li>
</ul></li>
</ul>
</div>
<div id="global-warming" class="section level3">
<h3>Global Warming</h3>
<ul>
<li>Global mean land–ocean temperature index from 1880 to 2015, with the
base period 1951-1980. In particular, the data are deviations, measured
in degrees centigrade, from the 1951-1980 average.</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="fu">tsplot</span>(gtemp_both, <span class="at">type=</span><span class="st">&quot;o&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Global Temperature Deviations&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>We note</p>
<ul>
<li>an apparent upward trend in the series during the latter part of the
twentieth century that has been used as an argument for the global
warming hypothesis.</li>
<li>Also, the leveling off at about 1935 and then another rather sharp
upward trend at about 1970.</li>
</ul></li>
<li><p>The question of interest for global warming proponents and
opponents is whether the overall trend is natural or whether it is
caused by some human-induced interface.</p></li>
<li><p>Again, the question of trend is of more interest than particular
periodicities.</p></li>
</ul>
</div>
<div id="speech-data" class="section level3">
<h3>Speech Data</h3>
<ul>
<li>A small <span class="math inline">\(.1\)</span> second (1000 point)
sample of recorded speech for the phrase <span
class="math inline">\(aaa\ldots hhh\)</span>.</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">tsplot</span>(speech)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>We note</p>
<ul>
<li>the repetitive nature of the signal and the rather regular
periodicities.</li>
</ul></li>
<li><p>One current problem of great interest is computer recognition of
speech, which would require converting this particular signal into the
recorded phrase <span class="math inline">\(aaa\ldots
hhh\)</span>.</p></li>
<li><p>Spectral analysis can be used in this context to produce a
signature of this phrase that can be compared with signatures of various
library syllables to look for a match.</p></li>
</ul>
</div>
<div id="dow-jones-industrial-average" class="section level3">
<h3>Dow Jones Industrial Average</h3>
<ul>
<li>Daily returns (or percent change) of the Dow Jones Industrial
Average (DJIA) from April 20, 2006 to April 20, 2016.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>djiar <span class="ot">=</span> <span class="fu">diff</span>(<span class="fu">log</span>(djia<span class="sc">$</span>Close))[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="fu">plot</span>(djiar, <span class="at">main=</span><span class="st">&quot;DJIA Returns&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-4-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>Note that
<ul>
<li>it is easy to spot the financial crisis of 2008 and</li>
<li>the mean of the series appears to be stable with an average return
of approximately zero, however, highly volatile (variable) periods tend
to be clustered together.</li>
</ul></li>
<li>A problem in the analysis of these type of financial data is to
forecast the volatility of future returns.</li>
</ul>
</div>
<div id="el-niño-and-fish-population" class="section level3">
<h3>El Niño and Fish Population</h3>
<ul>
<li>Monthly values of an environmental series called the Southern
Oscillation Index (SOI) and associated Recruitment (number of new fish).
Both series are for a period of 453 months ranging over the years
1950–1987.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="fu">tsplot</span>(soi, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Southern Oscillation Index&quot;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="fu">tsplot</span>(rec, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Recruitment&quot;</span>) </span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>Both series exhibit repetitive behavior, with regularly repeating
cycles that are easily visible.</p></li>
<li><p>This periodic behavior is of interest because underlying
processes of interest may be regular and the rate or frequency of
oscillation characterizing the behavior of the underlying series would
help to identify them.</p></li>
<li><p>The two series are also related; it is easy to imagine the fish
population is dependent on the ocean temperature. This possibility
suggests trying some version of regression analysis as a procedure for
relating the two series.</p></li>
</ul>
</div>
<div id="fmri-imaging" class="section level3">
<h3>fMRI Imaging</h3>
<ul>
<li><p>Data collected from various locations in the brain via functional
magnetic resonance imaging (fMRI).</p></li>
<li><p>The stimulus was applied for 32 seconds and then stopped for 32
seconds; thus, the signal period is 64 seconds. The sampling rate was
one observation every 2 seconds for 256 seconds (n = 128).</p></li>
<li><p>The series are consecutive measures of blood oxygenation-level
dependent (bold) signal intensity, which measures areas of activation in
the brain.</p></li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">0</span>)<span class="sc">+</span>.<span class="dv">5</span>, <span class="at">mgp=</span><span class="fu">c</span>(<span class="fl">1.6</span>,.<span class="dv">6</span>,<span class="dv">0</span>))  </span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="fu">ts.plot</span>(fmri1[,<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>], <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">ylab=</span><span class="st">&quot;BOLD&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Cortex&quot;</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="fu">ts.plot</span>(fmri1[,<span class="dv">6</span><span class="sc">:</span><span class="dv">9</span>], <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">ylab=</span><span class="st">&quot;BOLD&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Thalamus &amp; Cerebellum&quot;</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Time (1 pt = 2 sec)&quot;</span>, <span class="at">side=</span><span class="dv">1</span>, <span class="at">line=</span><span class="dv">2</span>) </span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-6-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>Periodicities appear strongly in the motor cortex series and less
strongly in the thalamus and cerebellum.</p></li>
<li><p>The fact that one has series from different areas of the brain
suggests testing whether the areas are responding differently to the
stimulus.</p></li>
</ul>
</div>
<div id="earthquakes-and-explosions" class="section level3">
<h3>Earthquakes and Explosions</h3>
<ul>
<li>The series represent two phases or arrivals along the surface,
denoted by P (<span class="math inline">\(t=1,\ldots , 1024\)</span>)
and S (<span class="math inline">\(t=1025,\ldots , 2048\)</span>), at a
seismic recording station. The recording instruments in Scandinavia are
observing earthquakes and mining explosions.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="fu">tsplot</span>(EQ5, <span class="at">main=</span><span class="st">&quot;Earthquake&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="fu">tsplot</span>(EXP6, <span class="at">main=</span><span class="st">&quot;Explosion&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-7-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The general problem of interest is in distinguishing or
discriminating between waveforms generated by earthquakes and those
generated by explosions.</p></li>
<li><p>Features that may be important are the rough amplitude ratios of
the first phase P to the second phase S, which tend to be smaller for
earthquakes than for explosions.</p></li>
<li><p>Remember: <img src="amplitude-period-ex0.jpg" /></p></li>
</ul>
</div>
</div>
<div id="time-series-statistical-models" class="section level2">
<h2>Time Series Statistical Models</h2>
<ul>
<li><p>The primary objective of time series analysis is to develop
mathematical models that provide plausible descriptions for sample
data.</p></li>
<li><p>A time series can be defined as a collection of random variables
indexed according to the order they are obtained in time.</p></li>
<li><p>For example, we may consider a time series as a sequence of
random variables, <span class="math inline">\(x_{1}, x_{2},
x_{3},\ldots\)</span>, where</p>
<ul>
<li>the random variable <span class="math inline">\(x_{1}\)</span>
denotes the value taken by the series at the first time point,</li>
<li>the variable <span class="math inline">\(x_{2}\)</span> denotes the
value for the second time period,</li>
<li><span class="math inline">\(x_{3}\)</span> denotes the value for the
third time period, and so on.</li>
</ul></li>
<li><p>In general, a collection of random variables, <span
class="math inline">\(\{x_{t}\}\)</span>, indexed by <span
class="math inline">\(t\)</span> is referred to as a stochastic
process.</p></li>
<li><p><span class="math inline">\(t\)</span> will typically be discrete
and vary over the integers <span class="math inline">\(t=0, \pm1,
\pm2,\ldots\)</span>, or some subset of the integers. The observed
values of a stochastic process are referred to as a realization of the
stochastic process.</p></li>
<li><p>The fundamental visual characteristic distinguishing the
different series shown before is their differing degrees of
smoothness.</p></li>
<li><p>One possible explanation for this smoothness is that it is being
induced by the supposition that adjacent points in time are correlated,
so the value of the series at time <span
class="math inline">\(t\)</span>, say, <span
class="math inline">\(x_{t}\)</span>, depends in some way on the past
values <span class="math inline">\(x_{t-1},
x_{t-2},\ldots\)</span>.</p></li>
<li><p>This model expresses a fundamental way in which we might think
about generating realistic-looking time series. To begin to develop an
approach to using collections of random variables to model time series,
consider the following example.</p></li>
</ul>
<div id="example-white-noise-3-flavors" class="section level3">
<h3>Example: White Noise (3 flavors)</h3>
<ul>
<li><p>A simple kind of generated series might be a collection of
uncorrelated random variables, <span
class="math inline">\(w_{t}\)</span>, with mean <span
class="math inline">\(0\)</span> and finite variance <span
class="math inline">\(\sigma^{2}_{w}\)</span>.</p></li>
<li><p>The time series generated from uncorrelated variables is used as
a model for noise in engineering applications, where it is called white
noise; we shall denote this process as <span
class="math inline">\(w_{t}\sim \textrm{wn}(0,
\sigma^{2}_{w})\)</span>.</p></li>
<li><p>The designation white originates from the analogy with white
light and indicates that all possible periodic oscillations are present
with equal strength.</p></li>
<li><p>We will sometimes require the noise to be independent and
identically distributed (iid) random variables with mean <span
class="math inline">\(0\)</span> and variance <span
class="math inline">\(\sigma^{2}_{w}\)</span>. We distinguish this by
writing <span class="math inline">\(w_{t}\sim \textrm{iid}(0,
\sigma^{2}_{w})\)</span> or by saying white independent noise or iid
noise.</p></li>
<li><p>A particularly useful white noise series is Gaussian white noise,
wherein the <span class="math inline">\(w_{t}\)</span> are independent
normal random variables, with mean <span
class="math inline">\(0\)</span> and variance <span
class="math inline">\(\sigma^{2}_{w}\)</span>; or more succinctly, <span
class="math inline">\(w_{t}\sim \textrm{iid}\;\textrm{N}(0,
\sigma^{2}_{w})\)</span>.</p></li>
<li><p>The following figure shows a collection of 500 such random
variables, with <span class="math inline">\(\sigma^{2}_{w}=1\)</span>,
plotted in the order in which they were drawn.</p></li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>w <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">500</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="fu">plot.ts</span>(w, <span class="at">main=</span><span class="st">&quot;white noise&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-8-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>If the stochastic behavior of all time series could be explained
in terms of the white noise model, classical statistical methods would
suffice.</p></li>
<li><p>Two ways of introducing serial correlation and more smoothness
into time series models are given in following examples.</p></li>
</ul>
</div>
<div id="example-moving-averages-and-filtering" class="section level3">
<h3>Example: Moving Averages and Filtering</h3>
<ul>
<li><p>We might replace the white noise series <span
class="math inline">\(w_{t}\)</span> by a moving average that smooths
the series. For example, consider replacing <span
class="math inline">\(w_{t}\)</span> by an average of its current value
and its immediate neighbors in the past and future.</p></li>
<li><p>That is, let <span class="math display">\[
\nu_{t}=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1}),
\]</span> which leads to the series</p></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>v <span class="ot">=</span> <span class="fu">filter</span>(w, <span class="at">sides=</span><span class="dv">2</span>, <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="fu">tsplot</span>(v, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>), <span class="at">main=</span><span class="st">&quot;moving average&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-9-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>A linear combination of values in a time series such as before is
referred to, generically, as a filtered series.</p></li>
<li><p>Some series differ from the moving average series because one
particular kind of oscillatory behavior seems to predominate, producing
a sinusoidal type of behavior. A number of methods exist for generating
series with this quasi-periodic behavior; we illustrate a popular one
based on the autoregressive model.</p></li>
</ul>
</div>
<div id="example-autoregressions" class="section level3">
<h3>Example: Autoregressions</h3>
<ul>
<li><p>Suppose we consider the white noise series <span
class="math inline">\(w_{t}\)</span> as input and calculate the output
using the second-order equation <span class="math display">\[
x_{t}=x_{t-1}-0.9x_{t-2}+w_{t}
\]</span> successively for <span class="math inline">\(t = 1, 2,\ldots,
500\)</span>. This Equation represents a regression or prediction of the
current value <span class="math inline">\(x_{t}\)</span> of a time
series as a function of the past two values of the series, and, hence,
the term autoregression is suggested for this model.</p></li>
<li><p>A problem with startup values exists here because the model also
depends on the initial conditions <span
class="math inline">\(x_{0}\)</span> and <span
class="math inline">\(x_{-1}\)</span>, but assuming we have the values,
we generate the succeeding values by substituting into the previous
equation. The resulting output series is shown the following figure, and
we note the periodic behavior of the series.</p></li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>w <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">550</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">filter</span>(w, <span class="at">filter=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span>.<span class="dv">9</span>), <span class="at">method=</span><span class="st">&quot;recursive&quot;</span>)[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>)]</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="fu">tsplot</span>(x, <span class="at">main=</span><span class="st">&quot;autoregression&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-10-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="example-random-walk-with-drift" class="section level3">
<h3>Example: Random Walk with Drift</h3>
<ul>
<li><p>A model for analyzing trend is the random walk with drift model
given by <span class="math display">\[
x_{t}=\delta +x_{t-1}+w_{t}
\]</span> for <span class="math inline">\(t = 1, 2,\ldots\)</span>, with
initial condition <span class="math inline">\(x_{0}=0\)</span>, and
where <span class="math inline">\(w_{t}\)</span> is white
noise.</p></li>
<li><p>The constant <span class="math inline">\(\delta\)</span> is
called the drift, and when <span class="math inline">\(\delta
=0\)</span>, this model is called simply a random walk.</p></li>
<li><p>The term random walk comes from the fact that, when <span
class="math inline">\(\delta =0\)</span>, the value of the time series
at time <span class="math inline">\(t\)</span> is the value of the
series at time <span class="math inline">\(t−1\)</span> plus a
completely random movement determined by <span
class="math inline">\(w_{t}\)</span>.</p></li>
<li><p>Note that <span class="math display">\[
x_{t}=\delta t+\sum_{j=1}^{t}w_{j}
\]</span> for <span class="math inline">\(t=1,
2,\ldots\)</span>.</p></li>
<li><p>The following figure shows <span
class="math inline">\(200\)</span> observations generated from the model
with <span class="math inline">\(\delta =0\)</span> and <span
class="math inline">\(.2\)</span>, and with <span
class="math inline">\(\sigma_{w}=1\)</span>. For comparison, we also
superimposed the straight line <span class="math inline">\(.2t\)</span>
on the graph.</p></li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">154</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>w <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">200</span>); x <span class="ot">=</span> <span class="fu">cumsum</span>(w)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>wd <span class="ot">=</span> w <span class="sc">+</span>.<span class="dv">2</span>;    xd <span class="ot">=</span> <span class="fu">cumsum</span>(wd)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="fu">tsplot</span>(xd, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">55</span>), <span class="at">main=</span><span class="st">&quot;random walk&quot;</span>, <span class="at">ylab=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="at">col=</span><span class="dv">4</span>) </span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>, <span class="at">col=</span><span class="dv">4</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span><span class="dv">0</span>, <span class="at">b=</span>.<span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-11-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="example-signal-in-noise" class="section level3">
<h3>Example: Signal in Noise</h3>
<ul>
<li><p>Many realistic models for generating time series assume an
underlying signal with some consistent periodic variation, contaminated
by adding a random noise. Consider the model <span
class="math display">\[
x_{t}=2\cos (2\pi \frac{t+15}{50})+w_{t}
\]</span> for <span class="math inline">\(t=1, 2,\ldots , 500\)</span>,
where the first term is regarded as the signal, shown in the upper panel
of the following figure.</p></li>
<li><p>An additive noise term was taken to be white noise with <span
class="math inline">\(\sigma_{w} =1\)</span> (middle panel) and <span
class="math inline">\(\sigma_{w} =5\)</span> (bottom panel), drawn from
a normal distribution.</p></li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>cs <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>)<span class="sc">/</span><span class="dv">50</span> <span class="sc">+</span> .<span class="dv">6</span><span class="sc">*</span>pi)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>w <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">500</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">cex.main=</span><span class="fl">1.5</span>)   <span class="co"># help(par) for info</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="fu">tsplot</span>(cs, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="fu">expression</span>(x[t]<span class="sc">==</span><span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>t<span class="sc">/</span><span class="dv">50</span><span class="fl">+.6</span><span class="sc">*</span>pi)))</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="fu">tsplot</span>(cs <span class="sc">+</span> w, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="fu">expression</span>(x[t]<span class="sc">==</span><span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>t<span class="sc">/</span><span class="dv">50</span><span class="fl">+.6</span><span class="sc">*</span>pi)<span class="sc">+</span><span class="fu">N</span>(<span class="dv">0</span>,<span class="dv">1</span>)))</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="fu">tsplot</span>(cs <span class="sc">+</span> <span class="dv">5</span><span class="sc">*</span>w, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="fu">expression</span>(x[t]<span class="sc">==</span><span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>t<span class="sc">/</span><span class="dv">50</span><span class="fl">+.6</span><span class="sc">*</span>pi)<span class="sc">+</span><span class="fu">N</span>(<span class="dv">0</span>,<span class="dv">25</span>)))</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-12-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>We note that a sinusoidal waveform can be written as <span
class="math display">\[
A\cos (2\pi \omega t+\phi),
\]</span> where <span class="math inline">\(A\)</span> is the
<em>amplitude</em>, <span class="math inline">\(\omega\)</span> is the
<em>frequency of oscillation</em>, and <span
class="math inline">\(\phi\)</span> is a <em>phase shift</em>.</p></li>
<li><p>In the above example, <span class="math inline">\(A=2\)</span>,
<span class="math inline">\(\omega=1/50\)</span> (one cycle every <span
class="math inline">\(50\)</span> time points), and <span
class="math inline">\(\phi=2\pi 15/50=.6\pi\)</span>.</p></li>
<li><p>Adding <span class="math inline">\(w_{t}\)</span> obscures the
signal. Of course, the degree to which the signal is obscured depends on
the amplitude of the signal and the size of <span
class="math inline">\(\sigma_{w}\)</span>.</p></li>
<li><p>Typically, we will not observe the signal but the signal obscured
by noise.</p></li>
<li><p>We will study the use of spectral analysis as a possible
technique for detecting regular or periodic signals.</p></li>
<li><p>In general, we would emphasize the importance of simple additive
models such as given above in the form <span class="math display">\[
x_{t}=s_{t}+\nu_{t},
\]</span> where <span class="math inline">\(s_{t}\)</span> denotes some
unknown signal and <span class="math inline">\(\nu_{t}\)</span> denotes
a time series that may be white or correlated over time.</p></li>
<li><p>The problems of detecting a signal and then in estimating or
extracting the waveform of <span class="math inline">\(s_{t}\)</span>
are of great interest in many areas of engineering and the physical and
biological sciences.</p></li>
<li><p>In economics, the underlying signal may be a trend or it may be a
seasonal component of a series.</p></li>
<li><p>Models such as before, where the signal has an autoregressive
structure, form the motivation for the state-space.</p></li>
</ul>
</div>
</div>
<div id="measures-of-dependence" class="section level2">
<h2>Measures of Dependence</h2>
<ul>
<li><p>A complete description of a time series, observed as a collection
of <span class="math inline">\(n\)</span> random variables at arbitrary
time points <span class="math inline">\(t_{1}, t_{2},\ldots ,
t_{n}\)</span>, for any positive integer <span
class="math inline">\(n\)</span>, is provided by the joint distribution
function, <span class="math display">\[
F_{t_{1}, t_{2},\ldots ,t_{n}}(c_{1}, c_{2},\ldots
,c_{n})=\textrm{Pr}(x_{t_{1}}\leq c_{1}, x_{t_{2}}\leq c_{2},\ldots ,
x_{t_{n}}\leq c_{n}).
\]</span></p></li>
<li><p>The marginal distribution functions <span class="math display">\[
F_{t}(x)=P\{x_{t}\leq x\}
\]</span> or the corresponding marginal density functions <span
class="math display">\[
f_{t}(x)=\frac{\partial F_{t}(x)}{\partial x},
\]</span> when they exist, are often informative for examining the
marginal behavior of a series.</p></li>
<li><p>Another informative marginal descriptive measure is the mean
function.</p></li>
</ul>
<div id="mean-function" class="section level3">
<h3>Mean Function</h3>
<blockquote>
<p>The mean function is defined as <span class="math display">\[
\mu_{x_{t}}=\textrm{E}(x_{t})=\int_{-\infty}^{\infty}xf_{t}(x)dx,
\]</span> provided it exists, where <span
class="math inline">\(\textrm{E}\)</span> denotes the usual expected
value operator. When no confusion exists about which time series we are
referring to, we will drop a subscript and write <span
class="math inline">\(\mu_{x_{t}}\)</span> as <span
class="math inline">\(\mu_{t}\)</span>.</p>
</blockquote>
</div>
<div id="example-mean-function-of-a-moving-average-series"
class="section level3">
<h3>Example: Mean Function of a Moving Average Series</h3>
<ul>
<li>If <span class="math inline">\(w_{t}\)</span> denotes a white noise
series, then <span
class="math inline">\(\mu_{w_{t}}=\textrm{E}(w_{t})=0\)</span> for all
<span class="math inline">\(t\)</span>. Let <span
class="math display">\[
\nu_{t}=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1}),
\]</span> where <span class="math inline">\(w_{t}\)</span> is white
noise. Then <span class="math display">\[
\mu_{\nu_{t}}=\textrm{E}(\nu_{t})=\frac{1}{3}[\textrm{E}(w_{t-1})+\textrm{E}(w_{t})+\textrm{E}(w_{t+1})]=0.
\]</span></li>
</ul>
</div>
<div id="example-mean-function-of-a-random-walk-with-drift"
class="section level3">
<h3>Example: Mean Function of a Random Walk with Drift</h3>
<ul>
<li>Consider the random walk with drift model <span
class="math display">\[
x_{t}=\delta t+\sum_{j=1}^{t}w_{j},\quad t=1, 2,\ldots .
\]</span> Because <span
class="math inline">\(\textrm{E}(w_{t})=0\)</span> for all <span
class="math inline">\(t\)</span>, and <span
class="math inline">\(\delta\)</span> is a constant, we have <span
class="math display">\[
\mu_{x_{t}}=\textrm{E}(x_{t})=\delta
t+\sum_{j=1}^{t}\textrm{E}(w_{j})=\delta t
\]</span> which is a straight line with slope <span
class="math inline">\(\delta\)</span>.</li>
</ul>
</div>
<div id="example-mean-function-of-signal-plus-noise"
class="section level3">
<h3>Example: Mean Function of Signal Plus Noise</h3>
<ul>
<li><p>On the model <span class="math display">\[
x_{t}=2\cos (2\pi \frac{t+15}{50})+w_{t}
\]</span> we have <span class="math display">\[\begin{align*}
\mu_{x_{t}} =\textrm{E}(x_{t}) &amp;
=\textrm{E}[2\cos(2\pi\frac{t+15}{50})+w_{t}]\\
&amp; =2\cos(2\pi\frac{t+15}{50})+\textrm{E}(w_{t})\\
&amp; =2\cos(2\pi\frac{t+15}{50}),
\end{align*}\]</span> and the mean function is just the cosine
wave.</p></li>
<li><p>The lack of independence between two adjacent values <span
class="math inline">\(x_{s}\)</span> and <span
class="math inline">\(x_{t}\)</span> can be assessed numerically, as in
classical statistics, using the notions of covariance and
correlation.</p></li>
</ul>
</div>
<div id="autocovariance-function" class="section level3">
<h3>Autocovariance Function</h3>
<ul>
<li>Assuming the variance of <span class="math inline">\(x_{t}\)</span>
is finite, we have the following definition.</li>
</ul>
<blockquote>
<p>The autocovariance function is defined as the second moment product
<span class="math display">\[
\gamma_{x}(s, t)=\textrm{cov}(x_{s},
x_{t})=\textrm{E}[(x_{s}-\mu_{s})(x_{t}-\mu_{t})],
\]</span> for all <span class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span>. When no possible confusion exists
about which time series we are referring to, we will drop the subscript
and write <span class="math inline">\(\gamma_{x}(s, t)\)</span> as <span
class="math inline">\(\gamma (s, t)\)</span>. Note that <span
class="math inline">\(\gamma_{x}(s, t)=\gamma_{x}(t, s)\)</span> for all
time points <span class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span>.</p>
</blockquote>
<ul>
<li><p>The autocovariance measures the linear dependence between two
points on the same series observed at different times.</p></li>
<li><p>Recall from classical statistics that if <span
class="math inline">\(\gamma_{x}(s, t)=0\)</span>, <span
class="math inline">\(x_{s}\)</span> and <span
class="math inline">\(x_{t}\)</span> are not linearly related, but there
still may be some dependence structure between them.</p></li>
<li><p>If, however, <span class="math inline">\(x_{s}\)</span> and <span
class="math inline">\(x_{t}\)</span> are bivariate normal, <span
class="math inline">\(\gamma_{x}(s, t)=0\)</span> ensures their
independence.</p></li>
<li><p>For <span class="math inline">\(s=t\)</span>, the autocovariance
reduces to the (assumed finite) variance <span class="math display">\[
\gamma_{x}(t, t)=\textrm{E}[(x_{t}-\mu_{t})^2]=\textrm{var}(x_{t}).
\]</span></p></li>
</ul>
</div>
<div id="example-autocovariance-of-white-noise" class="section level3">
<h3>Example: Autocovariance of White Noise</h3>
<ul>
<li>The white noise series <span class="math inline">\(w_{t}\)</span>
has <span class="math inline">\(\textrm{E}(w_{t})=0\)</span> and <span
class="math display">\[
\gamma_{w}(s,t)=\textrm{cov}(w_{s},w_{t})=\begin{cases}
\sigma_{w}^{2} &amp; s=t,\\
0 &amp; s\neq t.
\end{cases}
\]</span></li>
</ul>
</div>
<div id="covariance-of-linear-combinations" class="section level3">
<h3>Covariance of Linear Combinations</h3>
<ul>
<li>If the random variables <span class="math display">\[
U=\sum_{j=1}^{m}a_{j}X_{j}\quad\textrm{and}\quad
V=\sum_{k=1}^{r}b_{k}Y_{k}
\]</span> are linear combinations of (finite variance) random variables
<span class="math inline">\(\{X_j\}\)</span> and <span
class="math inline">\(\{Y_k\}\)</span>, respectively, then <span
class="math display">\[
\textrm{cov}(U,V)=\sum_{j=1}^{m}\sum_{k=1}^{r}a_{j}b_{k}\textrm{cov}(X_{j},Y_{k}).
\]</span> Furthermore, <span
class="math inline">\(\textrm{var}(U)=\textrm{cov}(U, U)\)</span>.</li>
</ul>
</div>
<div id="example-autocovariance-of-a-moving-average"
class="section level3">
<h3>Example: Autocovariance of a Moving Average</h3>
<ul>
<li><p>Consider <span class="math display">\[
\nu_{t}=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1}).
\]</span></p></li>
<li><p>In this case, <span class="math display">\[
\gamma_{\nu}(s, t)=\textrm{cov}(\nu_{s},
\nu_{t})=\textrm{cov}\{\frac{1}{3}(w_{s-1}+w_{s}+w_{s+1}),\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1})\}.
\]</span></p></li>
<li><p>When <span class="math inline">\(s=t\)</span> we have <span
class="math display">\[\begin{align*}
\gamma_{\nu}(t,t) &amp;
=\frac{1}{9}\textrm{cov}\{(w_{t-1}+w_{t}+w_{t+1}),(w_{t-1}+w_{t}+w_{t+1})\}\\
&amp;
=\frac{1}{9}[\textrm{cov}(w_{t-1},w_{t-1})+\textrm{cov}(w_{t},w_{t})+\textrm{cov}(w_{t+1},w_{t+1})]\\
&amp; =\frac{3}{9}\sigma_{w}^{2}.
\end{align*}\]</span></p></li>
<li><p>When <span class="math inline">\(s=t+1\)</span> <span
class="math display">\[\begin{align*}
\gamma_{\nu}(t+1,t) &amp;
=\frac{1}{9}\textrm{cov}\{(w_{t}+w_{t+1}+w_{t+2}),(w_{t-1}+w_{t}+w_{t+1})\}\\
&amp;
=\frac{1}{9}[\textrm{cov}(w_{t},w_{t})+\textrm{cov}(w_{t+1},w_{t+1})]\\
&amp; =\frac{2}{9}\sigma_{w}^{2}.
\end{align*}\]</span></p></li>
<li><p>Similar computations give <span
class="math inline">\(\gamma_{\nu}(t-1,t)=\frac{2}{9}\sigma_{w}^{2}\)</span>,
<span
class="math inline">\(\gamma_{\nu}(t+2,t)=\gamma_{\nu}(t-2,t)=\frac{1}{9}\sigma_{w}^{2}\)</span>,
and <span class="math inline">\(0\)</span> when <span
class="math inline">\(\left|t-s\right|&gt;2\)</span>.</p></li>
<li><p>We summarize the values for all <span
class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span> as <span class="math display">\[
\gamma_{\nu}(s,t)=\begin{cases}
\frac{3}{9}\sigma_{w}^{2} &amp; s=t,\\
\frac{2}{9}\sigma_{w}^{2} &amp; \left|t-s\right|=1,\\
\frac{1}{9}\sigma_{w}^{2} &amp; \left|t-s\right|=2,\\
0 &amp; \left|t-s\right|&gt;2.
\end{cases}
\]</span></p></li>
</ul>
</div>
<div id="example-autocovariance-of-a-random-walk"
class="section level3">
<h3>Example: Autocovariance of a Random Walk</h3>
<ul>
<li><p>For the random walk model, <span
class="math inline">\(x_{t}=\sum_{j=1}^{t}w_{j}\)</span>, we have <span
class="math display">\[
\gamma_{x}(s,t)=\textrm{cov}(x_{s},x_{t})=\textrm{cov}(\sum_{j=1}^{s}w_{j},\sum_{k=1}^{t}w_{k})=\min\{s,t\}\sigma_{w}^{2},
\]</span> because the <span class="math inline">\(w_{t}\)</span> are
uncorrelated random variables.</p></li>
<li><p>Notice that the variance of the random walk, <span
class="math display">\[
\textrm{var}(x_{t})=\gamma_{x}(t, t)=t\sigma_{w}^{2},
\]</span> increases without bound as time <span
class="math inline">\(t\)</span> increases.</p></li>
</ul>
</div>
<div id="autocorrelation-function" class="section level3">
<h3>Autocorrelation Function</h3>
<blockquote>
<p>The autocorrelation function (ACF) is defined as <span
class="math display">\[
\rho (s, t)=\frac{\gamma (s, t)}{\sqrt{\gamma (s, s)\gamma (t, t)}}.
\]</span></p>
</blockquote>
<ul>
<li><p>The ACF measures the linear predictability of the series at time
<span class="math inline">\(t\)</span>, say <span
class="math inline">\(x_{t}\)</span>, using only the value <span
class="math inline">\(x_{s}\)</span>.</p></li>
<li><p><span class="math inline">\(-1\leq \rho (s, t)\leq 1\)</span>.
The Cauchy–Schwarz inequality implies <span
class="math inline">\(\left|\gamma(s,t)\right|^{2}\leq\gamma(s,s)\gamma(t,t)\)</span>.</p></li>
</ul>
</div>
<div id="the-cross-covariance-function" class="section level3">
<h3>The Cross-Covariance Function</h3>
<ul>
<li>Often, we would like to measure the predictability of another series
<span class="math inline">\(y_{t}\)</span> from the series <span
class="math inline">\(x_{s}\)</span>. Assuming both series have finite
variances</li>
</ul>
<blockquote>
<p>The cross-covariance function between two series, <span
class="math inline">\(x_{s}\)</span> and <span
class="math inline">\(y_{t}\)</span>, is <span class="math display">\[
\gamma_{xy}(s,t)=\textrm{cov}(x_{s},y_{t})=\textrm{E}[(x_{s}-\mu_{x_{s}})(y_{t}-\mu_{y_{t}})].
\]</span></p>
</blockquote>
</div>
<div id="the-cross-correlation-function" class="section level3">
<h3>The Cross-Correlation Function</h3>
<blockquote>
<p>The cross-correlation function (CCF) is given by <span
class="math display">\[
\rho_{xy}(s,t)=\frac{\gamma_{xy}(s,t)}{\sqrt{\gamma_{x}(s,s)\gamma_{y}(t,t)}}
\]</span></p>
</blockquote>
<ul>
<li>We may easily extend the above ideas to the case of more than two
series, say, <span class="math inline">\(x_{t_{1}}, x_{t_{2}},\ldots ,
x_{t_{r}}\)</span>; that is, multivariate time series with <span
class="math inline">\(r\)</span> components. For example <span
class="math display">\[
\gamma_{jk}(s,t)=\textrm{E}[(x_{s_{j}}-\mu_{s_{j}})(x_{t_{k}}-\mu_{t_{k}})]\quad
j,k=1,2,\ldots,r.
\]</span></li>
</ul>
</div>
</div>
<div id="stationary-time-series" class="section level2">
<h2>Stationary Time Series</h2>
<ul>
<li>In the definitions above, the autocovariance and cross-covariance
functions may change as one moves along the series because the values
depend on both <span class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span>, the locations of the points in
time.</li>
</ul>
<div id="strictly-stationary" class="section level3">
<h3>Strictly Stationary</h3>
<blockquote>
<p>A strictly stationary time series is one for which the probabilistic
behavior of every collection of values <span class="math display">\[
\{x_{t_{1}},x_{t_{2}},\ldots,x_{t_{k}}\}
\]</span> is identical to that of the time shifted set <span
class="math display">\[
\{x_{t_{1}+h},x_{t_{2}+h},\ldots,x_{t_{k}+h}\}.
\]</span> That is, <span class="math display">\[
\textrm{Pr}\{x_{t_{1}}\leq c_{1},\ldots,x_{t_{k}}\leq
c_{k}\}=\textrm{Pr}\{x_{t_{1}+h}\leq c_{1},\ldots,x_{t_{k}+h}\leq
c_{k}\}
\]</span> for all <span class="math inline">\(k=1, 2,\ldots\)</span>,
all time points <span class="math inline">\(t_{1}, t_{2},\ldots ,
t_{k}\)</span>, all numbers <span class="math inline">\(c_{1},
c_{2},\ldots , c_{k}\)</span>, and all time shifts <span
class="math inline">\(h=0, \pm 1, \pm 2,\ldots\)</span>.</p>
</blockquote>
<ul>
<li><p>For example, when <span class="math inline">\(k=1\)</span>, it
implies that <span class="math display">\[
\textrm{Pr}\{x_{s}\leq c\}=\textrm{Pr}\{x_{t}\leq c\}
\]</span> for any time points <span class="math inline">\(s\)</span> and
<span class="math inline">\(t\)</span>.</p></li>
<li><p>It implies that if the mean function, <span
class="math inline">\(\mu_{t}\)</span>, of the series exists, then <span
class="math inline">\(\mu_{s}=\mu_{t}\)</span> for all <span
class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span>, and hence <span
class="math inline">\(\mu_{t}\)</span> must be constant.</p></li>
<li><p>A random walk process with drift is not strictly stationary.
Why?</p></li>
<li><p>When k = 2, we have <span class="math display">\[
\textrm{Pr}\{x_{s}\leq c_{1},x_{t}\leq c_{2}\}=\textrm{Pr}\{x_{s+h}\leq
c_{1},x_{t+h}\leq c_{2}\}
\]</span> for any time points <span class="math inline">\(s\)</span> and
<span class="math inline">\(t\)</span> and shift <span
class="math inline">\(h\)</span>. Thus, if the variance function of the
process exists, it implies that the autocovariance function of the
series <span class="math inline">\(x_{t}\)</span> satisfies <span
class="math display">\[
\gamma(s,t)=\gamma(s+h,t+h)
\]</span> for all <span class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span> and <span
class="math inline">\(h\)</span>.</p></li>
<li><p>The autocovariance function of the process depends only on the
time difference between <span class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span>, and not on the actual times.</p></li>
</ul>
</div>
<div id="weakly-stationary" class="section level3">
<h3>Weakly Stationary</h3>
<ul>
<li>The strictly stationarity is too strong for most applications.</li>
</ul>
<blockquote>
<p>A weakly stationary time series, <span
class="math inline">\(x_{t}\)</span>, is a finite variance process such
that<br />
<span class="math inline">\(\quad\)</span> i – the mean value function,
<span class="math inline">\(\mu_{t}\)</span>,is constant and does not
depend on time <span class="math inline">\(t\)</span>, and<br />
<span class="math inline">\(\quad\)</span> ii – the autocovariance
function, <span class="math inline">\(\gamma (s, t)\)</span>, depends on
<span class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span> only through their difference <span
class="math inline">\(\left| s-t\right|\)</span>.<br />
Henceforth, we will use the term stationary to mean weakly stationary;
if a process is stationary in the strict sense, we will use the term
strictly stationary.</p>
</blockquote>
<ul>
<li><p>A strictly stationary, finite variance, time series is also
stationary.</p></li>
<li><p>The converse is not true unless there are further
conditions.</p></li>
</ul>
<p>-One important case where stationarity implies strict stationarity is
if the time series is Gaussian.</p>
<ul>
<li><p>Because the mean function, <span
class="math inline">\(\textrm{E}(x_{t})=\mu_{t}\)</span>, of a
stationary time series is independent of time <span
class="math inline">\(t\)</span>, we will write <span
class="math display">\[
\mu_{t}=\mu.
\]</span></p></li>
<li><p>Because the autocovariance function, <span
class="math inline">\(\gamma (s, t)\)</span>, of a stationary time
series, <span class="math inline">\(x_{t}\)</span>, depends on <span
class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span> only through their difference <span
class="math inline">\(\left|s-t\right|\)</span>, we have <span
class="math display">\[
\gamma (t+h, t)=\textrm{cov}(x_{t+h},
x_{t})=\textrm{cov}(x_{h},x_{0})=\gamma (h,0)
\]</span> where <span class="math inline">\(s=t+h\)</span>, where <span
class="math inline">\(h\)</span> represents the time shift or
lag.</p></li>
</ul>
</div>
<div id="autocovariance-function-of-a-stationary-time-series"
class="section level3">
<h3>Autocovariance Function of a Stationary Time Series</h3>
<blockquote>
<p>The autocovariance function of a stationary time series will be
written as <span class="math display">\[
\gamma
(h)=\textrm{cov}(x_{t+h},x_{t})=\textrm{E}[(x_{t+h}-\mu)(x_{t}-\mu)].
\]</span></p>
</blockquote>
</div>
<div id="autocorrelation-function-acf-of-a-stationary-time-series"
class="section level3">
<h3>Autocorrelation Function (ACF) of a Stationary Time Series</h3>
<blockquote>
<p>The autocorrelation function (ACF) of a stationary time series will
be written as <span class="math display">\[
\rho
(h)=\frac{\gamma(t+h,t)}{\sqrt{\gamma(t+h,t+h)\gamma(t,t)}}=\frac{\gamma(h)}{\gamma(0)}.
\]</span></p>
</blockquote>
</div>
<div id="example-stationarity-of-white-noise" class="section level3">
<h3>Example: Stationarity of White Noise</h3>
<ul>
<li><p>The mean and autocovariance functions of the white noise series
are <span class="math inline">\(\mu_{w_{t}}=0\)</span> and <span
class="math display">\[
\gamma_{w}(h)=\textrm{cov}(w_{t+h},w_{t})=\begin{cases}
\sigma_{w}^{2} &amp; h=0,\\
0 &amp; h\neq0.
\end{cases}
\]</span></p></li>
<li><p>The white noise is weakly stationary or stationary. If the white
noise variates are also normally distributed or Gaussian, the series is
also strictly stationary.</p></li>
</ul>
</div>
<div id="example-stationarity-of-a-moving-average"
class="section level3">
<h3>Example: Stationarity of a Moving Average</h3>
<ul>
<li><p>Consider <span class="math display">\[
\nu_{t}=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1}).
\]</span></p></li>
<li><p>This process is stationary because</p>
<ul>
<li>the mean function <span
class="math inline">\(\mu_{\nu_{t}}=0\)</span> and</li>
<li>the autocovariance function <span class="math display">\[
\gamma_{\nu}(h)=\begin{cases}
\frac{3}{9}\sigma_{w}^{2} &amp; h=0,\\
\frac{2}{9}\sigma_{w}^{2} &amp; h=\pm 1,\\
\frac{1}{9}\sigma_{w}^{2} &amp; h=\pm 2,\\
0 &amp; \left|h\right|&gt;2.
\end{cases}
\]</span> are independent of time <span
class="math inline">\(t\)</span>.</li>
<li>The autocorrelation function is given by <span
class="math display">\[
\rho_{\nu}(h)=\begin{cases}
1 &amp; h=0,\\
\frac{2}{3} &amp; h=\pm1,\\
\frac{1}{3} &amp; h=\pm2,\\
0 &amp; \left|h\right|&gt;2.
\end{cases}
\]</span></li>
</ul></li>
<li><p>A plot of the autocorrelations as function of lag <span
class="math inline">\(h\)</span> is given by</p></li>
</ul>
</div>
<div id="example-a-random-walk-is-not-stationary"
class="section level3">
<h3>Example: A Random Walk is Not Stationary</h3>
<ul>
<li><p>A random walk is not stationary because its autocovariance
function, <span class="math display">\[
\gamma(s,t)=\min \{s,t\}\sigma_{w}^{2},
\]</span> depends on time.</p></li>
<li><p>Also, the mean function, <span
class="math inline">\(\mu_{x_{t}}=\delta t\)</span>, is also a function
of time <span class="math inline">\(t\)</span>.</p></li>
</ul>
</div>
<div id="example-trend-stationarity" class="section level3">
<h3>Example: Trend Stationarity</h3>
<ul>
<li><p>Consider <span class="math inline">\(x_{t}=\alpha +\beta
t+y_{t}\)</span>, where <span class="math inline">\(y_{t}\)</span> is
stationary.</p></li>
<li><p>Then the mean function is <span class="math display">\[
\mu_{x_{t}}=\textrm{E}(x_{t})=\alpha +\beta t+\mu_{y},
\]</span> which is not independent of time.</p></li>
<li><p>Therefore, the process is not stationary.</p></li>
<li><p>However, the autocovariance function is independent of time,
because <span class="math display">\[
\gamma_{x}(h)=\textrm{cov}(x_{t+h},x_{t})=\textrm{E}[(x_{t+h}-\mu_{x_{t+h}})(x_{t}-\mu_{x_{t}})]=\textrm{E}[(y_{t+h}-\mu_{y})(y_{t}-\mu_{y})]=\gamma_{y}(h).
\]</span></p></li>
<li><p>Thus, the model may be considered as having stationary behavior
around a linear trend; this behavior is sometimes called <em>trend
stationarity</em>.</p></li>
</ul>
</div>
<div id="autocovariance-function-properties" class="section level3">
<h3>Autocovariance Function Properties</h3>
<ol style="list-style-type: lower-roman">
<li><p><span class="math inline">\(\gamma(h)\)</span> is non-negative
definite ensuring that variances of linear combinations of the variates
<span class="math inline">\(x_{t}\)</span> will never be negative. That
is, for any <span class="math inline">\(n\geq 1\)</span>, and constants
<span class="math inline">\(a_{1},\ldots,a_{n}\)</span>, <span
class="math display">\[
0\leq\textrm{var}(a_{1}x_{1}+\cdots+a_{n}x_{n})=\sum_{j=1}^{n}\sum_{k=1}^{n}a_{j}a_{k}\gamma(j-k).
\]</span></p></li>
<li><p>The value at <span class="math inline">\(h=0\)</span>, namely
<span class="math display">\[
\gamma(0)=\textrm{E}[(x_{t}-\mu)^{2}]
\]</span> is the variance of the time series and the Cauchy–Schwarz
inequality implies <span class="math display">\[
\left|\gamma(h)\right|\leq\gamma(0).
\]</span></p></li>
<li><p>The autocovariance function of a stationary series is symmetric
around the origin; that is, <span class="math display">\[
\gamma(h)=\gamma(-h)
\]</span> for all <span class="math inline">\(h\)</span>. This property
follows because <span class="math display">\[
\gamma((t+h)-t)=\textrm{cov}(x_{t+h},x_{t})=\textrm{cov}(x_{t},x_{t+h})=\gamma(t-(t+h)).
\]</span></p></li>
</ol>
</div>
<div id="jointly-stationary" class="section level3">
<h3>Jointly Stationary</h3>
<blockquote>
<p>Two time series, say, <span class="math inline">\(x_{t}\)</span> and
<span class="math inline">\(y_{t}\)</span>, are said to be jointly
stationary if they are each stationary, and the cross-covariance
function <span class="math display">\[
\gamma_{xy}(h)=\textrm{cov}(x_{t+h},y_{t})=\textrm{E}[(x_{t+h}-\mu_{x})(y_{t}-\mu_{y})]
\]</span> is a function only of lag <span
class="math inline">\(h\)</span>.</p>
</blockquote>
</div>
<div id="cross-correlation-function-ccf" class="section level3">
<h3>Cross-Correlation function (CCF)</h3>
<blockquote>
<p>The cross-correlation function (CCF) of jointly stationary time
series <span class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(y_{t}\)</span> is defined as <span
class="math display">\[
\rho_{xy}(h)=\frac{\gamma_{xy}(h)}{\sqrt{\gamma_{x}(0)\gamma_{y}(0)}}.
\]</span></p>
</blockquote>
<ul>
<li><p>The cross-correlation function is not generally symmetric about
zero, i.e., typically <span class="math display">\[
\rho_{xy}(h)\neq\rho_{xy}(-h).
\]</span></p></li>
<li><p>It is the case, however, that <span class="math display">\[
\rho_{xy}(h)=\rho_{yx}(-h).
\]</span></p></li>
</ul>
</div>
<div id="example-joint-stationarity" class="section level3">
<h3>Example: Joint Stationarity</h3>
<ul>
<li><p>Consider <span class="math display">\[
x_{t}=w_{t}+w_{t-1}\quad\textrm{and}\quad y_{t}=w_{t}-w_{t-1}
\]</span> where <span class="math inline">\(w_{t}\)</span> are
independent random variables with zero means and variance <span
class="math inline">\(\sigma_{w}^2\)</span>.</p></li>
<li><p>We have <span class="math display">\[
\gamma_{x}(0)=\gamma_{y}(0)=2\sigma_{w}^2\quad \textrm{and}\quad
\gamma_{x}(1)=\gamma_{x}(-1)=\sigma_{w}^2,\;\gamma_{y}(1)=\gamma_{y}(-1)=-\sigma_{w}^2.
\]</span></p></li>
<li><p>Also, <span class="math display">\[
\gamma_{xy}(1)=\textrm{cov}(x_{t+1},y_{t})=\textrm{cov}(w_{t+1}+w_{t},w_{t}-w_{t-1})=\sigma_{w}^2.
\]</span></p></li>
<li><p>Similarly, <span class="math inline">\(\gamma_{xy}(0)=0\)</span>,
<span
class="math inline">\(\gamma_{xy}(-1)=-\sigma_{w}^2\)</span>.</p></li>
<li><p>Thus, <span class="math display">\[
\rho_{xy}(h)=\begin{cases}
0 &amp; h=0,\\
1/2 &amp; h=1,\\
-1/2 &amp; h=-1,\\
0 &amp; \left|h\right|\geq2.
\end{cases}.
\]</span></p></li>
<li><p>Hence, the autocovariance and cross-covariance functions depend
only on the lag separation, <span class="math inline">\(h\)</span>, so
the series are jointly stationary.</p></li>
</ul>
</div>
<div id="example-prediction-using-cross-correlation"
class="section level3">
<h3>Example: Prediction Using Cross-Correlation</h3>
<ul>
<li><p>Consider the problem of determining possible leading or lagging
relations between two series <span class="math inline">\(x_{t}\)</span>
and <span class="math inline">\(y_{t}\)</span>.</p></li>
<li><p>If the model <span class="math display">\[
y_{t}=Ax_{t-l}+w_{t}
\]</span> holds, the series <span class="math inline">\(x_{t}\)</span>
is said to <em>lead</em> <span class="math inline">\(y_{t}\)</span> for
<span class="math inline">\(l&gt;0\)</span> and is said to <em>lag</em>
<span class="math inline">\(y_{t}\)</span> for <span
class="math inline">\(l&lt;0\)</span>.</p></li>
<li><p>The analysis of leading and lagging relations might be important
in predicting the value of <span class="math inline">\(y_{t}\)</span>
from <span class="math inline">\(x_{t}\)</span>.</p></li>
<li><p>Assuming that the noise <span
class="math inline">\(w_{t}\)</span> is uncorrelated with <span
class="math inline">\(x_{t}\)</span>, we have <span
class="math display">\[
\begin{align*}
\gamma_{yx}(h) &amp;
=\textrm{cov}(y_{t+h},x_{t})=\textrm{cov}(Ax_{t+h-l}+w_{t+h},x_{t})\\
&amp; =\textrm{cov}(Ax_{t+h-l},x_{t})=A\gamma_{x}(h-l).
\end{align*}
\]</span></p></li>
<li><p>Since <span class="math inline">\(\left| \gamma_{x}(h-l)
\right|\leq\gamma_{x}(0)\)</span> the cross-covariance function will
look like the autocovariance of <span
class="math inline">\(x_{t}\)</span>, and it will have</p>
<ul>
<li>a peak on the positive side if <span
class="math inline">\(x_{t}\)</span> leads <span
class="math inline">\(y_{t}\)</span> and</li>
<li>a peak on the negative side if <span
class="math inline">\(x_{t}\)</span> lags <span
class="math inline">\(y_{t}\)</span>.</li>
</ul></li>
<li><p>An example where <span class="math inline">\(x_{t}\)</span> is
white noise, <span class="math inline">\(l=5\)</span>, and with <span
class="math inline">\(\hat{\gamma}_{yx}(h)\)</span> is</p></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">lag</span>(x, <span class="sc">-</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="fu">ccf</span>(y, x, <span class="at">ylab=</span><span class="st">&#39;CCovF&#39;</span>, <span class="at">type=</span><span class="st">&#39;covariance&#39;</span>)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">11</span>, .<span class="dv">9</span>, <span class="st">&#39;x leads&#39;</span>)</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="dv">9</span>, .<span class="dv">9</span>, <span class="st">&#39;y leads&#39;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-24-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="linear-process" class="section level3">
<h3>Linear Process</h3>
<blockquote>
<p>A linear process, <span class="math inline">\(x_{t}\)</span>, is
defined to be a linear combination of white noise variates <span
class="math inline">\(w_{t}\)</span>, and is given by <span
class="math display">\[
x_{t}=\mu+\sum_{j=-\infty}^{\infty}\psi_{j}w_{t-j},\quad
\sum_{j=-\infty}^{\infty}\left|\psi_{j}\right|&lt;\infty.
\]</span></p>
</blockquote>
<ul>
<li><p>The autocovariance function is given by <span
class="math display">\[
\gamma_{x}(h)=\sigma_{w}^{2}\sum_{j=-\infty}^{\infty}\psi_{j+h}\psi_{j}
\]</span> for <span class="math inline">\(h\geq0\)</span>.</p></li>
<li><p>If <span
class="math inline">\(\sum_{j=-\infty}^{\infty}\psi_{j}^{2}&lt;\infty\)</span>,
then the process have finite variance.</p></li>
<li><p>The linear process is dependent on the future (<span
class="math inline">\(j&lt;0\)</span>), the present (<span
class="math inline">\(j=0\)</span>), and the past (<span
class="math inline">\(j&gt;0\)</span>).</p></li>
<li><p>For the purpose of forecasting, a future dependent model will be
useless. Consequently, we will focus on processes that do not depend on
the future.</p></li>
<li><p>Such models are called <em>causal</em>, and a causal linear
process has <span class="math inline">\(\psi_{j}=0\)</span> for <span
class="math inline">\(j&lt;0\)</span>.</p></li>
</ul>
</div>
<div id="gaussian-process" class="section level3">
<h3>Gaussian Process</h3>
<ul>
<li>An important case in which a weakly stationary series is also
strictly stationary is the normal or Gaussian series.</li>
</ul>
<blockquote>
<p>A process, <span class="math inline">\(\{x_{t}\}\)</span>, is said to
be a Gaussian process if the <span
class="math inline">\(n\)</span>-dimensional vectors <span
class="math inline">\(x=(x_{t_{1}},x_{t_{2}},\ldots,x_{t_{n}})^{\prime}\)</span>,
for every collection of distinct time points <span
class="math inline">\(t_{1},t_{2},\ldots,t_{n}\)</span>, and every
positive integer <span class="math inline">\(n\)</span>, have a
multivariate normal distribution.</p>
</blockquote>
<ul>
<li><p>Defining the <span class="math inline">\(n\times 1\)</span> mean
vector <span
class="math inline">\(\textrm{E}(x)\equiv\mu=(\mu_{t_{1}},\mu_{t_{2}},\ldots,\mu_{t_{n}})^{\prime}\)</span>
and the <span class="math inline">\(n\times n\)</span> covariance matrix
as <span
class="math inline">\(\textrm{var}(x)\equiv\Gamma=\{\gamma(t_{i},t_{j});\;i,j=1,\ldots,n\}\)</span>
(positive definite) the multivariate normal density function can be
written as <span class="math display">\[
f(x)=(2\pi)^{-n/2}\left|\Gamma\right|^{-1/2}\exp\{-1/2(x-\mu)^{\prime}\Gamma^{-1}(x-\mu)\},
\]</span> for <span
class="math inline">\(x\in\mathbb{R}^{n}\)</span>.</p></li>
<li><p>We list some important items regarding linear and Gaussian
processes.</p>
<ul>
<li>If a Gaussian time series, <span
class="math inline">\(\{x_{t}\}\)</span>, is weakly stationary, then the
series must be strictly stationary.</li>
<li><em>Wold Decomposition</em> states that a stationary
non-deterministic time series is a causal linear process (but with <span
class="math inline">\(\sum\psi_{j}^{2}&lt;\infty\)</span>).</li>
<li>A linear process need not be Gaussian, but if a time series is
Gaussian, then it is a causal linear process with <span
class="math inline">\(w_{t}\sim \textrm{iid
N}(0,\sigma_{w}^{2})\)</span>.</li>
<li>It is not enough for the marginal distributions to be Gaussian for
the process to be Gaussian. It is easy to construct a situation where
<span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are normal, but <span
class="math inline">\((X,Y)\)</span> is not bivariate normal; e.g., let
<span class="math inline">\(X\)</span> and <span
class="math inline">\(Z\)</span> be independent normals and let <span
class="math display">\[
Y=\begin{cases}
Z &amp; XZ&gt;0,\\
-Z &amp; XZ\leq0.
\end{cases}
\]</span></li>
</ul></li>
</ul>
</div>
</div>
<div id="estimation-of-correlation" class="section level2">
<h2>Estimation of Correlation</h2>
<ul>
<li><p>The sampled points <span
class="math inline">\(x_{1},x_{2},\ldots,x_{n}\)</span> only are
available for estimating the mean, autocovariance, and au- tocorrelation
functions.</p></li>
<li><p>We will typically not have iid copies of <span
class="math inline">\(x_{t}\)</span> that are available for estimating
the covariance and correlation functions.</p></li>
<li><p>In the usual situation with only one realization, however, the
assumption of stationarity becomes critical.</p></li>
<li><p>Somehow, we must use averages over this single realization to
estimate the population means and covariance functions.</p></li>
<li><p>If a time series is stationary, the mean function <span
class="math inline">\(\mu_{t}=\mu\)</span> is constant so that we can
estimate it by the sample mean, <span class="math display">\[
\bar{x}=\frac{1}{n}\sum_{t=1}^{n}x_{t}.
\]</span></p></li>
<li><p>We have <span
class="math inline">\(\textrm{E}(\bar{x})=\mu\)</span> and the standard
error of the estimate is the square root of <span
class="math inline">\(\textrm{var}(\bar{x})\)</span>, <span
class="math display">\[
\begin{align*}
\textrm{var}(\bar{x}) &amp;
=\textrm{var}(\frac{1}{n}\sum_{t=1}^{n}x_{t})=\frac{1}{n^{2}}\textrm{cov}(\sum_{t=1}^{n}x_{t},\sum_{s=1}^{n}x_{s})\\
&amp;
=\frac{1}{n^{2}}(n\gamma_{x}(0)+(n-1)\gamma_{x}(1)+(n-2)\gamma_{x}(2)+\cdots+\gamma_{x}(n-1)\\
&amp; +(n-1)\gamma_{x}(-1)+(n-2)\gamma_{x}(-2)+\cdots+\gamma_{x}(1-n))\\
&amp;
=\frac{1}{n}\sum_{h=-n}^{n}(1-\frac{\left|h\right|}{n})\gamma_{x}(h).
\end{align*}
\]</span></p></li>
<li><p>If the process is white noise, it reduces to the familiar <span
class="math inline">\(\sigma_{x}^{2}/n\)</span> recalling that <span
class="math inline">\(\gamma_{x}(0)=\sigma_{x}^{2}\)</span>.</p></li>
<li><p>In the case of dependence, the standard error of <span
class="math inline">\(\bar{x}\)</span>̄ may be smaller or larger than
the white noise case depending on the nature of the correlation
structure.</p></li>
</ul>
<div id="sample-autocovariance-function" class="section level3">
<h3>Sample Autocovariance Function</h3>
<blockquote>
<p>The sample autocovariance function is defined as <span
class="math display">\[
\hat{\gamma}(h)=n^{-1}\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(x_{t}-\bar{x}),
\]</span> with <span
class="math inline">\(\hat{\gamma}(-h)=\hat{\gamma}(h)\)</span> for
<span class="math inline">\(h=0,1,\ldots,n-1\)</span>.</p>
</blockquote>
<ul>
<li><p>The sum runs over a restricted range because <span
class="math inline">\(x_{t+h}\)</span> is not available for <span
class="math inline">\(t+h&gt;n\)</span>.</p></li>
<li><p>We divide by <span class="math inline">\(n\)</span> instead <span
class="math inline">\(n-h\)</span> to obtain a non-negative definite
function ensuring that variances of linear combinations of the variates
<span class="math inline">\(x_{t}\)</span> will never be negative, that
is, <span class="math display">\[
0\leq\widehat{\textrm{var}}(a_{1}x_{1}+\cdots+a_{n}x_{n})=\sum_{j=1}^{n}\sum_{k=1}^{n}a_{j}a_{k}\hat{\gamma}(j-k).
\]</span></p></li>
<li><p>Neither dividing by <span class="math inline">\(n\)</span> nor
<span class="math inline">\(n-h\)</span> yields an unbiased estimator of
<span class="math inline">\(\gamma(h)\)</span>.</p></li>
</ul>
</div>
<div id="sample-autocorrelation-function" class="section level3">
<h3>Sample Autocorrelation Function</h3>
<blockquote>
<p>The sample autocorrelation function is defined as <span
class="math display">\[
\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}.
\]</span></p>
</blockquote>
</div>
<div id="example-sample-acf-and-scatterplots" class="section level3">
<h3>Example: Sample ACF and Scatterplots</h3>
<ul>
<li><p>We have pairs of observations, say (<span
class="math inline">\(x_{i},y_{i}\)</span>), for <span
class="math inline">\(i=1,\ldots,n\)</span>.</p></li>
<li><p>For example, if we have time series data <span
class="math inline">\(x_{t}\)</span> for <span
class="math inline">\(t=1,\ldots,n\)</span>, then the pairs of
observations for estimating <span class="math inline">\(\rho(h)\)</span>
are the <span class="math inline">\(n−h\)</span> pairs given by <span
class="math display">\[
\{(x_{t},x_{t+h});\;t=1,\ldots,n-h\}.
\]</span></p></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>(<span class="at">r =</span> <span class="fu">round</span>(<span class="fu">acf</span>(soi, <span class="dv">6</span>, <span class="at">plot=</span><span class="cn">FALSE</span>)<span class="sc">$</span>acf[<span class="sc">-</span><span class="dv">1</span>], <span class="dv">3</span>)) <span class="co"># first 6 sample acf values</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lag</span>(soi,<span class="sc">-</span><span class="dv">1</span>), soi); <span class="fu">legend</span>(<span class="st">&#39;topleft&#39;</span>, <span class="at">legend=</span>r[<span class="dv">1</span>])</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lag</span>(soi,<span class="sc">-</span><span class="dv">6</span>), soi); <span class="fu">legend</span>(<span class="st">&#39;topleft&#39;</span>, <span class="at">legend=</span>r[<span class="dv">6</span>])</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-25-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="large-sample-distribution-of-the-acf" class="section level3">
<h3>Large-Sample Distribution of the ACF</h3>
<ul>
<li><p>Under general conditions, if <span
class="math inline">\(x_{t}\)</span> is white noise, then for <span
class="math inline">\(n\)</span> large, the sample ACF, <span
class="math inline">\(\hat{\rho}_{x}(h)\)</span>, for <span
class="math inline">\(h=1,2,\ldots,H\)</span>, where <span
class="math inline">\(H\)</span> is fixed but arbitrary, is
approximately normally distributed with zero mean and standard deviation
given by <span class="math display">\[
\sigma_{\hat{\rho}_{x}(h)}=\frac{1}{\sqrt{n}}.
\]</span></p></li>
<li><p>The general conditions are that <span
class="math inline">\(x_{t}\)</span> is iid with finite fourth moment. A
sufficient condition for this to hold is that <span
class="math inline">\(x_{t}\)</span> is white Gaussian noise.</p></li>
</ul>
</div>
<div id="example-a-simulated-time-series" class="section level3">
<h3>Example: A Simulated Time Series</h3>
<ul>
<li><p>Consider a contrived set of data generated by tossing a fair
coin, letting <span class="math inline">\(x_{t}=1\)</span> when a head
is obtained and <span class="math inline">\(x_{t}=-1\)</span> when a
tail is obtained. Then, construct <span
class="math inline">\(y_{t}\)</span> as <span class="math display">\[
y_{t}=5+x_{t}-.7x_{t-1}.
\]</span></p></li>
<li><p>To simulate data, we consider two cases, one with a small sample
size (<span class="math inline">\(n=10\)</span>) and another with a
moderate sample size (<span
class="math inline">\(n=100\)</span>).</p></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101010</span>)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">rbinom</span>(<span class="dv">11</span>, <span class="dv">1</span>, .<span class="dv">5</span>) <span class="sc">-</span> <span class="dv">1</span> <span class="co"># simulated sequence of coin tosses</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">rbinom</span>(<span class="dv">101</span>, <span class="dv">1</span>, .<span class="dv">5</span>) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>y1 <span class="ot">=</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fu">filter</span>(x1, <span class="at">sides=</span><span class="dv">1</span>, <span class="at">filter=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span>.<span class="dv">7</span>))[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>y2 <span class="ot">=</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fu">filter</span>(x2, <span class="at">sides=</span><span class="dv">1</span>, <span class="at">filter=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span>.<span class="dv">7</span>))[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="fu">tsplot</span>(y1, <span class="at">type=</span><span class="st">&#39;s&#39;</span>)  <span class="co"># plot series</span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-26-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="fu">tsplot</span>(y2, <span class="at">type=</span><span class="st">&#39;s&#39;</span>)   </span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-26-2.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">mean</span>(y1), <span class="fu">mean</span>(y2))  <span class="co"># the sample means</span></span></code></pre></div>
<pre><code>## [1] 5.080 5.002</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="fu">acf</span>(y1, <span class="at">lag.max=</span><span class="dv">4</span>, <span class="at">plot=</span><span class="cn">FALSE</span>) <span class="co"># 1/sqrt(10) = .32 </span></span></code></pre></div>
<pre><code>## 
## Autocorrelations of series &#39;y1&#39;, by lag
## 
##      0      1      2      3      4 
##  1.000 -0.688  0.425 -0.306 -0.007</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="fu">acf</span>(y2, <span class="at">lag.max=</span><span class="dv">4</span>, <span class="at">plot=</span><span class="cn">FALSE</span>) <span class="co"># 1/sqrt(100)=.1</span></span></code></pre></div>
<pre><code>## 
## Autocorrelations of series &#39;y2&#39;, by lag
## 
##      0      1      2      3      4 
##  1.000 -0.480 -0.002 -0.004  0.000</code></pre>
<ul>
<li>The theoretical ACF can be obtained from the model using the fact
that <span class="math inline">\(\textrm{E}(x_{t})=0\)</span> and <span
class="math inline">\(\textrm{var}(x_{t})=1\)</span>. We have <span
class="math display">\[
\rho_{y}(1)=\frac{-.7}{1+.7^{2}}=-.47
\]</span> and <span class="math inline">\(\rho_{y}(h)=0\)</span> for
<span class="math inline">\(\left|h\right|&gt;1\)</span>.</li>
</ul>
</div>
<div id="example-acf-of-a-speech-signal" class="section level3">
<h3>Example: ACF of a Speech Signal</h3>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="fu">tsplot</span>(speech)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-3-ACF-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">acf1</span>(speech, <span class="dv">250</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-3-ACF-2.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The original series appears to contain a sequence of repeating
short signals.</p></li>
<li><p>The ACF confirms this behavior, showing repeating peaks spaced at
about <span class="math inline">\(106\)</span>-<span
class="math inline">\(109\)</span> points.</p></li>
<li><p>Autocorrelation functions of the short signals appear, spaced at
the intervals mentioned above.</p></li>
<li><p>The distance between the repeating signals is known as the pitch
period.</p></li>
<li><p>Because the series is sampled at <span
class="math inline">\(10,000\)</span> points per second, the pitch
period appears to be between <span class="math inline">\(.0106\)</span>
and <span class="math inline">\(.0109\)</span> seconds.</p></li>
</ul>
</div>
<div id="sample-cross-covariance-function" class="section level3">
<h3>Sample Cross-Covariance Function</h3>
<blockquote>
<p>The estimators for the cross-covariance function, <span
class="math inline">\(\gamma_{xy}(h)\)</span> and the cross-correlation,
<span class="math inline">\(\rho_{xy}(h)\)</span>ρ xy (h) are given,
respectively, by the sample cross-covariance function <span
class="math display">\[
\hat{\gamma}_{xy}(h)=n^{-1}\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(y_{t}-\bar{y}),
\]</span> where <span
class="math inline">\(\hat{\gamma}_{xy}(-h)=\hat{\gamma}_{yx}(h)\)</span>
determines the function for negative lags, and the sample
cross-correlation function <span class="math display">\[
\hat{\rho}_{xy}(h)=\frac{\hat{\gamma}_{xy}(h)}{\sqrt{\hat{\gamma}_{x}(0)\hat{\gamma}_{y}(0)}}.
\]</span></p>
</blockquote>
<ul>
<li>The sample cross-correlation function can be examined graphically as
a function of lag <span class="math inline">\(h\)</span> to search for
leading or lagging relations in the data for the theoretical
cross-covariance function.</li>
</ul>
</div>
<div id="large-sample-distribution-of-cross-correlation"
class="section level3">
<h3>Large-Sample Distribution of Cross-Correlation</h3>
<ul>
<li><p>Furthermore, for <span class="math inline">\(x_{t}\)</span> and
<span class="math inline">\(y_{t}\)</span> independent linear processes,
we have the following property.</p></li>
<li><p>The large sample distribution of <span
class="math inline">\(\hat{\rho}_{xy}(h)\)</span>) is normal with mean
zero and <span class="math display">\[
\sigma_{\hat{\rho}_{xy}}=\frac{1}{\sqrt{n}}
\]</span> if at least one of the processes is independent white
noise.</p></li>
</ul>
</div>
<div id="homework" class="section level3">
<h3>Homework</h3>
<ul>
<li>Read Example 1.28 SOI and Recruitment Correlation Analysis.</li>
</ul>
</div>
<div id="example-prewhitening-and-cross-correlation-analysis"
class="section level3">
<h3>Example: Prewhitening and Cross Correlation Analysis</h3>
<ul>
<li><p>The basic idea of prewhitening is that at least one of the series
must be white noise.</p></li>
<li><p>For example, we generated two series, <span
class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(y_{t}\)</span>, for <span
class="math inline">\(t=1,\ldots,120\)</span> independently as <span
class="math display">\[
x_{t}=2\cos(2\pi t\frac{1}{12})+w_{t_{1}}\quad \textrm{and}\quad
y_{t}=2\cos(2\pi [t+5]\frac{1}{12})+w_{t_{2}}
\]</span> where <span
class="math inline">\(\{w_{t_{1}},w_{t_{2}};\;t=1,\ldots,120\}\)</span>
are all independent standard normals.</p></li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1492</span>)</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>num<span class="ot">=</span><span class="dv">120</span>; t<span class="ot">=</span><span class="dv">1</span><span class="sc">:</span>num</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">ts</span>(<span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>t<span class="sc">/</span><span class="dv">12</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(num), <span class="at">freq=</span><span class="dv">12</span>)</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">ts</span>(<span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>(t<span class="sc">+</span><span class="dv">5</span>)<span class="sc">/</span><span class="dv">12</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(num), <span class="at">freq=</span><span class="dv">12</span>)</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>Yw <span class="ot">=</span> <span class="fu">resid</span>( <span class="fu">lm</span>(Y<span class="sc">~</span> <span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>t<span class="sc">/</span><span class="dv">12</span>) <span class="sc">+</span> <span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>t<span class="sc">/</span><span class="dv">12</span>), <span class="at">na.action=</span><span class="cn">NULL</span>) )</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>), <span class="at">mgp=</span><span class="fu">c</span>(<span class="fl">1.6</span>,.<span class="dv">6</span>,<span class="dv">0</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>) )</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a><span class="fu">tsplot</span>(X)</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a><span class="fu">tsplot</span>(Y)</span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a><span class="fu">acf1</span>(X, <span class="dv">48</span>, <span class="at">ylab=</span><span class="st">&#39;ACF(X)&#39;</span>)</span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a><span class="fu">acf1</span>(Y, <span class="dv">48</span>, <span class="at">ylab=</span><span class="st">&#39;ACF(Y)&#39;</span>)</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a><span class="fu">ccf2</span>(X, Y, <span class="dv">24</span>)</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a><span class="fu">ccf2</span>(X, Yw, <span class="dv">24</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">6</span>,.<span class="dv">6</span>))</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-29-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The bottom row (left) shows the sample CCF between <span
class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(y_{t}\)</span>, which appears to show
cross-correlation even though the series are independent.</p></li>
<li><p>The bottom row (right) also displays the sample CCF between <span
class="math inline">\(x_{t}\)</span> and the prewhitened <span
class="math inline">\(y_{t}\)</span>, which shows that the two sequences
are uncorrelated.</p></li>
<li><p>By prewhtiening <span class="math inline">\(y_{t}\)</span>, we
mean that the signal has been removed from the data by running a
regression of <span class="math inline">\(y_{t}\)</span> on <span
class="math inline">\(\cos(2\pi t)\)</span> and <span
class="math inline">\(\sin(2\pi t)\)</span> and then putting <span
class="math inline">\(\tilde{y}_{t}=y_{t}-\hat{y}_{t}\)</span>, where
<span class="math inline">\(\hat{y}_{t}\)</span> are the predicted
values from the regression.</p></li>
</ul>
</div>
<div id="homework-1" class="section level3">
<h3>Homework</h3>
<ul>
<li>Read Section 1.6 Vector-Valued and Multidimensional Series.</li>
</ul>
</div>
<div id="review" class="section level3">
<h3>Review</h3>
<ul>
<li>Appendix A Large Sample Theory.</li>
</ul>
</div>
</div>
</div>
<div id="time-series-regression-and-exploratory-data-analysis"
class="section level1">
<h1>Time series regression and exploratory data analysis</h1>
<div id="classical-regression-in-the-time-series-context"
class="section level2">
<h2>Classical Regression in the Time Series Context</h2>
<ul>
<li><p>We have some output or dependent time series, <span
class="math inline">\(x_{t}\)</span>, for <span
class="math inline">\(t=1,\ldots,n\)</span>, which is being influenced
by a collection of possible inputs or independent series, <span
class="math inline">\(z_{t_{1}},z_{t_{2}},\ldots,z_{t_{q}}\)</span> (we
first regard the inputs as fixed and known).</p></li>
<li><p>We express this relation through the linear regression model
<span class="math display">\[
x_{t}=\beta_{0}+\beta_{1}z_{t_{1}}+\beta_{2}z_{t_{2}}+\cdots+\beta_{q}z_{t_{q}}+w_{t},
\]</span> where <span
class="math inline">\(\beta_{0},\beta_{1},\ldots,\beta_{q}\)</span> are
unknown fixed regression coefficients, and <span
class="math inline">\(w_{t}\sim\textrm{iid N}(0,\sigma_{w}^{2})\)</span>
(we will can to relax this assumption).</p></li>
</ul>
<div id="example-estimating-a-linear-trend" class="section level3">
<h3>Example: Estimating a Linear Trend</h3>
<ul>
<li>Consider the monthly price (per pound) of a chicken in the US from
mid-2001 to mid-2016 (180 months), <span
class="math inline">\(x_{t}\)</span>.</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(chicken<span class="sc">~</span><span class="fu">time</span>(chicken)) <span class="co"># regress price on time</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="fu">tsplot</span>(chicken, <span class="at">ylab=</span><span class="st">&quot;cents per pound&quot;</span>, <span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a><span class="fu">abline</span>(fit)           <span class="co"># add the fitted regression line to the plot</span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-1-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>There is an obvious upward trend in the series, and we might use
simple linear regression to estimate that trend by fitting the model
<span class="math display">\[
x_{t}=\beta_{0}+\beta_{1}z_{t}+w_{t},\quad
z_{t}=2001\frac{7}{12},2001\frac{8}{12},\ldots,2016\frac{6}{12}.
\]</span></p></li>
<li><p>Note that we are making the assumption that the errors, <span
class="math inline">\(w_{t}\)</span>, are an iid normal sequence, which
may not be true (autocorrelated errors, next class).</p></li>
<li><p>In ordinary least squares (OLS), we minimize the error sum of
squares <span class="math display">\[
Q=\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}(x_{t}-[\beta_{0}+\beta_{1}z_{t}])^{2}
\]</span> with respect to <span class="math inline">\(\beta_{i}\)</span>
for <span class="math inline">\(i=0,1\)</span>.</p></li>
<li><p>We calculate <span class="math inline">\(\partial Q/\partial
\beta_{i}\)</span> for <span class="math inline">\(i=0,1\)</span>, to
obtain two equations to solve for the <span
class="math inline">\(\beta\)</span>s.</p></li>
<li><p>The OLS estimates of the coefficients are explicit and given by
<span class="math display">\[
\hat{\beta}_{1}=\frac{\sum_{t=1}^{n}(x_{t}-\bar{x})(z_{t}-\bar{z})}{\sum_{t=1}^{n}(z_{t}-\bar{z})^{2}}\quad\textrm{and}\quad
\hat{\beta}_{0}=\bar{x}-\hat{\beta}_{1}\bar{z},
\]</span> where <span
class="math inline">\(\bar{x}=\sum_{t}x_{t}/n\)</span> and <span
class="math inline">\(\bar{z}=\sum_{t}z_{t}/n\)</span> are the
respective sample means.</p></li>
<li><p>We obtained the estimated slope coefficient of <span
class="math inline">\(\hat{\beta}_{1}=3.59\)</span> (with a standard
error of <span class="math inline">\(.08\)</span>) yielding a
significant estimated increase of about <span
class="math inline">\(3.6\)</span> cents per year.</p></li>
</ul>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="fu">summary</span>(fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(chicken<span class="sc">~</span><span class="fu">time</span>(chicken))) <span class="co"># regress price on time</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = chicken ~ time(chicken))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7411 -3.4730  0.8251  2.7738 11.5804 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -7.131e+03  1.624e+02  -43.91   &lt;2e-16 ***
## time(chicken)  3.592e+00  8.084e-02   44.43   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.696 on 178 degrees of freedom
## Multiple R-squared:  0.9173, Adjusted R-squared:  0.9168 
## F-statistic:  1974 on 1 and 178 DF,  p-value: &lt; 2.2e-16</code></pre>
<hr />
<ul>
<li><p>The multiple linear regression model can be conveniently written
in a more general notation by defining the column vectors <span
class="math inline">\(z_{t}=(1,z_{t_{1}},z_{t_{2}},\ldots,z_{t_{q}})^{\prime}\)</span>
and <span
class="math inline">\(\beta=(\beta_{0},\beta_{1},\ldots,\beta_{q})^{\prime}\)</span>
as <span class="math display">\[
x_{t}=\beta_{0}+\beta_{1}z_{t_{1}}+\cdots+\beta_{q}z_{t_{q}}+w_{t}=\beta^{\prime}z_{t}+w_{t},
\]</span> where <span class="math inline">\(w_{t}\sim\textrm{iid
N}(0,\sigma_{w}^{2})\)</span>.</p></li>
<li><p>Again, OLS estimation finds the coefficient vector β that
minimizes the error sum of squares <span class="math display">\[
Q=\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}(x_{t}-\beta^{\prime}z_{t})^{2},
\]</span> with respect to <span
class="math inline">\(\beta_{0},\beta_{1},\ldots,\beta_{q}\)</span>.</p></li>
<li><p>The solution must satisfy <span
class="math inline">\(\sum_{t=1}^{n}(x_{t}-\hat{\beta}^{\prime}z_{t})z_{t}^{\prime}=0\)</span>.
This procedure gives the <em>normal equations</em> <span
class="math display">\[
(\sum_{t=1}^{n}z_{t}z_{t}^{\prime})\hat{\beta}=\sum_{t=1}^{n}z_{t}x_{t}.
\]</span></p></li>
<li><p>The minimized error sum of squares, denoted SSE, can be written
as <span class="math display">\[
SSE=\sum_{t=1}^{n}(x_{t}-\hat{\beta}^{\prime}z_{t})^{2}.
\]</span></p></li>
<li><p>The ordinary least squares estimators are unbiased (why?), i.e.,
<span class="math inline">\(\textrm{E}(\hat{\beta})=\beta\)</span>, and
have the smallest variance within the class of linear unbiased
estimators (Gauss–Markov theorem).</p></li>
<li><p>If the errors <span class="math inline">\(w_{t}\)</span> are
normally distributed, <span class="math inline">\(\hat{\beta}\)</span>
is also the maximum likelihood estimator for <span
class="math inline">\(\beta\)</span> and is normally distributed with
<span class="math display">\[
\textrm{cov}(\hat{\beta})=\sigma_{w}^{2}C,
\]</span> where <span class="math display">\[
C=(\sum_{t=1}^{n}z_{t}z_{t}^{\prime})^{-1}.
\]</span></p></li>
<li><p>An unbiased estimator for the variance <span
class="math inline">\(\sigma_{w}^{2}\)</span> is <span
class="math display">\[
s_{w}^{2}=MSE=\frac{SSE}{n-(q+1)},
\]</span> where <span class="math inline">\(MSE\)</span> denotes the
<em>mean squared error</em>.</p></li>
<li><p>Under the normal assumption, <span class="math display">\[
t=\frac{(\hat{\beta}_{i}-\beta_{i})}{s_{w}\sqrt{c_{ii}}}
\]</span> has the <span class="math inline">\(t\)</span>-distribution
with <span class="math inline">\(n−(q+1)\)</span> degrees of freedom;
<span class="math inline">\(c_{ii}\)</span> denotes the <span
class="math inline">\(i\)</span>-th diagonal element of <span
class="math inline">\(C\)</span>.</p></li>
<li><p>This result is often used for individual tests of the null
hypothesis <span
class="math inline">\(\textrm{H}_{0}:\beta_{i}=0\)</span> for <span
class="math inline">\(i=1,\ldots,q\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Various competing models are often of interest to isolate or
select the best subset of independent variables.</p></li>
<li><p>Suppose a proposed model specifies that only a subset <span
class="math inline">\(r&lt;q\)</span> independent variables, <span
class="math inline">\(z_{t_{1:r}}=\{z_{t_{1}},z_{t_{2}},\ldots,z_{t_{r}}\}\)</span>
is influencing the dependent variable <span
class="math inline">\(x_{t}\)</span>.</p></li>
<li><p>The reduced model is <span class="math display">\[
x_{t}=\beta_{0}+\beta_{1}z_{t_{1}}+\cdots+\beta_{r}z_{t_{r}}+w_{t}
\]</span> where <span
class="math inline">\(\beta_{1},\beta_{2},\ldots,\beta_{r}\)</span> are
a subset of coefficients of the original <span
class="math inline">\(q\)</span> variables.</p></li>
<li><p>The null hypothesis in this case is <span
class="math inline">\(\textrm{H}_{0}:\beta_{r+1}=\cdots=\beta_{q}=0\)</span>.</p></li>
<li><p>We can test the reduced model against the full model by comparing
the error sums of squares under the two models using the <span
class="math inline">\(F\)</span>-statistic <span class="math display">\[
F=\frac{(SSE_{r}-SSE)/(q-r)}{SSE/(n-q-1)}=\frac{MSR}{MSE},
\]</span> where <span class="math inline">\(SSE_{r}\)</span> is the
error sum of squares under the reduced model. Note that <span
class="math inline">\(SSE_{r}\geq SSE\)</span>.</p></li>
<li><p>If <span
class="math inline">\(\textrm{H}_{0}:\beta_{r+1}=\cdots=\beta_{q}=0\)</span>
is true, then <span class="math inline">\(SSE_{r}\approx SSE\)</span>.
Hence, we do not believe <span
class="math inline">\(\textrm{H}_{0}\)</span> if <span
class="math inline">\(SSR=SSE_{r}-SSE\)</span> is big.</p></li>
<li><p>Under the null hypothesis, <span class="math inline">\(F\)</span>
has a central <span class="math inline">\(F\)</span>-distribution with
<span class="math inline">\(q−r\)</span> and <span
class="math inline">\(n−q−1\)</span> degrees of freedom when the reduced
model is the correct model.</p></li>
</ul>
<hr />
<ul>
<li>These results are often summarized in an Analysis of Variance
(ANOVA) table as</li>
</ul>
<table style="width:100%;">
<colgroup>
<col width="19%" />
<col width="12%" />
<col width="21%" />
<col width="21%" />
<col width="24%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="left">df</th>
<th align="left">Sum of squares</th>
<th align="left">Mean square</th>
<th align="left"><span class="math inline">\(F\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(z_{t_{r+1:q}}\)</span></td>
<td align="left"><span class="math inline">\(q-r\)</span></td>
<td align="left"><span
class="math inline">\(SSR=SSE_{r}-SSE\)</span></td>
<td align="left"><span class="math inline">\(MSR=SSR/(q-r)\)</span></td>
<td align="left"><span
class="math inline">\(F=\frac{MSR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left"><span class="math inline">\(n-(q+1)\)</span></td>
<td align="left"><span class="math inline">\(SSE\)</span></td>
<td align="left"><span
class="math inline">\(MSE=SSE/(n-q-1)\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<ul>
<li><p>The null hypothesis is rejected at level <span
class="math inline">\(\alpha\)</span> if <span
class="math inline">\(F&gt;F_{n-q-1}^{q-r}(\alpha)\)</span>.</p></li>
<li><p>A special case of interest is the null hypothesis <span
class="math inline">\(\textrm{H}_{0}:\beta_{1}=\cdots=\beta_{q}=0\)</span>.
In this case <span class="math inline">\(r=0\)</span>, and the model is
<span class="math display">\[
x_{t}=\beta_{0}+w_{t}.
\]</span></p></li>
<li><p>The proportion of variation accounted for by all the variables
(<em>coefficient of determination</em>) is <span class="math display">\[
R^{2}=\frac{SSE_{0}-SSE}{SSE_{0}},
\]</span> where the residual sum of squares under the reduced model is
<span class="math display">\[
SSE_{0}=\sum_{t=1}^{n}(x_{t}-\bar{x})^{2}.
\]</span></p></li>
<li><p>The techniques discussed before can be used to test various
models against one another using the <span
class="math inline">\(F\)</span> test.</p></li>
<li><p>These tests have been used in the past in a stepwise manner,
where variables are added or deleted when the values from the <span
class="math inline">\(F\)</span>-test either exceed or fail to exceed
some predetermined levels (stepwise multiple regression).</p></li>
</ul>
<hr />
<ul>
<li>An alternative is to focus on a procedure for model selection that
does not proceed sequentially, but simply evaluates each model on its
own merits.</li>
</ul>
</div>
<div id="akaikes-information-criterion-aic" class="section level3">
<h3>Akaike’s Information Criterion (AIC)</h3>
<ul>
<li><p>Suppose we consider a normal regression model with <span
class="math inline">\(k\)</span> coefficients and denote the maximum
likelihood estimator for the variance as <span class="math display">\[
\hat{\sigma}_{k}^{2}=\frac{SSE(k)}{n},
\]</span> where <span class="math inline">\(SSE(k)\)</span> denotes the
residual sum of squares under the model with <span
class="math inline">\(k\)</span> regression coefficients.</p></li>
<li><p>Then, Akaike (1969, 1973, 1974) suggested measuring the goodness
of fit for this particular model by balancing the error of the fit
against the number of parameters in the model.</p></li>
</ul>
<blockquote>
<p>The Akaike’s information criterion is <span class="math display">\[
\textrm{AIC}=\log\hat{\sigma}_{k}^{2}+\frac{n+2k}{n},
\]</span> where <span
class="math inline">\(\hat{\sigma}_{k}^{2}\)</span> is the residual sum
of squares under the model with <span class="math inline">\(k\)</span>
regression coefficients.</p>
</blockquote>
<ul>
<li>The value of <span class="math inline">\(k\)</span> yielding the
minimum <span class="math inline">\(\textrm{AIC}\)</span> specifies the
best model.</li>
</ul>
</div>
<div id="aic-bias-corrected-aicc" class="section level3">
<h3>AIC, Bias Corrected (AICc)</h3>
<ul>
<li>A corrected form, suggested by Sugiura (1978), and expanded by
Hurvich and Tsai (1989), can be based on small-sample distributional
results for the linear regression model.</li>
</ul>
<blockquote>
<p>The AIC, bias corrected is <span class="math display">\[
\textrm{AICc}=\log\hat{\sigma}_{k}^{2}+\frac{n+k}{n-k-2}.
\]</span></p>
</blockquote>
</div>
<div id="bayesian-information-criterion-bic" class="section level3">
<h3>Bayesian Information Criterion (BIC)</h3>
<ul>
<li>There is a correction term based on Bayesian arguments, as in
Schwarz (1978).</li>
</ul>
<blockquote>
<p>The bayesian information criterion is <span class="math display">\[
\textrm{BIC}=\log\hat{\sigma}_{k}^{2}+\frac{k\log n}{n}.
\]</span></p>
</blockquote>
<ul>
<li><p><span class="math inline">\(\textrm{BIC}\)</span> is also called
the Schwarz Information Criterion (SIC).</p></li>
<li><p>The penalty term in <span
class="math inline">\(\textrm{BIC}\)</span> is much larger than in <span
class="math inline">\(\textrm{AIC}\)</span>, consequently, <span
class="math inline">\(\textrm{BIC}\)</span> tends to choose smaller
models.</p></li>
<li><p>Various simulation studies have tended to verify that <span
class="math inline">\(\textrm{BIC}\)</span> does well at getting the
correct order in large samples, whereas <span
class="math inline">\(\textrm{AICc}\)</span> tends to be superior in
smaller samples where the relative number of parameters is large
(McQuarrie and Tsai, 1998).</p></li>
</ul>
</div>
<div id="example-pollution-temperature-and-mortality"
class="section level3">
<h3>Example: Pollution, Temperature and Mortality</h3>
<ul>
<li>The data shown the possible effects of temperature and pollution on
weekly mortality in Los Angeles County.</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="fu">tsplot</span>(cmort, <span class="at">main=</span><span class="st">&quot;Cardiovascular Mortality&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a><span class="fu">tsplot</span>(tempr, <span class="at">main=</span><span class="st">&quot;Temperature&quot;</span>,  <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a><span class="fu">tsplot</span>(part, <span class="at">main=</span><span class="st">&quot;Particulates&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-2-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>Note
<ul>
<li>the strong seasonal components in all of the series, corresponding
to winter-summer variations and</li>
<li>the downward trend in the cardiovascular mortality over the 10-year
period.</li>
</ul></li>
<li>A scatterplot matrix, indicates a possible linear relation between
mortality and the pollutant particulates and a possible relation to
temperature.</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="fu">pairs</span>(<span class="fu">cbind</span>(<span class="at">Mortality=</span>cmort, <span class="at">Temperature=</span>tempr, <span class="at">Particulates=</span>part))</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-2-scatterplot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>Note the curvilinear shape of the temperature mortality
curve.</p></li>
<li><p>We propose four models where <span
class="math inline">\(M_{t}\)</span> denotes cardiovascular mortality,
<span class="math inline">\(T_{t}\)</span> denotes temperature and <span
class="math inline">\(P_{t}\)</span> denotes the particulate levels.
They are <span class="math display">\[
\begin{align*}
M_{t} &amp; =\beta_{0}+\beta_{1}t+w_{t}\quad\textrm{(trend only
model)}\\
M_{t} &amp;
=\beta_{0}+\beta_{1}t+\beta_{2}(T_{t}-T_{\cdot})+w_{t}\quad\textrm{(linear
temperature)}\\
M_{t} &amp;
=\beta_{0}+\beta_{1}t+\beta_{2}(T_{t}-T_{\cdot})+\beta_{3}(T_{t}-T_{\cdot})^{2}+w_{t}\quad\textrm{(curvilinear
temperature)}\\
M_{t} &amp;
=\beta_{0}+\beta_{1}t+\beta_{2}(T_{t}-T_{\cdot})+\beta_{3}(T_{t}-T_{\cdot})^{2}+\beta_{4}P_{t}+w_{t}\quad\textrm{(curvilinear
temperature and pollution)}
\end{align*}
\]</span></p></li>
</ul>
<table>
<colgroup>
<col width="7%" />
<col width="5%" />
<col width="17%" />
<col width="5%" />
<col width="17%" />
<col width="10%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="left"><span class="math inline">\(k\)</span></th>
<th align="left"><span class="math inline">\(\textrm{SSE}\)</span></th>
<th align="left">df</th>
<th align="left"><span class="math inline">\(\textrm{MSE}\)</span></th>
<th align="left"><span class="math inline">\(R^{2}\)</span></th>
<th align="left"><span class="math inline">\(\textrm{AIC}\)</span></th>
<th align="left"><span class="math inline">\(\textrm{BIC}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">2</td>
<td align="left"><span class="math inline">\(40,020\)</span></td>
<td align="left">506</td>
<td align="left"><span class="math inline">\(79.0\)</span></td>
<td align="left"><span class="math inline">\(.21\)</span></td>
<td align="left"><span class="math inline">\(5.38\)</span></td>
<td align="left"><span class="math inline">\(5.40\)</span></td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">3</td>
<td align="left"><span class="math inline">\(31,413\)</span></td>
<td align="left">505</td>
<td align="left"><span class="math inline">\(62.2\)</span></td>
<td align="left"><span class="math inline">\(.38\)</span></td>
<td align="left"><span class="math inline">\(5.14\)</span></td>
<td align="left"><span class="math inline">\(5.17\)</span></td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">4</td>
<td align="left"><span class="math inline">\(27,985\)</span></td>
<td align="left">504</td>
<td align="left"><span class="math inline">\(55.5\)</span></td>
<td align="left"><span class="math inline">\(.45\)</span></td>
<td align="left"><span class="math inline">\(5.03\)</span></td>
<td align="left"><span class="math inline">\(5.07\)</span></td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">5</td>
<td align="left"><span class="math inline">\(20,508\)</span></td>
<td align="left">503</td>
<td align="left"><span class="math inline">\(40.8\)</span></td>
<td align="left"><span class="math inline">\(.60\)</span></td>
<td align="left"><span class="math inline">\(4.72\)</span></td>
<td align="left"><span class="math inline">\(4.77\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>The best model (AIC, AICc and BIC) is the model including
temperature, temperature squared, and particulates, accounting for some
<span class="math inline">\(60\%\)</span> of the variability.</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="co">#  Regression</span></span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>temp  <span class="ot">=</span> tempr<span class="sc">-</span><span class="fu">mean</span>(tempr)  <span class="co"># center temperature    </span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>temp2 <span class="ot">=</span> temp<span class="sc">^</span><span class="dv">2</span>             <span class="co"># square it  </span></span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>trend <span class="ot">=</span> <span class="fu">time</span>(cmort)        <span class="co"># time</span></span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(cmort<span class="sc">~</span> trend <span class="sc">+</span> temp <span class="sc">+</span> temp2 <span class="sc">+</span> part, <span class="at">na.action=</span><span class="cn">NULL</span>)</span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>            </span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a><span class="fu">summary</span>(fit)       <span class="co"># regression results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = cmort ~ trend + temp + temp2 + part, na.action = NULL)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.0760  -4.2153  -0.4878   3.7435  29.2448 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.831e+03  1.996e+02   14.19  &lt; 2e-16 ***
## trend       -1.396e+00  1.010e-01  -13.82  &lt; 2e-16 ***
## temp        -4.725e-01  3.162e-02  -14.94  &lt; 2e-16 ***
## temp2        2.259e-02  2.827e-03    7.99 9.26e-15 ***
## part         2.554e-01  1.886e-02   13.54  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.385 on 503 degrees of freedom
## Multiple R-squared:  0.5954, Adjusted R-squared:  0.5922 
## F-statistic:   185 on 4 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
<ul>
<li><p>We obtain the best prediction model, <span
class="math display">\[
\widehat{M}_{t}=2831.5-1.396_{(.10)}t-.472_{(.032)}(T_{t}-74.26)+.023_{(.003)}(T_{t}-74.26)^{2}+.255_{(.019)}P_{t},
\]</span> for mortality.</p></li>
<li><p>As expected, a negative trend is present in time as well as a
negative coefficient for adjusted temperature.</p></li>
<li><p>Pollution weights positively and can be interpreted as the
incremental contribution to daily deaths per unit of particulate
pollution.</p></li>
<li><p>It would still be essential to check the residuals <span
class="math inline">\(\hat{w}_{t}=M_{t}-\widehat{M}_{t}\)</span> for
autocorrelation (next class).</p></li>
</ul>
</div>
<div id="example-regression-with-lagged-variables"
class="section level3">
<h3>Example: Regression With Lagged Variables</h3>
<ul>
<li>We considered simultaneous monthly readings of the SOI and the
number of new fish (Recruitment).</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="fu">tsplot</span>(soi, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Southern Oscillation Index&quot;</span>)</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a><span class="fu">tsplot</span>(rec, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Recruitment&quot;</span>) </span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-5-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>The autocorrelation and cross-correlation functions (ACFs and CCF)
for these two series are</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a><span class="fu">acf1</span>(soi, <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;Southern Oscillation Index&quot;</span>)</span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a><span class="fu">acf1</span>(rec, <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;Recruitment&quot;</span>)</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a><span class="fu">ccf2</span>(soi, rec, <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;SOI vs Recruitment&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-28-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The Southern Oscillation Index (SOI) measured at time <span
class="math inline">\(t−6\)</span> months is associated with the
Recruitment series at time <span class="math inline">\(t\)</span>,
indicating that the SOI leads the Recruitment series by six
months.</p></li>
<li><p>We consider the following regression <span
class="math display">\[
R_{t}=\beta_{0}+\beta_{1}S_{t-6}+w_{t},
\]</span> where <span class="math inline">\(R_{t}\)</span> denotes
Recruitment for month <span class="math inline">\(t\)</span> and <span
class="math inline">\(S_{t-6}\)</span> denotes SOI six months
prior.</p></li>
<li><p>Assuming the <span class="math inline">\(w_{t}\)</span> sequence
is white, the fitted model is <span class="math display">\[
\hat{R}_{t}=65.79-44.28_{(2.78)}S_{t-6}
\]</span> with <span
class="math inline">\(\hat{\sigma}_{w}=22.5\)</span> on <span
class="math inline">\(445\)</span> degrees of freedom.</p></li>
</ul>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>fish <span class="ot">=</span> <span class="fu">ts.intersect</span>(rec, <span class="at">soiL6=</span><span class="fu">lag</span>(soi,<span class="sc">-</span><span class="dv">6</span>), <span class="at">dframe=</span><span class="cn">TRUE</span>)   </span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="fu">summary</span>(fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(rec<span class="sc">~</span>soiL6, <span class="at">data=</span>fish, <span class="at">na.action=</span><span class="cn">NULL</span>))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = rec ~ soiL6, data = fish, na.action = NULL)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -65.187 -18.234   0.354  16.580  55.790 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   65.790      1.088   60.47   &lt;2e-16 ***
## soiL6        -44.283      2.781  -15.92   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22.5 on 445 degrees of freedom
## Multiple R-squared:  0.3629, Adjusted R-squared:  0.3615 
## F-statistic: 253.5 on 1 and 445 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="exploratory-data-analysis" class="section level2">
<h2>Exploratory Data Analysis</h2>
<ul>
<li><p>It would be difficult to measure the dependence if the dependence
structure is not regular or is changing at every time point.</p></li>
<li><p>A number of our examples came from clearly nonstationary
series.</p></li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="fu">tsplot</span>(jj, <span class="at">type=</span><span class="st">&quot;o&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Quarterly Earnings per Share&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-1-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>Note that,
<ul>
<li>it has a mean that increases exponentially over time, and</li>
<li>the increase in the magnitude of the fluctuations around this trend
causes changes in the covariance function; the variance of the process,
for example, clearly increases as one progresses over the length of the
series.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="fu">tsplot</span>(gtemp_both, <span class="at">type=</span><span class="st">&quot;o&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Global Temperature Deviations&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-2-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Note that, - it contains some evidence of a trend over time;
human-induced global warming advocates seize on this as empirical
evidence to advance the hypothesis that temperatures are increasing.</p>
<hr />
<ul>
<li><p>Perhaps the easiest form of nonstationarity to work with is the
trend stationary model wherein the process has stationary behavior
around a trend.</p></li>
<li><p>The model is <span class="math display">\[
x_{t}=\mu_{t}+y_{t}
\]</span> where <span class="math inline">\(x_{t}\)</span> are the
observations, <span class="math inline">\(\mu_{t}\)</span> denotes the
trend, and <span class="math inline">\(y_{t}\)</span> is a stationary
process.</p></li>
<li><p>There is some advantage to removing the trend as a first step in
an exploratory analysis of such time series.</p></li>
<li><p>The steps involved are to obtain a reasonable estimate of the
trend component, say <span class="math inline">\(\hat{\mu}_{t}\)</span>,
and then work with the residuals <span class="math display">\[
\hat{y}_{t}=x_{t}-\hat{\mu}_{t}.
\]</span></p></li>
</ul>
<div id="example-detrending-chicken-prices" class="section level3">
<h3>Example: Detrending Chicken Prices</h3>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(chicken<span class="sc">~</span><span class="fu">time</span>(chicken)) <span class="co"># regress price on time</span></span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a><span class="fu">tsplot</span>(chicken, <span class="at">ylab=</span><span class="st">&quot;cents per pound&quot;</span>, <span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a><span class="fu">abline</span>(fit)           <span class="co"># add the fitted regression line to the plot</span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-1-plot-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>We suppose the model is <span class="math display">\[
x_{t}=\mu_{t}+y_{t},
\]</span> where a straight line might be useful for detrending the data;
i.e., <span class="math display">\[
\mu_{t}=\beta_{0}+\beta_{1}t.
\]</span></p></li>
<li><p>We estimated the trend using OLS and found <span
class="math display">\[
\hat{\mu}_{t}=-7131+3.59t.
\]</span></p></li>
<li><p>We obtain the detrended series subtracting <span
class="math inline">\(\hat{\mu}_{t}\)</span> from the observations,
<span class="math inline">\(x_{t}\)</span>, to obtain the detrended
series <span class="math display">\[
\hat{y}_{t}=x_{t}+7131-3.59t.
\]</span></p></li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(chicken<span class="sc">~</span><span class="fu">time</span>(chicken), <span class="at">na.action=</span><span class="cn">NULL</span>) <span class="co"># regress chicken on time</span></span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">resid</span>(fit), <span class="at">main=</span><span class="st">&quot;detrended&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-4-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))     <span class="co"># plot ACFs</span></span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a><span class="fu">acf1</span>(chicken, <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;chicken&quot;</span>)</span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a><span class="fu">acf1</span>(<span class="fu">resid</span>(fit), <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;detrended&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-4-2.png" width="768" style="display: block; margin: auto;" /></p>
<hr />
<ul>
<li><p>Rather than modeling trend as fixed, we might model trend as a
stochastic component using the random walk with drift model, <span
class="math display">\[
\mu_{t}=\delta+\mu_{t-1}+w_{t},
\]</span> where <span class="math inline">\(w_{t}\)</span> is white
noise and is independent of <span
class="math inline">\(y_{t}\)</span>.</p></li>
<li><p>If the appropriate model is <span
class="math inline">\(x_{t}=\mu_{t}+y_{t}\)</span>, then
<em>differencing</em> the data, <span
class="math inline">\(x_{t}\)</span>, yields a stationary process; that
is, <span class="math display">\[
\begin{align*}
x_{t}-x_{t-1} &amp; =(\mu_{t}+y_{t})-(\mu_{t-1}+y_{t-1})\\
&amp; =\delta+w_{t}+y_{t}-y_{t-1}.
\end{align*}
\]</span> where <span class="math inline">\(z_{t}=y_{t}-y_{t-1}\)</span>
is stationary (why?). Indeed, we have <span
class="math inline">\(x_{t}-x_{t-1}\)</span> is stationary
(why?).</p></li>
<li><p>One advantage of differencing over detrending to remove trend is
that no parameters are estimated in the differencing operation.</p></li>
<li><p>One disadvantage, however, is that differencing does not yield an
estimate of the stationary process <span
class="math inline">\(y_{t}\)</span>. As before, <span
class="math inline">\(x_{t}-x_{t-1}=\delta+w_{t}+y_{t}-y_{t-1}\)</span>.</p></li>
<li><p>If an estimate of <span class="math inline">\(y_{t}\)</span> is
essential, then detrending may be more appropriate.</p></li>
<li><p>If the goal is to coerce the data to stationarity, then
differencing may be more appropriate.</p></li>
<li><p>Differencing is also a viable tool if the trend is fixed. For
example, if <span
class="math inline">\(\mu_{t}=\beta_{0}+\beta_{1}t\)</span> in the model
<span class="math inline">\(x_{t}=\mu_{t}+y_{t}\)</span>, differencing
the data produces stationarity <span class="math display">\[
x_{t}-x_{t-1}=(\mu_{t}+y_{t})-(\mu_{t-1}+y_{t-1})=\beta_{1}+y_{t}-y_{t-1}.
\]</span></p></li>
<li><p>The first difference is denoted as <span class="math display">\[
\nabla x_{t}=x_{t}-x_{t-1}.
\]</span></p></li>
<li><p>As we have seen,</p>
<ul>
<li>the first difference eliminates a linear trend.</li>
<li>A second difference, that is, <span class="math inline">\(\nabla
(\nabla x_{t})\)</span> can eliminate a quadratic trend, and so on.</li>
</ul></li>
</ul>
</div>
<div id="backshift-operator" class="section level3">
<h3>Backshift Operator</h3>
<blockquote>
<p>We define the backshift operator by <span class="math display">\[
B x_{t}=x_{t-1}
\]</span> and extend it to powers <span class="math inline">\(B^{2}
x_{t}=B(Bx_{t})=Bx_{t-1}=x_{t-2}\)</span>, and so on. Thus, <span
class="math display">\[
B^{k} x_{t}=x_{t-k}.
\]</span></p>
</blockquote>
<ul>
<li><p>The idea of an inverse operator can also be given if we require
<span class="math inline">\(B^{-1}B=1\)</span>, so that <span
class="math display">\[
x_{t}=B^{-1}B x_{t}=B^{-1}x_{t-1}.
\]</span> That is, <span class="math inline">\(B^{-1}\)</span> is the
<em>forward-shift operator</em>.</p></li>
<li><p>Thus, <span class="math display">\[
\nabla x_{t}=(1-B)x_{t}.
\]</span></p></li>
<li><p>For example, the second difference becomes <span
class="math display">\[
\nabla^{2}x_{t}=(1-B)^{2}x_{t}=(1-2B+B^{2})x_{t}=x_{t}-2x_{t-1}+x_{t-2}
\]</span> by the linearity of the operator.</p></li>
</ul>
</div>
<div id="differences-of-order-d" class="section level3">
<h3>Differences of Order <span class="math inline">\(d\)</span></h3>
<blockquote>
<p>Differences of order d are defined as <span class="math display">\[
\nabla^{d}=(1-B)^{d},
\]</span> where we may expand the operator <span
class="math inline">\((1−B)^{d}\)</span> algebraically to evaluate for
higher integer values of <span class="math inline">\(d\)</span>.</p>
</blockquote>
</div>
<div id="example-differencing-chicken-prices" class="section level3">
<h3>Example: Differencing Chicken Prices</h3>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="fu">tsplot</span>(chicken, <span class="at">ylab=</span><span class="st">&quot;cents per pound&quot;</span>, <span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-1-plot-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(chicken<span class="sc">~</span><span class="fu">time</span>(chicken), <span class="at">na.action=</span><span class="cn">NULL</span>) <span class="co"># regress chicken on time</span></span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">resid</span>(fit), <span class="at">main=</span><span class="st">&quot;detrended&quot;</span>)</span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">diff</span>(chicken), <span class="at">main=</span><span class="st">&quot;first difference&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-5-plot-1-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>The differenced series does not contain the long (five-year) cycle
we observe in the detrended series.</li>
</ul>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>))     <span class="co"># plot ACFs</span></span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a><span class="fu">acf1</span>(chicken, <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;chicken&quot;</span>)</span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a><span class="fu">acf1</span>(<span class="fu">resid</span>(fit), <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;detrended&quot;</span>)</span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a><span class="fu">acf1</span>(<span class="fu">diff</span>(chicken), <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;first difference&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-5-plot-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>The differenced series exhibits an annual cycle that was obscured in
the original or detrended data.</li>
</ul>
</div>
<div id="differencing-global-temperature" class="section level3">
<h3>Differencing Global Temperature</h3>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a><span class="fu">tsplot</span>(gtemp_both, <span class="at">type=</span><span class="st">&quot;o&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Global Temperature Deviations&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-2-plot-1-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>It appears to behave more as a random walk than a trend stationary
series.</li>
</ul>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">diff</span>(gtemp_both), <span class="at">type=</span><span class="st">&quot;o&quot;</span>)</span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a> <span class="fu">mean</span>(<span class="fu">diff</span>(gtemp_both))     <span class="co"># drift estimate = .008</span></span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a><span class="fu">acf1</span>(<span class="fu">diff</span>(gtemp_both), <span class="dv">48</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-6-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>It appears that the differenced process shows minimal
autocorrelation, which may imply the global temperature series is nearly
a random walk with drift.</p></li>
<li><p>It is interesting to note that if the series is a random walk
with drift, the mean of the differenced series, which is an estimate of
the drift, is about <span class="math inline">\(.008\)</span>, or an
increase of about one degree centigrade per <span
class="math inline">\(100\)</span> years.</p></li>
</ul>
</div>
<div id="power-transformations" class="section level3">
<h3>Power Transformations</h3>
<ul>
<li><p>Transformations may be useful to equalize the variability over
the length of a single series.</p></li>
<li><p>For example, <span class="math display">\[
y_{t}=\log x_{t},
\]</span> which tends to suppress larger fluctuations that occur over
portions of the series where the underlying values are larger.</p></li>
<li><p>Other possibilities are power transformations in the Box–Cox
family of the form <span class="math display">\[
y_{t}=\begin{cases}
(x_{t}^{\lambda}-1)/\lambda &amp; \lambda\neq0,\\
\log x_{t} &amp; \lambda=0.
\end{cases}
\]</span></p></li>
<li><p>Often, transformations are also used to improve the approximation
to normality or to improve linearity in predicting the value of one
series from another.</p></li>
</ul>
</div>
<div id="example-paleoclimatic-glacial-varves" class="section level3">
<h3>Example: Paleoclimatic Glacial Varves</h3>
<ul>
<li>Varves (sedimentary deposits) can be used as proxies for
paleoclimatic parameters, such as temperature.</li>
</ul>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a><span class="fu">tsplot</span>(varve, <span class="at">main=</span><span class="st">&quot;varve&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-7-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>The variation in thicknesses increases in proportion to the amount
deposited, a logarithmic transformation could remove the nonstationarity
observable in the variance as a function of time.</li>
</ul>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">log</span>(varve), <span class="at">main=</span><span class="st">&quot;log(varve)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span> )</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-7-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="scatterplot-matrices" class="section level3">
<h3>Scatterplot Matrices</h3>
<ul>
<li><p>It is another preliminary data processing technique that is used
for the purpose of visualizing the relations between series at different
lags.</p></li>
<li><p>The ACF gives a profile of the linear correlation at all possible
lags and shows which values of <span class="math inline">\(h\)</span>
lead to the best predictability.</p></li>
<li><p>The restriction of this idea to linear predictability, however,
may mask a possible nonlinear relation between current values, <span
class="math inline">\(x_{t}\)</span>, and past values, <span
class="math inline">\(x_{t-h}\)</span>.</p></li>
<li><p>This idea extends to two series where one may be interested in
examining scatterplots of <span class="math inline">\(y_{t}\)</span>
versus <span class="math inline">\(x_{t-h}\)</span>.</p></li>
</ul>
</div>
<div id="example-scatterplot-matrices-soi-and-recruitment"
class="section level3">
<h3>Example: Scatterplot Matrices, SOI and Recruitment</h3>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb49-2"><a href="#cb49-2" tabindex="-1"></a><span class="fu">tsplot</span>(soi, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Southern Oscillation Index&quot;</span>)</span>
<span id="cb49-3"><a href="#cb49-3" tabindex="-1"></a><span class="fu">tsplot</span>(rec, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Recruitment&quot;</span>) </span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-5-plot-1-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a><span class="fu">lag1.plot</span>(soi, <span class="dv">12</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="Slides_files/figure-html/example2-8-plot-1-1.png" alt="Scatterplot matrix relating current SOI values, $S_{t}$, to past SOI values, $S_{t-h}$, at lags $h=1,2,...,12$." width="768" />
<p class="caption">
Scatterplot matrix relating current SOI values, <span
class="math inline">\(S_{t}\)</span>, to past SOI values, <span
class="math inline">\(S_{t-h}\)</span>, at lags <span
class="math inline">\(h=1,2,...,12\)</span>.
</p>
</div>
<ul>
<li><p>We notice that the lowess fits are approximately linear, so that
the sample autocorrelations are meaningful.</p></li>
<li><p>We see strong positive linear relations at lags <span
class="math inline">\(h=1,2,11,12\)</span>, that is, between <span
class="math inline">\(S_{t}\)</span> and <span
class="math inline">\(S_{t-1}\)</span>, <span
class="math inline">\(S_{t-2}\)</span>, <span
class="math inline">\(S_{t-11}\)</span>, <span
class="math inline">\(S_{t-12}\)</span>, and a negative linear relation
at lags <span class="math inline">\(h=6,7\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" tabindex="-1"></a><span class="fu">lag2.plot</span>(soi, rec, <span class="dv">8</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="Slides_files/figure-html/example2-8-plot-2-1.png" alt="Scatterplot matrix of the Recruitment series, $R_{t}$, on the vertical axis plotted against the SOI series, $S_{t-h}$, on the horizontal axis at lags $h=0,1,...,8$." width="768" />
<p class="caption">
Scatterplot matrix of the Recruitment series, <span
class="math inline">\(R_{t}\)</span>, on the vertical axis plotted
against the SOI series, <span class="math inline">\(S_{t-h}\)</span>, on
the horizontal axis at lags <span
class="math inline">\(h=0,1,...,8\)</span>.
</p>
</div>
<ul>
<li><p>It shows a fairly strong nonlinear relationship between
Recruitment, <span class="math inline">\(R_{t}\)</span>, and the SOI
series at <span class="math inline">\(S_{t-5}\)</span>, <span
class="math inline">\(S_{t-6}\)</span>, <span
class="math inline">\(S_{t-7}\)</span>, <span
class="math inline">\(S_{t-8}\)</span>, indicating the SOI series tends
to lead the Recruitment series (negatively).</p></li>
<li><p>The nonlinearity observed in the scatterplots indicates that the
behavior between Recruitment and the SOI is different for positive
values of SOI than for negative values of SOI.</p></li>
</ul>
</div>
<div id="example-regression-with-lagged-variables-cont"
class="section level3">
<h3>Example: Regression with Lagged Variables (cont)</h3>
<ul>
<li><p>We had proposed <span class="math display">\[
R_{t}=\beta_{0}+\beta_{1}S_{t-6}+w_{t}.
\]</span> However, before we saw that the relationship is nonlinear and
different when SOI is positive or negative.</p></li>
<li><p>We may consider adding a dummy variable to account for this
change through <span class="math display">\[
R_{t}=\beta_{0}+\beta_{1}S_{t-6}+\beta_{2}D_{t-6}+\beta_{3}D_{t-6}S_{t-6}+w_{t},
\]</span> where <span class="math inline">\(D_{t}\)</span> is a dummy
variable that is <span class="math inline">\(0\)</span> if <span
class="math inline">\(S_{t}&lt;0\)</span> and <span
class="math inline">\(1\)</span> otherwise.</p></li>
<li><p>This means that <span class="math display">\[
R_{t}=\begin{cases}
\beta_{0}+\beta_{1}S_{t-6}+w_{t} &amp; \textrm{if}\;S_{t-6}&lt;0,\\
(\beta_{0}+\beta_{2})+(\beta_{1}+\beta_{3})S_{t-6}+w_{t} &amp;
\textrm{if}\;S_{t-6}\geq0.
\end{cases}
\]</span></p></li>
</ul>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a>dummy <span class="ot">=</span> <span class="fu">ifelse</span>(soi<span class="sc">&lt;</span><span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb52-2"><a href="#cb52-2" tabindex="-1"></a>fish  <span class="ot">=</span> <span class="fu">ts.intersect</span>(rec, <span class="at">soiL6=</span><span class="fu">lag</span>(soi,<span class="sc">-</span><span class="dv">6</span>), <span class="at">dL6=</span><span class="fu">lag</span>(dummy,<span class="sc">-</span><span class="dv">6</span>), <span class="at">dframe=</span><span class="cn">TRUE</span>)</span>
<span id="cb52-3"><a href="#cb52-3" tabindex="-1"></a><span class="fu">summary</span>(fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(rec<span class="sc">~</span> soiL6<span class="sc">*</span>dL6, <span class="at">data=</span>fish, <span class="at">na.action=</span><span class="cn">NULL</span>))</span>
<span id="cb52-4"><a href="#cb52-4" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" tabindex="-1"></a><span class="fu">plot</span>(fish<span class="sc">$</span>soiL6, fish<span class="sc">$</span>rec)</span>
<span id="cb52-6"><a href="#cb52-6" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">lowess</span>(fish<span class="sc">$</span>soiL6, fish<span class="sc">$</span>rec), <span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb52-7"><a href="#cb52-7" tabindex="-1"></a><span class="fu">points</span>(fish<span class="sc">$</span>soiL6, <span class="fu">fitted</span>(fit), <span class="at">pch=</span><span class="st">&#39;+&#39;</span>, <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-9-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">resid</span>(fit)) <span class="co"># not shown ...</span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-9-2.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" tabindex="-1"></a><span class="fu">acf1</span>(<span class="fu">resid</span>(fit))   <span class="co"># ... but obviously not noise</span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-9-3.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="assessing-periodic-behavior-through-regression-analysis"
class="section level3">
<h3>Assessing Periodic Behavior Through Regression Analysis</h3>
<ul>
<li>Before, we generated <span class="math inline">\(n=500\)</span>
observations from the model <span class="math display">\[
x_{t}=A\cos (2\pi \omega t+\phi)+w_{t},
\]</span> where <span class="math inline">\(\omega=1/50\)</span>, <span
class="math inline">\(A=2\)</span>, <span
class="math inline">\(\phi=.6\pi\)</span>, and <span
class="math inline">\(\sigma_{w}=5\)</span>.</li>
</ul>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">90210</span>)  <span class="co"># so you can reproduce these results</span></span>
<span id="cb55-2"><a href="#cb55-2" tabindex="-1"></a>x <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span><span class="dv">1</span><span class="sc">:</span><span class="dv">500</span><span class="sc">/</span><span class="dv">50</span> <span class="sc">+</span> .<span class="dv">6</span><span class="sc">*</span>pi) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">500</span>,<span class="dv">0</span>,<span class="dv">5</span>)</span>
<span id="cb55-3"><a href="#cb55-3" tabindex="-1"></a>z1 <span class="ot">=</span> <span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span><span class="dv">1</span><span class="sc">:</span><span class="dv">500</span><span class="sc">/</span><span class="dv">50</span>)  </span>
<span id="cb55-4"><a href="#cb55-4" tabindex="-1"></a>z2 <span class="ot">=</span> <span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span><span class="dv">1</span><span class="sc">:</span><span class="dv">500</span><span class="sc">/</span><span class="dv">50</span>)</span>
<span id="cb55-5"><a href="#cb55-5" tabindex="-1"></a><span class="fu">tsplot</span>(x)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-10-plot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>We assume the frequency of oscillation <span
class="math inline">\(\omega=1/50\)</span> is known, but <span
class="math inline">\(A\)</span> and <span
class="math inline">\(\phi\)</span> are unknown parameters.</p></li>
<li><p>In this case the parameters appear in a nonlinear way, so we use
a trigonometric identity <span class="math inline">\(\cos
(\alpha\pm\beta)=\cos(\alpha)\cos(\beta)\mp\sin(\alpha)\sin(\beta)\)</span>
and write <span class="math display">\[
A\cos(2\pi\omega t+\phi)=\beta_{1}\cos(2\pi\omega
t)+\beta_{2}\sin(2\pi\omega t),
\]</span> where <span
class="math inline">\(\beta_{1}=A\cos(\phi)\)</span> and <span
class="math inline">\(\beta_{2}=-A\sin(\phi)\)</span>.</p></li>
<li><p>Thus, the model can be written by <span class="math display">\[
x_{t}=\beta_{1}\cos(2\pi t/50)+\beta_{2}\sin(2\pi t/50)+w_{t}.
\]</span></p></li>
<li><p>Using linear regression, we find <span
class="math inline">\(\hat{\beta}_{1}=-.74_{(.33)}\)</span>, <span
class="math inline">\(\hat{\beta}_{2}=-1.99_{(.33)}\)</span> with <span
class="math inline">\(\hat{\sigma}_{w}=5.18\)</span>.</p></li>
<li><p>We note the actual values of the coefficients for this example
are <span class="math inline">\(\beta_{1}=2\cos(.6\pi)=-.62\)</span>,
and <span
class="math inline">\(\beta_{2}=-2\sin(.6\pi)=-1.90\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" tabindex="-1"></a><span class="fu">summary</span>(fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(x<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>z1<span class="sc">+</span>z2))  <span class="co"># zero to exclude the intercept</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = x ~ 0 + z1 + z2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.8584  -3.8525  -0.3186   3.3487  15.5440 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## z1  -0.7442     0.3274  -2.273   0.0235 *  
## z2  -1.9949     0.3274  -6.093 2.23e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.177 on 498 degrees of freedom
## Multiple R-squared:  0.07827,    Adjusted R-squared:  0.07456 
## F-statistic: 21.14 on 2 and 498 DF,  p-value: 1.538e-09</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" tabindex="-1"></a><span class="fu">tsplot</span>(x, <span class="at">col=</span><span class="dv">8</span>, <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">hat</span>(x)))</span>
<span id="cb58-2"><a href="#cb58-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">fitted</span>(fit), <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-10-results-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>It is clear that we are able to detect the signal in the noise using
regression, even though the signal-to-noise ratio is small.</li>
</ul>
</div>
</div>
<div id="smoothing-in-the-time-series-context" class="section level2">
<h2>Smoothing in the Time Series Context</h2>
<ul>
<li><p>A symmetric moving average of the data is <span
class="math display">\[
m_{t}=\sum_{j=-k}^{k}a_{j}x_{t-j},
\]</span> where <span class="math inline">\(a_{j}=a_{-j}\geq0\)</span>
and <span
class="math inline">\(\sum_{j=-k}^{k}a_{j}=1\)</span>.</p></li>
<li><p>This method is useful in discovering certain traits in a time
series, such as long-term trend and seasonal components.</p></li>
</ul>
<div id="example-moving-average-smoother" class="section level3">
<h3>Example: Moving Average Smoother</h3>
<ul>
<li>The monthly SOI series smoothed by a symmetric moving average with
weights <span
class="math inline">\(a_{0}=a_{\pm1}=\cdots=a_{\pm5}=1/12\)</span>, and
<span class="math inline">\(a_{\pm6}=1/24\)</span>; <span
class="math inline">\(k=6\)</span>.</li>
</ul>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" tabindex="-1"></a>wgts <span class="ot">=</span> <span class="fu">c</span>(.<span class="dv">5</span>, <span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">11</span>), .<span class="dv">5</span>)<span class="sc">/</span><span class="dv">12</span></span>
<span id="cb59-2"><a href="#cb59-2" tabindex="-1"></a>soif <span class="ot">=</span> <span class="fu">filter</span>(soi, <span class="at">sides=</span><span class="dv">2</span>, <span class="at">filter=</span>wgts)</span>
<span id="cb59-3"><a href="#cb59-3" tabindex="-1"></a><span class="fu">tsplot</span>(soi)</span>
<span id="cb59-4"><a href="#cb59-4" tabindex="-1"></a><span class="fu">lines</span>(soif, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb59-5"><a href="#cb59-5" tabindex="-1"></a><span class="fu">par</span>(<span class="at">fig =</span> <span class="fu">c</span>(.<span class="dv">75</span>, <span class="dv">1</span>, .<span class="dv">75</span>, <span class="dv">1</span>), <span class="at">new =</span> <span class="cn">TRUE</span>) <span class="co"># the insert</span></span>
<span id="cb59-6"><a href="#cb59-6" tabindex="-1"></a>nwgts <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">20</span>), wgts, <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">20</span>))</span>
<span id="cb59-7"><a href="#cb59-7" tabindex="-1"></a><span class="fu">plot</span>(nwgts, <span class="at">type=</span><span class="st">&quot;l&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">02</span>,.<span class="dv">1</span>), <span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">ann=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-11-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>This particular method removes (filters out) the obvious annual
temperature cycle and helps emphasize the El Niño cycle.</li>
</ul>
</div>
<div id="kernel-smoothing" class="section level3">
<h3>Kernel Smoothing</h3>
<ul>
<li><p>It is a moving average smoother that uses a weight function, or
kernel, to average the observations.</p></li>
<li><p>The Kernel smoothing is <span class="math display">\[
m_{t}=\sum_{i=1}^{n}w_{i}(t)x_{i},
\]</span> where <span class="math display">\[
w_{i}(t)=\frac{K(\frac{t-i}{b})}{\sum_{j=1}^{n}K(\frac{t-j}{b})}
\]</span> are the weights and <span
class="math inline">\(K(\cdot)\)</span> is a Kernel function.</p></li>
<li><p>Typically, the normal kernel, <span
class="math inline">\(K(z)=\frac{1}{\sqrt{2\pi}}\exp(-z^{2}/2)\)</span>,
is used.</p></li>
<li><p>The kernel smoothing of the SOI series (with the normal kernel)
is</p></li>
</ul>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" tabindex="-1"></a><span class="fu">tsplot</span>(soi)</span>
<span id="cb60-2"><a href="#cb60-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(<span class="fu">time</span>(soi), soi, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">1</span>), <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb60-3"><a href="#cb60-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">fig =</span> <span class="fu">c</span>(.<span class="dv">75</span>, <span class="dv">1</span>, .<span class="dv">75</span>, <span class="dv">1</span>), <span class="at">new =</span> <span class="cn">TRUE</span>) <span class="co"># the insert</span></span>
<span id="cb60-4"><a href="#cb60-4" tabindex="-1"></a>gauss <span class="ot">=</span> <span class="cf">function</span>(x) { <span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>) }</span>
<span id="cb60-5"><a href="#cb60-5" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">to =</span> <span class="dv">3</span>, <span class="at">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb60-6"><a href="#cb60-6" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">gauss</span>(x), <span class="at">type =</span><span class="st">&quot;l&quot;</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">02</span>,.<span class="dv">45</span>), <span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">ann=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-12-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>This estimator, which was originally explored by Parzen (1962) and
Rosenblatt (1956b), is often called the Nadaraya–Watson estimator
(Watson, 1966).</li>
</ul>
</div>
<div id="lowess-locally-weighted-scatterplot-smoothers"
class="section level3">
<h3>Lowess (Locally Weighted Scatterplot Smoothers)</h3>
<ul>
<li><p>It is based on <span class="math inline">\(k\)</span>-nearest
neighbors regression, wherein one uses only the data <span
class="math display">\[
\{x_{t-k/2},\ldots,x_{t},\ldots,x_{t+k/2}\}
\]</span> to predict <span class="math inline">\(x_{t}\)</span> via
regression, and then sets <span
class="math inline">\(m_{t}=\hat{x}_{t}\)</span> .</p></li>
<li><p>The smoothing of SOI using lowess</p></li>
</ul>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" tabindex="-1"></a><span class="fu">tsplot</span>(soi)</span>
<span id="cb61-2"><a href="#cb61-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">lowess</span>(soi, <span class="at">f=</span>.<span class="dv">05</span>), <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>) <span class="co"># El Nino cycle</span></span>
<span id="cb61-3"><a href="#cb61-3" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">lowess</span>(soi), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>) <span class="co"># trend (with default span)</span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example2-13-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>One smoother is used to obtain an estimate of the El Niño cycle
of the data.</p></li>
<li><p>Other smoother is used to obtain a (negative) trend in SOI would
indicate the long-term warming of the Pacific Ocean.</p></li>
</ul>
</div>
<div id="homework-2" class="section level3">
<h3>Homework</h3>
<ul>
<li>Read Example 2.14 Smoothing Splines and Example 2.15 Smoothing One
Series as a Function of Another.</li>
</ul>
</div>
</div>
</div>
<div id="arima-models" class="section level1">
<h1>ARIMA Models</h1>
<ul>
<li><p>In the time series case, it is desirable to allow the dependent
variable to be influenced by the past values of the independent
variables and possibly by its own past values.</p></li>
<li><p>If the present can be plausibly modeled in terms of only the past
values of the independent inputs, we have the enticing prospect that
forecasting will be possible.</p></li>
</ul>
<div id="autoregressive-models" class="section level2">
<h2>Autoregressive Models</h2>
<ul>
<li><p>The current value of the series, <span
class="math inline">\(x_{t}\)</span>, can be explained as a function of
<span class="math inline">\(p\)</span> past values, <span
class="math inline">\(x_{t-1},x_{t-2},\ldots,x_{t-p}\)</span>, where
<span class="math inline">\(p\)</span> determines the number of steps
into the past needed to forecast the current value.</p></li>
<li><p>For example <span class="math display">\[
x_{t}=x_{t-1}-.90x_{t-2}+w_{t},
\]</span> where <span class="math inline">\(w_{t}\)</span> is white
Gaussian noise with <span
class="math inline">\(\sigma_{w}^{2}=1\)</span>.</p></li>
<li><p>We have now assumed the current value is a particular linear
function of past values.</p></li>
<li><p>The forecasting for such a model might be <span
class="math display">\[
x_{n+1}^{n}=x_{n}-.90x_{n-1},
\]</span> where the quantity on the left-hand side denotes the forecast
at the next period <span class="math inline">\(n+1\)</span> based on the
observed data, <span
class="math inline">\(x_{1},x_{2},\ldots,x_{n}\)</span>.</p></li>
<li><p>Forecast a real data series from its own past values can be
assessed by looking at the autocorrelation function and the lagged
scatterplot matrices.</p></li>
</ul>
<blockquote>
<p>An autoregressive model of order <span
class="math inline">\(p\)</span>, abbreviated <strong>AR</strong>(<span
class="math inline">\(p\)</span>), is of the form <span
class="math display">\[
x_{t}=\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+\cdots+\phi_{p}x_{t-p}+w_{t},
\]</span> where <span class="math inline">\(x_{t}\)</span> is
stationary, <span class="math inline">\(w_{t}\sim wn(0,
\sigma_{w}^{2})\)</span>, and <span
class="math inline">\(\phi_{1},\phi_{2},\ldots,\phi_{p}\)</span> are
constants (<span class="math inline">\(\phi_{p}\neq0\)</span>). The mean
of <span class="math inline">\(x_{t}\)</span> is zero. If the mean,
<span class="math inline">\(\mu\)</span>, of <span
class="math inline">\(x_{t}\)</span> is not zero, replace <span
class="math inline">\(x_{t}\)</span> by <span
class="math inline">\(x_{t}-\mu\)</span>, <span class="math display">\[
x_{t}-\mu=\phi_{1}(x_{t-1}-\mu)+\phi_{2}(x_{t-2}-\mu)+\cdots+\phi_{p}(x_{t-p}-\mu)+w_{t},
\]</span> or write <span class="math display">\[
x_{t}=\alpha+\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+\cdots+\phi_{p}x_{t-p}+w_{t},
\]</span> where <span
class="math inline">\(\alpha=\mu(1-\phi_{1}-\cdots-\phi_{p})\)</span>.</p>
</blockquote>
<ul>
<li><p>A useful form follows by using the backshift operator to write
the AR(<span class="math inline">\(p\)</span>) model as <span
class="math display">\[
(1-\phi_{1}B-\phi_{2}B^{2}-\cdots-\phi_{p}B^{p})x_{t}=w_{t},
\]</span> or even more concisely as <span class="math display">\[
\phi(B)x_{t}=w_{t}.
\]</span></p></li>
<li><p>The properties of <span class="math inline">\(\phi(B)\)</span>
are important in solving <span
class="math inline">\(\phi(B)x_{t}=w_{t}\)</span> for <span
class="math inline">\(x_{t}\)</span>.</p></li>
</ul>
<blockquote>
<p>The <strong>autoregressive operator</strong> is defined to be <span
class="math display">\[
\phi(B)=1-\phi_{1}B-\phi_{2}B^{2}-\cdots-\phi_{p}B^{p}.
\]</span></p>
</blockquote>
<div id="example-the-ar1-model" class="section level3">
<h3>Example: The AR(1) Model</h3>
<ul>
<li><p>Consider the first-order model, AR(1), given by <span
class="math display">\[
x_{t}=\phi x_{t-1}+w_{t}.
\]</span></p></li>
<li><p>Iterating backwards <span class="math inline">\(k\)</span> times,
we get <span class="math display">\[
\begin{align*}
x_{t} &amp; =\phi x_{t-1}+w_{t}=\phi(\phi x_{t-2}+w_{t-1})+w_{t}\\
&amp; =\phi^{2}x_{t-2}+\phi w_{t-1}+w_{t}\\
&amp; \vdots\\
&amp; =\phi^{k}x_{t-k}+\sum_{j=0}^{k-1}\phi^{j}w_{t-j}.
\end{align*}
\]</span></p></li>
<li><p>This method suggests that, by continuing to iterate backward, and
provided that <span
class="math inline">\(\left|\phi\right|&lt;1\)</span> and <span
class="math inline">\(\sup_{t}\textrm{var}(x_{t})&lt;\infty\)</span>, we
can represent an AR(1) model as a linear process given by <span
class="math display">\[
x_{t}=\sum_{j=0}^{\infty}\phi^{j}w_{t-j}.
\]</span></p></li>
<li><p>This representation is called the stationary solution of the
model.</p></li>
<li><p>The limit above exists in the mean square sense because <span
class="math display">\[
\lim_{k\rightarrow\infty}\textrm{E}(x_{t}-\sum_{j=0}^{k-1}\phi^{j}w_{t-j})^{2}=\lim_{k\rightarrow\infty}\phi^{2k}\textrm{E}(x_{t-k}^{2})=0.
\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>The AR(1) process defined by <span
class="math inline">\(x_{t}=\sum_{j=0}^{\infty}\phi^{j}w_{t-j}\)</span>
is stationary with mean <span class="math display">\[
\textrm{E}(x_{t})=\sum_{j=0}^{\infty}\phi^{j}\textrm{E}(w_{t-j})=0,
\]</span> and autocovariance function, <span class="math display">\[
\begin{align*}
\gamma(h) &amp;
=\textrm{cov}(x_{t+h},x_{t})=\textrm{E}[(\sum_{j=0}^{\infty}\phi^{j}w_{t+h-j})(\sum_{k=0}^{\infty}\phi^{k}w_{t-k})]\\
&amp;
=\textrm{E}[(w_{t+h}+\cdots+\phi^{h}w_{t}+\phi^{h+1}w_{t-1}+\cdots)(w_{t}+\phi
w_{t-1}+\cdots)]\\
&amp;
=\sigma_{w}^{2}\sum_{j=0}^{\infty}\phi^{h+j}\phi^{j}=\sigma_{w}^{2}\phi^{h}\sum_{j=0}^{\infty}\phi^{2j}=\frac{\sigma_{w}^{2}\phi^{h}}{1-\phi^{2}},\quad
h\geq0.
\end{align*}
\]</span></p></li>
<li><p>The ACF of an AR(1) is <span class="math display">\[
\rho(h)=\frac{\gamma(h)}{\gamma(0)}=\phi^{h},\quad h\geq0,
\]</span> and <span class="math inline">\(\rho(h)\)</span> satisfies the
recursion <span class="math display">\[
\rho(h)=\phi\rho(h-1),\quad h=1,2,\ldots.
\]</span></p></li>
</ul>
</div>
<div id="example-the-sample-path-of-an-ar1-process"
class="section level3">
<h3>Example: The Sample Path of an AR(1) Process</h3>
<ul>
<li>We shows a time plot of two AR(1) processes, one with <span
class="math inline">\(\phi=.9\)</span> and one with <span
class="math inline">\(\phi=-.9\)</span>; in both cases, <span
class="math inline">\(\sigma_{w}^{2}=1\)</span>.</li>
</ul>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))                         </span>
<span id="cb62-2"><a href="#cb62-2" tabindex="-1"></a><span class="co"># in the expressions below, ~ is a space and == is equal</span></span>
<span id="cb62-3"><a href="#cb62-3" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">arima.sim</span>(<span class="fu">list</span>(<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">ar=</span>.<span class="dv">9</span>), <span class="at">n=</span><span class="dv">100</span>), <span class="at">ylab=</span><span class="st">&quot;x&quot;</span>, <span class="at">main=</span>(<span class="fu">expression</span>(<span class="fu">AR</span>(<span class="dv">1</span>)<span class="sc">~</span><span class="er">~~</span>phi<span class="sc">==+</span>.<span class="dv">9</span>))) </span>
<span id="cb62-4"><a href="#cb62-4" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">arima.sim</span>(<span class="fu">list</span>(<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">ar=</span><span class="sc">-</span>.<span class="dv">9</span>), <span class="at">n=</span><span class="dv">100</span>), <span class="at">ylab=</span><span class="st">&quot;x&quot;</span>, <span class="at">main=</span>(<span class="fu">expression</span>(<span class="fu">AR</span>(<span class="dv">1</span>)<span class="sc">~</span><span class="er">~~</span>phi<span class="sc">==-</span>.<span class="dv">9</span>))) </span></code></pre></div>
<p><img src="Slides_files/figure-html/example3-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>In the first case, <span
class="math inline">\(\rho(h)=.9^{h}\)</span>, for <span
class="math inline">\(h\geq0\)</span>, so observations close together in
time are positively correlated with each other.</p></li>
<li><p>Contrast this with the case in which <span
class="math inline">\(\phi=-.9\)</span>, so that <span
class="math inline">\(\rho(h)=(-.9)^{h}\)</span>, for <span
class="math inline">\(h\geq0\)</span>. This result means that
observations at contiguous time points are negatively correlated but
observations two time points apart are positively correlated.</p></li>
</ul>
</div>
<div id="example-explosive-ar-models-and-causality"
class="section level3">
<h3>Example: Explosive AR Models and Causality</h3>
<ul>
<li><p>We know that the random walk <span
class="math inline">\(x_{t}=x_{t-1}+w_{t}\)</span> is not
stationary.</p></li>
<li><p>We might wonder whether there is a stationary AR(1) process with
<span class="math inline">\(\left|\phi\right|&gt;1\)</span>.</p></li>
<li><p>Such processes are called explosive because the values of the
time series quickly become large in magnitude.</p></li>
<li><p>Clearly, because <span
class="math inline">\(\left|\phi\right|^{j}\)</span> increases without
bound as <span class="math inline">\(j\rightarrow\infty\)</span>, <span
class="math inline">\(\sum_{j=0}^{k-1}\phi^{j}w_{t-j}\)</span> will not
converge (in mean square) as <span
class="math inline">\(k\rightarrow\infty\)</span>, so the intuition used
to get <span
class="math inline">\(x_{t}=\sum_{j=0}^{\infty}\phi^{j}w_{t-j}\)</span>
will not work directly.</p></li>
<li><p>However, we can modify that argument to obtain a stationary
model.</p></li>
<li><p>Write <span class="math display">\[
x_{t+1}=\phi x_{t}+w_{t+1},
\]</span> in which case, <span class="math display">\[
\begin{align*}
x_{t} &amp;
=\phi^{-1}x_{t+1}-\phi^{-1}w_{t+1}=\phi^{-1}(\phi^{-1}x_{t+2}-\phi^{-1}w_{t+2})-\phi^{-1}w_{t+1}\\
&amp; \vdots\\
&amp; =\phi^{-k}x_{t+k}-\sum_{j=1}^{k-1}\phi^{-j}w_{t+j},
\end{align*}
\]</span> by iterating forward <span class="math inline">\(k\)</span>
steps.</p></li>
<li><p>Because <span
class="math inline">\(\left|\phi\right|^{-1}&lt;1\)</span>, this result
suggests the stationary (verify) future dependent AR(1) model <span
class="math display">\[
x_{t}=-\sum_{j=1}^{\infty}\phi^{-j}w_{t+j}.
\]</span></p></li>
<li><p>Unfortunately, this model is useless because it requires us to
know the future to be able to predict the future (not causal).</p></li>
</ul>
</div>
<div id="example-every-explosion-has-a-cause" class="section level3">
<h3>Example: Every Explosion Has a Cause</h3>
<ul>
<li><p>Excluding explosive models from consideration is not a problem
because the models have causal counterparts.</p></li>
<li><p>Consider, for example, <span class="math display">\[
x_{t}=\phi x_{t-1}+w_{t}\quad\textrm{with}\;\left|\phi\right|&gt;1
\]</span> and <span class="math inline">\(w_{t}\sim\textrm{iid
N}(0,\sigma_{w}^{2})\)</span>.</p></li>
<li><p>Then using <span
class="math inline">\(x_{t}=-\sum_{j=1}^{\infty}\phi^{-j}w_{t+j}\)</span>,
<span class="math inline">\(\{x_{t}\}\)</span> is a non-causal
stationary Gaussian process with <span
class="math inline">\(\textrm{E}(x_{t})=0\)</span> and <span
class="math display">\[
\begin{align*}
\gamma_{x}(h) &amp;
=\textrm{cov}(x_{t+h},x_{t})=\textrm{cov}(-\sum_{j=1}^{\infty}\phi^{-j}w_{t+h+j},-\sum_{k=1}^{\infty}\phi^{-k}w_{t+k})\\
&amp; =\sigma_{w}^{2}\phi^{-2}\phi^{-h}/(1-\phi^{-2}).
\end{align*}
\]</span></p></li>
<li><p>Thus, the causal process defined by <span class="math display">\[
y_{t}=\phi^{-1}y_{t-1}+\nu_{t}
\]</span> where <span class="math inline">\(\nu_{t}\sim\textrm{iid
N}(0,\sigma_{w}^{2}\phi^{-2})\)</span> and is stochastically equal to
the <span class="math inline">\(x_{t}\)</span> process (i.e., all finite
distributions of the processes are the same).</p></li>
<li><p>Take into account that, earlier, we found that for <span
class="math inline">\(x_{t}=\phi x_{t-1}+w_{t}\)</span> with <span
class="math inline">\(\left|\phi\right|&lt;1\)</span> and <span
class="math inline">\(\sup_{t}\textrm{var}(x_{t})&lt;\infty\)</span> we
have <span class="math display">\[
\gamma(h)=\frac{\sigma_{w}^{2}\phi^{h}}{1-\phi^{2}},\quad h\geq0.
\]</span></p></li>
<li><p>For example, if <span
class="math inline">\(x_{t}=2x_{t-1}+w_{t}\)</span> with <span
class="math inline">\(\sigma_{w}^{2}=1\)</span>, then <span
class="math inline">\(y_{t}=\frac{1}{2}y_{t-1}+\nu_{t}\)</span> with
<span class="math inline">\(\sigma_{\nu}^{2}=1/4\)</span> is an
equivalent causal process.</p></li>
</ul>
<hr />
<ul>
<li><p>The technique of iterating backward to get an idea of the
stationary solution of AR models works well when <span
class="math inline">\(p=1\)</span>, but not for larger orders.</p></li>
<li><p>A general technique is that of matching coefficients.</p></li>
<li><p>Consider the AR(1) model in operator form <span
class="math display">\[
\phi(B)x_{t}=w_{t},
\]</span> where <span class="math inline">\(\phi(B)=1-\phi B\)</span>,
and <span
class="math inline">\(\left|\phi\right|&lt;1\)</span>.</p></li>
<li><p>Also, write the model <span
class="math inline">\(x_{t}=\sum_{j=0}^{\infty}\phi^{j}w_{t-j}\)</span>
using operator form as <span class="math display">\[
x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}=\psi(B)w_{t},
\]</span> where <span
class="math inline">\(\psi(B)=\sum_{j=0}^{\infty}\psi_{j}B^{j}\)</span>
and <span class="math inline">\(\psi_{j}=\phi^{j}\)</span>.</p></li>
<li><p>Suppose we did not know that <span
class="math inline">\(\psi_{j}=\phi^{j}\)</span>. We could substitute
<span class="math inline">\(\psi(B)w_{t}\)</span> from <span
class="math inline">\(x_{t}=\psi(B)w_{t}\)</span> for <span
class="math inline">\(x_{t}\)</span> in <span
class="math inline">\(\phi(B)x_{t}=w_{t}\)</span> to obtain <span
class="math display">\[
\phi(B)\psi(B)w_{t}=w_{t}.
\]</span></p></li>
<li><p>The coefficients of <span class="math inline">\(B\)</span> on the
left-hand side must be equal to those on right-hand side, which means
<span class="math display">\[
(1-\phi B)(1+\psi_{1}B+\psi_{2}B^{2}+\cdots+\psi_{j}B^{j}+\cdots)=1.
\]</span></p></li>
<li><p>Reorganizing the coefficients, <span class="math display">\[
1+(\psi_{1}-\phi)B+(\psi_{2}-\psi_{1}\phi)B^{2}+\cdots+(\psi_{j}-\psi_{j-1}\phi)B^{j}+\cdots=1,
\]</span> we see that for each <span
class="math inline">\(j=1,2,\ldots\)</span>, the coefficient of <span
class="math inline">\(B^{j}\)</span> on the left must be zero because it
is zero on the right.</p></li>
<li><p>The coefficient of <span class="math inline">\(B\)</span> on the
left is <span class="math inline">\((\psi_{1}-\phi)\)</span>, and
equating this to zero, <span
class="math inline">\(\psi_{1}-\phi=0\)</span>, leads to <span
class="math inline">\(\psi_{1}=\phi\)</span>.</p></li>
<li><p>Continuing, the coefficient of <span
class="math inline">\(B^{2}\)</span> is <span
class="math inline">\((\psi_{2}-\psi_{1}\phi)\)</span>, so <span
class="math inline">\(\psi_{2}=\phi^{2}\)</span>.</p></li>
<li><p>In general, <span class="math display">\[
\psi_{j}=\psi_{j-1}\phi,
\]</span> with <span class="math inline">\(\psi_{0}=1\)</span>, which
leads to the solution <span
class="math inline">\(\psi_{j}=\phi^{j}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Another way to think about the operations we just performed is to
consider the AR(1) model in operator form, <span class="math display">\[
\phi(B)x_{t}=w_{t}.
\]</span></p></li>
<li><p>Now multiply both sides by <span
class="math inline">\(\phi^{-1}(B)\)</span> (assuming the inverse
operator exists) to get <span class="math display">\[
\phi^{-1}(B)\phi(B)x_{t}=\phi^{-1}(B)w_{t},
\]</span> or <span class="math display">\[
x_{t}=\phi^{-1}(B)w_{t}.
\]</span></p></li>
<li><p>We know already that <span class="math display">\[
\phi^{-1}(B)=1+\phi B+\phi^{2}B^{2}+\cdots+\phi^{j}B^{j}+\cdots,
\]</span> that is, <span class="math inline">\(\phi^{-1}(B)\)</span> is
<span class="math inline">\(\psi(B)\)</span> in <span
class="math inline">\(x_{t}=\psi(B)w_{t}\)</span>.</p></li>
<li><p>Thus, we notice that working with operators is like working with
polynomials.</p></li>
<li><p>That is, consider the polynomial <span
class="math inline">\(\phi(z)=1-\phi z\)</span>, where <span
class="math inline">\(z\)</span> is a complex number and <span
class="math inline">\(\left|\phi\right|&lt;1\)</span>.</p></li>
<li><p>Then, <span class="math display">\[
\phi^{-1}(z)=\frac{1}{(1-\phi z)}=1+\phi
z+\phi^{2}z^{2}+\cdots+\phi^{j}z^{j}+\cdots,\quad\left|z\right|\leq1,
\]</span> and the coefficients of <span
class="math inline">\(B^{j}\)</span> in <span
class="math inline">\(\phi^{-1}(B)\)</span> are the same as the
coefficients of <span class="math inline">\(z^{j}\)</span> in <span
class="math inline">\(\phi^{-1}(z)\)</span>.</p></li>
<li><p>In other words, we may treat the backshift operator, <span
class="math inline">\(B\)</span>, as a complex number, <span
class="math inline">\(z\)</span>.</p></li>
</ul>
</div>
</div>
<div id="moving-average-models" class="section level2">
<h2>Moving Average Models</h2>
<blockquote>
<p>The <strong>moving average model</strong> of order <span
class="math inline">\(q\)</span>, or <strong>MA</strong>(<span
class="math inline">\(q\)</span>) model, is defined to be <span
class="math display">\[
x_{t}=w_{t}+\theta_{1}w_{t-1}+\theta_{2}w_{t-2}+\cdots+\theta_{q}w_{t-q},
\]</span> where <span class="math inline">\(w_{t}\sim
wn(0,\sigma_{w}^{2})\)</span>, and <span
class="math inline">\(\theta_{1},\theta_{2},\ldots,\theta_{q}\)</span>
(<span class="math inline">\(\theta_{q}\neq0\)</span>) are
parameters.</p>
</blockquote>
<ul>
<li><p>The system is the same as the infinite moving average defined as
the linear process <span
class="math inline">\(x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}\)</span>,
where <span class="math inline">\(\psi_{0}=1\)</span>, <span
class="math inline">\(\psi_{j}=\theta_{j}\)</span>, for <span
class="math inline">\(j=1,\ldots,q\)</span>, and <span
class="math inline">\(\psi_{j}=0\)</span> for other values.</p></li>
<li><p>We may also write the MA(q) process in the equivalent form <span
class="math display">\[
x_{t}=\theta(B)w_{t}.
\]</span></p></li>
</ul>
<blockquote>
<p>The <strong>moving average operator</strong> is <span
class="math display">\[
\theta(B)=1+\theta_{1}B+\theta_{2}B^{2}+\cdots+\theta_{q}B^{q}.
\]</span></p>
</blockquote>
<ul>
<li>Unlike the autoregressive process, the moving average process is
stationary for any values of the parameters <span
class="math inline">\(\theta_{1},\ldots,\theta_{q}\)</span>.</li>
</ul>
<div id="example-the-ma1-process" class="section level3">
<h3>Example: The MA(1) Process</h3>
<ul>
<li><p>Consider the MA(1) model <span
class="math inline">\(x_{t}=w_{t}+\theta w_{t-1}\)</span>. Then, <span
class="math inline">\(\textrm{E}(x_{t})=0\)</span>, <span
class="math display">\[
\gamma(h)=\begin{cases}
(1+\theta^{2})\sigma_{w}^{2} &amp; h=0,\\
\theta\sigma_{w}^{2} &amp; h=1,\\
0 &amp; h&gt;1,
\end{cases}
\]</span> and the ACF is <span class="math display">\[
\rho(h)=\begin{cases}
\frac{\theta}{(1+\theta^{2})} &amp; h=1,\\
0 &amp; h&gt;1.
\end{cases}
\]</span></p></li>
<li><p>Note <span
class="math inline">\(\left|\rho(1)\right|\leq1/2\)</span> for all
values of <span class="math inline">\(\theta\)</span> (why?).</p></li>
<li><p>Also, <span class="math inline">\(x_{t}\)</span> is correlated
with <span class="math inline">\(x_{t−1}\)</span>, but not with <span
class="math inline">\(x_{t-2},x_{t-3},\ldots\)</span>. Contrast this
with the case of the AR(1) model in which the correlation between <span
class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(x_{t-k}\)</span> is never zero.</p></li>
<li><p>When <span class="math inline">\(\theta=.9\)</span>, for example,
<span class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(x_{t-1}\)</span> are positively correlated, and
<span class="math inline">\(\rho(1)=.497\)</span>.</p></li>
<li><p>When <span class="math inline">\(\theta=-.9\)</span>, <span
class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(x_{t-1}\)</span> are negatively correlated, <span
class="math inline">\(\rho(1)=-.497\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))                                   </span>
<span id="cb63-2"><a href="#cb63-2" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">arima.sim</span>(<span class="fu">list</span>(<span class="at">order=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">ma=</span>.<span class="dv">9</span>), <span class="at">n=</span><span class="dv">100</span>), <span class="at">ylab=</span><span class="st">&quot;x&quot;</span>, <span class="at">main=</span>(<span class="fu">expression</span>(<span class="fu">MA</span>(<span class="dv">1</span>)<span class="sc">~</span><span class="er">~~</span>theta<span class="sc">==+</span>.<span class="dv">9</span>)))    </span>
<span id="cb63-3"><a href="#cb63-3" tabindex="-1"></a><span class="fu">tsplot</span>(<span class="fu">arima.sim</span>(<span class="fu">list</span>(<span class="at">order=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">ma=</span><span class="sc">-</span>.<span class="dv">9</span>), <span class="at">n=</span><span class="dv">100</span>), <span class="at">ylab=</span><span class="st">&quot;x&quot;</span>, <span class="at">main=</span>(<span class="fu">expression</span>(<span class="fu">MA</span>(<span class="dv">1</span>)<span class="sc">~</span><span class="er">~~</span>theta<span class="sc">==-</span>.<span class="dv">9</span>)))  </span></code></pre></div>
<p><img src="Slides_files/figure-html/example3-5-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="example-non-uniqueness-of-ma-models-and-invertibility"
class="section level3">
<h3>Example: Non-uniqueness of MA Models and Invertibility</h3>
<ul>
<li><p>We note that for an MA(1) model, <span
class="math inline">\(\rho(h)\)</span> is the same for <span
class="math inline">\(\theta\)</span> and <span
class="math inline">\(\frac{1}{\theta}\)</span>.</p></li>
<li><p>In addition, the pair <span
class="math inline">\(\sigma_{w}^{2}=1\)</span> and <span
class="math inline">\(\theta=5\)</span> yield the same autocovariance
function as the pair <span
class="math inline">\(\sigma_{w}^{2}=25\)</span> and <span
class="math inline">\(\theta=1/5\)</span>, namely, <span
class="math display">\[
\gamma(h)=\begin{cases}
26 &amp; h=0,\\
5 &amp; h=1,\\
0 &amp; h&gt;1.
\end{cases}
\]</span></p></li>
<li><p>Thus, the MA(1) processes <span class="math display">\[
x_{t}=w_{t}+\frac{1}{5}w_{t-1},\quad w_{t}\sim\textrm{iid N}(0,25)
\]</span> and <span class="math display">\[
y_{t}=\nu_{t}+5\nu_{t-1},\quad\nu_{t}\sim\textrm{iid N}(0,1)
\]</span> are the same because of normality (i.e., all finite
distributions are the same).</p></li>
<li><p>We can only observe the time series, <span
class="math inline">\(x_{t}\)</span> or <span
class="math inline">\(y_{t}\)</span>, and not the noise, <span
class="math inline">\(w_{t}\)</span> or <span
class="math inline">\(\nu_{t}\)</span>, so we cannot distinguish between
the models.</p></li>
<li><p>For convenience, by mimicking the criterion of causality for AR
models, we will choose the model with an infinite AR representation.
Such a process is called an <em>invertible</em> process.</p></li>
<li><p>To discover which model is the invertible model, we can reverse
the roles of <span class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(w_{t}\)</span> (because we are mimicking the AR
case) and write the MA(1) model as <span class="math display">\[
w_{t}=-\theta w_{t-1}+x_{t}.
\]</span></p></li>
<li><p>Following the AR steps, if <span
class="math inline">\(\left|\theta\right|&lt;1\)</span>, then <span
class="math display">\[
w_{t}=\sum_{j=0}^{\infty}(-\theta)^{j}x_{t-j},
\]</span> which is the desired infinite AR representation of the
model.</p></li>
<li><p>Hence, given a choice, we will choose the model with <span
class="math inline">\(\sigma_{w}^{2}=25\)</span> and <span
class="math inline">\(\theta=1/5\)</span> because it is
invertible.</p></li>
<li><p>As in the AR case, the polynomial, <span
class="math inline">\(\theta(z)\)</span>, corresponding to the moving
average operators, <span class="math inline">\(\theta(B)\)</span>, will
be useful in exploring general properties of MA processes.</p></li>
<li><p>For example, we can write the MA(1) model as <span
class="math inline">\(x_{t}=\theta(B)w_{t}\)</span>, where <span
class="math inline">\(\theta(B)=1+\theta B\)</span>.</p></li>
<li><p>If <span class="math inline">\(\left|\theta\right|&lt;1\)</span>,
then we can write the model as <span class="math display">\[
\pi(B)x_{t}=w_{t},
\]</span> where <span
class="math inline">\(\pi(B)=\theta^{-1}(B)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\theta(z)=1+\theta z\)</span>,
for <span class="math inline">\(\left|z\right|\leq1\)</span>, then <span
class="math display">\[
\pi(z)=\theta^{-1}(z)=\frac{1}{(1+\theta
z)}=\sum_{j=0}^{\infty}(-\theta)^{j}z^{j},
\]</span> and we determine that <span
class="math inline">\(\pi(B)=\sum_{j=0}^{\infty}(-\theta)^{j}B^{j}\)</span>.</p></li>
</ul>
</div>
</div>
<div id="autoregressive-moving-average-models" class="section level2">
<h2>Autoregressive Moving Average Models</h2>
<ul>
<li><p>A time series <span
class="math inline">\(\{x_{t};\;t=0,\pm1,\pm2,\ldots\}\)</span> is
<strong>ARMA</strong>(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) if it is stationary and <span
class="math display">\[
x_{t}=\phi_{1}x_{t-1}+\cdots+\phi_{p}x_{t-p}+w_{t}+\theta_{1}w_{t-1}+\cdots+\theta_{q}w_{t-q},
\]</span> with <span class="math inline">\(\phi_{p}\neq0\)</span>, <span
class="math inline">\(\theta_{q}\neq0\)</span>, and <span
class="math inline">\(\sigma_{w}^{2}&gt;0\)</span>. The parameters <span
class="math inline">\(p\)</span> and <span
class="math inline">\(q\)</span> are called the autoregressive and the
moving average orders, respectively. If <span
class="math inline">\(x_{t}\)</span> has a nonzero mean <span
class="math inline">\(\mu\)</span>, we set <span
class="math inline">\(\alpha=\mu(1-\phi_{1}-\cdots-\phi_{p})\)</span>
and write the model as <span class="math display">\[
x_{t}=\alpha+\phi_{1}x_{t-1}+\cdots+\phi_{p}x_{t-p}+w_{t}+\theta_{1}w_{t-1}+\cdots+\theta_{q}w_{t-q},
\]</span> where <span class="math inline">\(w_{t}\sim
wn(0,\sigma_{w}^{2})\)</span>.</p></li>
<li><p>The ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) model can then be written in concise
form as <span class="math display">\[
\phi(B)x_{t}=\theta(B)w_{t}.
\]</span></p></li>
<li><p>The concise form of the model points to a potential problem in
that we can unnecessarily complicate the model by multiplying both sides
by another operator, say <span class="math display">\[
\eta(B)\phi(B)x_{t}=\eta(B)\theta(B)w_{t},
\]</span> without changing the dynamics.</p></li>
</ul>
<div id="example-parameter-redundancy" class="section level3">
<h3>Example: Parameter Redundancy</h3>
<ul>
<li><p>Consider a white noise process <span
class="math inline">\(x_{t}=w_{t}\)</span>.</p></li>
<li><p>If we multiply both sides of the equation by <span
class="math inline">\(\eta(B)=1-.5B\)</span>, then the model becomes
<span class="math inline">\((1-.5B)x_{t}=(1-.5B)w_{t}\)</span>, or <span
class="math display">\[
x_{t}=.5x_{t-1}-.5w_{t-1}+w_{t},
\]</span> which looks like an ARMA(<span
class="math inline">\(1\)</span>, <span
class="math inline">\(1\)</span>) model.</p></li>
<li><p>Of course, <span class="math inline">\(x_{t}\)</span> is still
white noise; nothing has changed in this regard [i.e., <span
class="math inline">\(x_{t}=w_{t}\)</span> is the solution], but we have
hidden the fact that <span class="math inline">\(x_{t}\)</span> is white
noise because of the parameter redundancy or
over-parameterization.</p></li>
<li><p>The consideration of parameter redundancy will be crucial when we
discuss estimation for general ARMA models.</p></li>
<li><p>We might fit an ARMA(<span class="math inline">\(1\)</span>,
<span class="math inline">\(1\)</span>) model to white noise data and
find that the parameter estimates are significant.</p></li>
<li><p>If we were unaware of parameter redundancy, we might claim the
data are correlated when in fact they are not.</p></li>
</ul>
</div>
<div id="example" class="section level3">
<h3>Example</h3>
<ul>
<li>We generated 150 iid normals and then fit an ARMA(<span
class="math inline">\(1\)</span>, <span
class="math inline">\(1\)</span>) to the data.</li>
</ul>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8675309</span>)         <span class="co"># Jenny, I got your number</span></span>
<span id="cb64-2"><a href="#cb64-2" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">150</span>, <span class="at">mean=</span><span class="dv">5</span>)    <span class="co"># Generate iid N(5,1)s</span></span>
<span id="cb64-3"><a href="#cb64-3" tabindex="-1"></a><span class="fu">arima</span>(x, <span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>))  <span class="co"># Estimation</span></span></code></pre></div>
<pre><code>## 
## Call:
## arima(x = x, order = c(1, 0, 1))
## 
## Coefficients:
##           ar1     ma1  intercept
##       -0.9595  0.9527     5.0462
## s.e.   0.1688  0.1750     0.0727
## 
## sigma^2 estimated as 0.7986:  log likelihood = -195.98,  aic = 399.96</code></pre>
<ul>
<li><p>Note that <span class="math inline">\(\hat{\phi}=-.96\)</span>
and <span class="math inline">\(\hat{\theta}=.95\)</span>, and both are
significant.</p></li>
<li><p>Thus, forgetting the mean estimate, the fitted model looks like
<span class="math display">\[
(1+.96B)x_{t}=(1+.95B)w_{t},
\]</span> which we should recognize as an over-parametrized
model.</p></li>
</ul>
<hr />
<ul>
<li>To summarize, we have seen the following problems:
<ol style="list-style-type: lower-roman">
<li>parameter redundant models,</li>
<li>stationary AR models that depend on the future, and</li>
<li>MA models that are not unique.</li>
</ol></li>
<li>To overcome these problems, we will require some additional
restrictions on the model parameters.</li>
</ul>
<blockquote>
<p>The <strong>AR and MA polynomials</strong> are defined as <span
class="math display">\[
\phi(z)=1-\phi_{1}z-\cdots-\phi_{p}z^{p},\quad\phi_{p}\neq0,
\]</span> and <span class="math display">\[
\theta(z)=1+\theta_{1}z+\cdots+\theta_{q}z^{q},\quad\theta_{q}\neq0,
\]</span> respectively, where <span class="math inline">\(z\)</span> is
a complex number.</p>
</blockquote>
<ul>
<li><p>To address the first problem, we will henceforth refer to an
ARMA(p, q) model to mean that it is in its simplest form.</p></li>
<li><p>That is, we will require that <span
class="math inline">\(\phi(z)\)</span> and <span
class="math inline">\(\theta(z)\)</span> have no common
factors.</p></li>
<li><p>So, the process, <span
class="math inline">\(x_{t}=.5x_{t-1}-.5w_{t-1}+w_{t}\)</span>,
discussed above is not referred to as an ARMA(<span
class="math inline">\(1\)</span>, <span
class="math inline">\(1\)</span>) process because, in its reduced form,
<span class="math inline">\(x_{t}\)</span> is white noise.</p></li>
<li><p>To address the problem of future-dependent models, we formally
introduce the concept of <em>causality</em>.</p></li>
</ul>
<blockquote>
<p>An ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) model is said to be
<strong>causal</strong>, if the time series <span
class="math inline">\(\{x_{t};\;t=0,\pm1,\pm2,\ldots\}\)</span> can be
written as a one-sided linear process: <span class="math display">\[
x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}=\psi(B)w_{t},
\]</span> where <span
class="math inline">\(\psi(B)=\sum_{j=0}^{\infty}\psi_{j}B^{j}\)</span>,
and <span
class="math inline">\(\sum_{j=0}^{\infty}\left|\psi_{j}\right|&lt;\infty\)</span>;
we set <span class="math inline">\(\psi_{0}=1\)</span>.</p>
</blockquote>
<ul>
<li><p>The AR(1) process, <span class="math inline">\(x_{t}=\phi
x_{t-1}+w_{t}\)</span>, is causal only when <span
class="math inline">\(\left|\phi\right|&lt;1\)</span>.</p></li>
<li><p>Equivalently, the process is causal only when the root of <span
class="math inline">\(\phi(z)=1-\phi z\)</span> is bigger than one in
absolute value.</p></li>
<li><p>That is, the root, say, <span
class="math inline">\(z_{0}\)</span>, of <span
class="math inline">\(\phi(z)\)</span> is <span
class="math inline">\(z_{0}=1/\phi\)</span> (because <span
class="math inline">\(\phi(z_{0})=0\)</span>) and <span
class="math inline">\(\left|z_{0}\right|&gt;1\)</span> because <span
class="math inline">\(\left|\phi\right|&lt;1\)</span>.</p></li>
</ul>
</div>
<div id="causality-of-an-armap-q-process" class="section level3">
<h3>Causality of an ARMA(p, q) Process</h3>
<blockquote>
<p>An ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) model is causal if and only if <span
class="math inline">\(\phi(z)\neq0\)</span> for <span
class="math inline">\(\left|z\right|\leq1\)</span>. The coefficients of
the linear process can be determined by solving <span
class="math display">\[
\psi(z)=\sum_{j=0}^{\infty}\psi_{j}z^{j}=\frac{\theta(z)}{\phi(z)},\quad\left|z\right|\leq1.
\]</span> Another way is that an ARMA process is causal only when the
roots of <span class="math inline">\(\phi(z)\)</span> lie outside the
unit circle; that is, <span class="math inline">\(\phi(z)=0\)</span>
only when <span class="math inline">\(\left|z\right|&gt;1\)</span>.</p>
</blockquote>
<ul>
<li>Finally, to address the problem of uniqueness, we choose the model
that allows an infinite autoregressive representation.</li>
</ul>
<blockquote>
<p>An ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) model is said to be
<strong>invertible</strong>, if the time series <span
class="math inline">\(\{x_{t};\;t=0,\pm1,\pm2,\ldots\}\)</span> can be
written as <span class="math display">\[
\pi(B)x_{t}=\sum_{j=0}^{\infty}\pi_{j}x_{t-j}=w_{t},
\]</span> where <span
class="math inline">\(\pi(B)=\sum_{j=0}^{\infty}\pi_{j}B^{j}\)</span>,
and <span
class="math inline">\(\sum_{j=0}^{\infty}\left|\pi_{j}\right|&lt;\infty\)</span>;
we set <span class="math inline">\(\pi_{0}=1\)</span>.</p>
</blockquote>
</div>
<div id="invertibility-of-an-armap-q-process" class="section level3">
<h3>Invertibility of an ARMA(p, q) Process</h3>
<blockquote>
<p>An ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) model is invertible if and only if
<span class="math inline">\(\theta(z)\neq0\)</span> for <span
class="math inline">\(\left|z\right|\leq1\)</span>. The coefficients
<span class="math inline">\(\pi_{j}\)</span> of <span
class="math inline">\(\pi(B)\)</span> can be determined by solving <span
class="math display">\[
\pi(z)=\sum_{j=0}^{\infty}\pi_{j}z^{j}=\frac{\phi(z)}{\theta(z)},\quad\left|z\right|\leq1.
\]</span> Another way is that an ARMA process is invertible only when
the roots of <span class="math inline">\(\theta(z)\)</span> lie outside
the unit circle; that is, <span
class="math inline">\(\theta(z)=0\)</span> only when <span
class="math inline">\(\left|z\right|&gt;1\)</span>.</p>
</blockquote>
</div>
<div id="example-parameter-redundancy-causality-invertibility"
class="section level3">
<h3>Example: Parameter Redundancy, Causality, Invertibility</h3>
<ul>
<li><p>Consider the process <span class="math display">\[
x_{t}=.4x_{t-1}+.45x_{t-2}+w_{t}+w_{t-1}+.25w_{t-2},
\]</span> or, in operator form, <span class="math display">\[
(1-.4B-.45B^{2})x_{t}=(1+B+.25B^{2})w_{t}.
\]</span></p></li>
<li><p>At first, <span class="math inline">\(x_{t}\)</span> appears to
be an ARMA(<span class="math inline">\(2\)</span>, <span
class="math inline">\(2\)</span>) process. But notice that <span
class="math display">\[
\phi(B)=1-.4B-.45B^{2}=(1+.5B)(1-.9B)
\]</span> and <span class="math display">\[
\theta(B)=1+B+.25B^{2}=(1+.5B)^{2}
\]</span> have a common factor that can be canceled.</p></li>
<li><p>After cancellation, the operators are <span
class="math inline">\(\phi(B)=1-.9B\)</span> and <span
class="math inline">\(\theta(B)=1+.5B\)</span>, so the model is an
ARMA(<span class="math inline">\(1\)</span>, <span
class="math inline">\(1\)</span>) model, <span class="math display">\[
(1-.9B)x_{t}=(1+.5B)w_{t},
\]</span> or <span class="math display">\[
x_{t}=.9x_{t-1}+.5w_{t-1}+w_{t}.
\]</span></p></li>
<li><p>The model is causal because <span
class="math inline">\(\phi(z)=1-.9z=0\)</span> when <span
class="math inline">\(z=10/9\)</span>, which is outside the unit
circle.</p></li>
<li><p>The model is also invertible because the root of <span
class="math inline">\(\theta(z)=1+.5z\)</span> is <span
class="math inline">\(z=−2\)</span>, which is outside the unit
circle.</p></li>
<li><p>To write the model as a linear process, we can obtain the <span
class="math inline">\(\psi\)</span>-weights using <span
class="math inline">\(\phi(z)\psi(z)=\theta(z)\)</span>, or <span
class="math display">\[
(1-.9z)(1+\psi_{1}z+\psi_{2}z^{2}+\cdots+\psi_{j}z^{j}+\cdots)=1+.5z.
\]</span></p></li>
<li><p>Rearranging, we get <span class="math display">\[
1+(\psi_{1}-.9)z+(\psi_{2}-.9\psi_{1})z^{2}+\cdots+(\psi_{j}-.9\psi_{j-1})z^{j}+\cdots=1+.5z.
\]</span></p></li>
<li><p>Matching the coefficients of <span
class="math inline">\(z\)</span> on the left and right sides we get
<span class="math inline">\(\psi_{1}-.9=.5\)</span> and <span
class="math inline">\(\psi_{j}-.9\psi_{j-1}=0\)</span> for <span
class="math inline">\(j&gt;1\)</span>.</p></li>
<li><p>Thus, <span class="math inline">\(\psi_{j}=1.4(.9)^{j-1}\)</span>
for <span class="math inline">\(j\geq1\)</span> and <span
class="math display">\[
x_{t}=w_{t}+1.4\sum_{j=1}^{\infty}.9^{j-1}w_{t-j}.
\]</span></p></li>
<li><p>The values of <span class="math inline">\(\psi_{j}\)</span> may
be calculated in R as follows:</p></li>
</ul>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" tabindex="-1"></a><span class="fu">ARMAtoMA</span>(<span class="at">ar =</span> .<span class="dv">9</span>,  <span class="at">ma =</span> .<span class="dv">5</span>,  <span class="dv">10</span>)   <span class="co"># first 10 psi-weights</span></span></code></pre></div>
<pre><code>##  [1] 1.4000000 1.2600000 1.1340000 1.0206000 0.9185400 0.8266860 0.7440174
##  [8] 0.6696157 0.6026541 0.5423887</code></pre>
<ul>
<li><p>The invertible representation is obtained by matching
coefficients in <span
class="math inline">\(\theta(z)\pi(z)=\phi(z)\)</span>, <span
class="math display">\[
(1+.5z)(1+\pi_{1}z+\pi_{2}z^{2}+\pi_{3}z^{3}+\cdots)=1-.9z.
\]</span></p></li>
<li><p>In this case, the <span
class="math inline">\(\pi\)</span>-weights are given by <span
class="math inline">\(\pi_{j}=(-1)^{j}1.4(.5)^{j-1}\)</span>, for <span
class="math inline">\(j\geq1\)</span>, and hence, because <span
class="math inline">\(w_{t}=\sum_{j=0}^{\infty}\pi_{j}x_{t-j}\)</span>,
we have <span class="math display">\[
x_{t}=1.4\sum_{j=1}^{\infty}(-.5)^{j-1}x_{t-j}+w_{t}.
\]</span></p></li>
<li><p>The values of <span class="math inline">\(\pi_{j}\)</span> may be
calculated in R as follows by reversing the roles of <span
class="math inline">\(w_{t}\)</span> and <span
class="math inline">\(x_{t}\)</span>; i.e., write the model as <span
class="math inline">\(w_{t}=-.5w_{t-1}+x_{t}-.9x_{t-1}\)</span>:</p></li>
</ul>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" tabindex="-1"></a><span class="fu">ARMAtoMA</span>(<span class="at">ar =</span> <span class="sc">-</span>.<span class="dv">5</span>, <span class="at">ma =</span> <span class="sc">-</span>.<span class="dv">9</span>, <span class="dv">10</span>)   <span class="co"># first 10 pi-weights</span></span></code></pre></div>
<pre><code>##  [1] -1.400000000  0.700000000 -0.350000000  0.175000000 -0.087500000
##  [6]  0.043750000 -0.021875000  0.010937500 -0.005468750  0.002734375</code></pre>
</div>
<div id="example-causal-condition-for-an-ar1-process"
class="section level3">
<h3>Example: Causal Condition for an AR(1) Process</h3>
<ul>
<li><p>For an AR(<span class="math inline">\(1\)</span>) model, <span
class="math display">\[
(1-\phi B)x_{t}=w_{t},
\]</span> to be causal, the root of <span
class="math inline">\(\phi(z)=1-\phi z\)</span> must lie outside of the
unit circle.</p></li>
<li><p>In this case, <span class="math inline">\(\phi(z)=0\)</span> when
<span class="math inline">\(z=1/\phi\)</span>, so it is easy to go from
the causal requirement on the root, <span
class="math inline">\(\left|1/\phi\right|&gt;1\)</span>, to a
requirement on the parameter, <span
class="math inline">\(\left|\phi\right|&lt;1\)</span>.</p></li>
<li><p>It is not so easy to establish this relationship for higher order
models.</p></li>
</ul>
</div>
<div id="example-causal-conditions-for-an-ar2-process"
class="section level3">
<h3>Example: Causal Conditions for an AR(2) Process</h3>
<ul>
<li><p>The AR(<span class="math inline">\(2\)</span>) model, <span
class="math display">\[
(1-\phi_{1}B-\phi_{2}B^{2})x_{t}=w_{t},
\]</span> is causal when the two roots of <span
class="math inline">\(\phi(z)=1-\phi_{1}z-\phi_{2}z^{2}\)</span> lie
outside of the unit circle.</p></li>
<li><p>Using the quadratic formula (<span
class="math inline">\(ax^{2}+bx+c=0,\:x=(-b\pm\sqrt{b^{2}-4ac})/2a\)</span>),
this requirement can be written as <span class="math display">\[
\left|\frac{\phi_{1}\pm\sqrt{\phi_{1}^{2}+4\phi_{2}}}{-2\phi_{2}}\right|&gt;1.
\]</span></p></li>
<li><p>The roots of <span class="math inline">\(\phi(z)\)</span> may be
real and distinct, real and equal, or a complex conjugate pair.</p></li>
<li><p>If we denote those roots by <span
class="math inline">\(z_{1}\)</span> and <span
class="math inline">\(z_{2}\)</span>, we can write <span
class="math display">\[\phi(z)=(1-z_{1}^{-1}z)(1-z_{2}^{-1}z);\]</span>
note that <span
class="math inline">\(\phi(z_{1})=\phi(z_{2})=0\)</span>.</p></li>
<li><p>The model can be written in operator form as <span
class="math display">\[
(1-z_{1}^{-1}B)(1-z_{2}^{-1}B)x_{t}=w_{t}.
\]</span></p></li>
<li><p>From this representation, it follows that <span
class="math display">\[
\phi_{1}=(z_{1}^{-1}+z_{2}^{-1})\quad\textrm{and}\quad\phi_{2}=-(z_{1}z_{2})^{-1}.
\]</span></p></li>
<li><p>This relationship and the fact that <span
class="math inline">\(\left|z_{1}\right|&gt;1\)</span> and <span
class="math inline">\(\left|z_{2}\right|&gt;1\)</span> can be used to
establish the following equivalent condition for causality: <span
class="math display">\[
\phi_{1}+\phi_{2}&lt;1,\quad\phi_{2}-\phi_{1}&lt;1,\quad\textrm{and}\quad\left|\phi_{2}\right|&lt;1.
\]</span></p></li>
</ul>
</div>
<div id="homework-3" class="section level3">
<h3>Homework</h3>
<ul>
<li>Read Section 3.2 Difference Equations.</li>
</ul>
</div>
</div>
<div id="autocorrelation-and-partial-autocorrelation"
class="section level2">
<h2>Autocorrelation and Partial Autocorrelation</h2>
<div id="maq-process" class="section level3">
<h3>MA(<span class="math inline">\(q\)</span>) Process</h3>
<ul>
<li><p>Consider <span class="math display">\[
x_{t}=\theta(B)w_{t},
\]</span> where <span
class="math inline">\(\theta(B)=1+\theta_{1}B+\cdots+\theta_{q}B^{q}\)</span>.</p></li>
<li><p>This process is stationary with mean <span
class="math display">\[
\text{E}(x_{t})=\sum_{j=0}^{q}\theta_{j}\textrm{E}(w_{t-j})=0,
\]</span> where we have written <span
class="math inline">\(\theta_{0}=1\)</span>, and with autocovariance
function <span class="math display">\[
\begin{align*}
\gamma(h) &amp;
=\textrm{cov}(x_{t+h},x_{t})=\textrm{cov}(\sum_{j=0}^{q}\theta_{j}w_{t+h-j},\sum_{k=0}^{q}\theta_{k}w_{t-k})\\
&amp; =\begin{cases}
\sigma_{w}^{2}\sum_{j=0}^{q-h}\theta_{j}\theta_{j+h} &amp; 0\leq h\leq
q,\\
0 &amp; h&gt;q.
\end{cases}
\end{align*}
\]</span></p></li>
<li><p>Note that <span class="math inline">\(\gamma(q)\)</span> cannot
be zero because <span
class="math inline">\(\theta_{q}\neq0\)</span>.</p></li>
<li><p>The cutting off of <span class="math inline">\(\gamma(h)\)</span>
after <span class="math inline">\(q\)</span> lags is the signature of
the MA(<span class="math inline">\(q\)</span>) model.</p></li>
<li><p>The ACF of a MA(<span class="math inline">\(q\)</span>) is <span
class="math display">\[
\rho(h)=\begin{cases}
\frac{\sum_{j=0}^{q-h}\theta_{j}\theta_{j+h}}{1+\theta_{1}^{2}+\cdots+\theta_{q}^{2}}
&amp; 1\leq h\leq q,\\
0 &amp; h&gt;q.
\end{cases}
\]</span></p></li>
</ul>
</div>
<div id="causal-armap-q-model" class="section level3">
<h3>Causal ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>) Model</h3>
<ul>
<li><p>Consider <span class="math display">\[
\phi(B)x_{t}=\theta(B)w_{t},
\]</span> where the zeros of <span
class="math inline">\(\phi(z)\)</span> are outside the unit
circle.</p></li>
<li><p>Write (How to find the <span
class="math inline">\(\psi_{j}\)</span>’s? Do Example 3.12) <span
class="math display">\[
x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}.
\]</span></p></li>
<li><p>We have <span class="math inline">\(\textrm{E}(x_{t})=0\)</span>
and the autocovariance function of <span
class="math inline">\(x_{t}\)</span> is <span class="math display">\[
\gamma(h)=\textrm{cov}(x_{t+h},x_{t})=\sigma_{w}^{2}\sum_{j=0}^{\infty}\psi_{j}\psi_{j+h},\quad
h\geq0.
\]</span></p></li>
<li><p>In turn, we could solve for <span
class="math inline">\(\gamma(h)\)</span>, and the ACF <span
class="math inline">\(\rho(h)=\gamma(h)/\gamma(0)\)</span>.</p></li>
</ul>
</div>
<div id="example-the-acf-of-an-arma1-1" class="section level3">
<h3>Example: The ACF of an ARMA(<span class="math inline">\(1\)</span>,
<span class="math inline">\(1\)</span>)</h3>
<ul>
<li><p>Consider the ARMA(<span class="math inline">\(1\)</span>, <span
class="math inline">\(1\)</span>) process <span class="math display">\[
x_{t}=\phi x_{t-1}+\theta
w_{t-1}+w_{t},\quad\textrm{where}\;\left|\phi\right|&lt;1.
\]</span></p></li>
<li><p>We have <span class="math display">\[
\gamma(0)=\sigma_{w}^{2}\frac{1+2\theta\phi+\theta^{2}}{1-\phi^{2}}\quad\textrm{and}\quad\gamma(1)=\sigma_{w}^{2}\frac{(1+\theta\phi)(\phi+\theta)}{1-\phi^{2}}.
\]</span></p></li>
<li><p>Also <span class="math display">\[
\gamma(h)=\frac{\gamma(1)}{\phi}\phi^{h}=\sigma_{w}^{2}\frac{(1+\theta\phi)(\phi+\theta)}{1-\phi^{2}}\phi^{h-1},\quad
h\geq1.
\]</span></p></li>
<li><p>Finally, dividing through by <span
class="math inline">\(\gamma(0)\)</span> yields the ACF <span
class="math display">\[
\rho(h)=\frac{(1+\theta\phi)(\phi+\theta)}{1+2\theta\phi+\theta^{2}}\phi^{h-1},\quad
h\geq1.
\]</span></p></li>
</ul>
</div>
<div id="the-partial-autocorrelation-function-pacf"
class="section level3">
<h3>The Partial Autocorrelation Function (PACF)</h3>
<ul>
<li><p>For MA(<span class="math inline">\(q\)</span>) models, the ACF
will be zero for lags greater than <span
class="math inline">\(q\)</span>.</p></li>
<li><p>Moreover, because <span
class="math inline">\(\theta_{q}\neq0\)</span>, the ACF will not be zero
at lag <span class="math inline">\(q\)</span>.</p></li>
<li><p>Thus, the ACF provides a considerable amount of information about
the order of the dependence when the process is a moving average
process.</p></li>
<li><p>If the process, however, is ARMA or AR, the ACF alone tells us
little about the orders of dependence.</p></li>
<li><p>Hence, it is worthwhile pursuing a function that will behave like
the ACF of MA models, but for AR models, namely, the partial
autocorrelation function (PACF).</p></li>
<li><p>Recall that if <span class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span>, and <span
class="math inline">\(Z\)</span> are random variables, then the partial
correlation between <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(Z\)</span> is obtained by regressing <span
class="math inline">\(X\)</span> on <span
class="math inline">\(Z\)</span> to obtain <span
class="math inline">\(\hat{X}\)</span>, regressing <span
class="math inline">\(Y\)</span> on <span
class="math inline">\(Z\)</span> to obtain <span
class="math inline">\(\hat{Y}\)</span>, and then calculating <span
class="math display">\[
\rho_{XY\mid Z}=\textrm{corr}\{X-\hat{X},Y-\hat{Y}\}.
\]</span></p></li>
<li><p>The idea is that <span class="math inline">\(\rho_{XY\mid
Z}\)</span> measures the correlation between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> with the linear effect of <span
class="math inline">\(Z\)</span> removed (or partialled out).</p></li>
<li><p>If the variables are multivariate normal, then <span
class="math display">\[
\rho_{XY\mid Z}=\textrm{corr}(X,Y\mid Z).
\]</span></p></li>
<li><p>Consider a causal AR(<span class="math inline">\(1\)</span>)
model <span class="math inline">\(x_{t}=\phi x_{t-1}+w_{t}\)</span>.
Then (by causality), <span class="math display">\[
\begin{align*}
\gamma_{x}(2) &amp; =\textrm{cov}(x_{t},x_{t-2})=\textrm{cov}(\phi
x_{t-1}+w_{t},x_{t-2})\\
&amp; =\textrm{cov}(\phi^{2}x_{t-2}+\phi
w_{t-1}+w_{t},x_{t-2})=\phi^{2}\gamma_{x}(0).
\end{align*}
\]</span></p></li>
<li><p>The correlation between <span
class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(x_{t-2}\)</span> is not zero, as it would be for
an MA(<span class="math inline">\(1\)</span>), because <span
class="math inline">\(x_{t}\)</span> is dependent on <span
class="math inline">\(x_{t-2}\)</span> through <span
class="math inline">\(x_{t-1}\)</span>.</p></li>
<li><p>Consider the correlation between <span
class="math inline">\(x_{t}-\phi x_{t-1}\)</span> and <span
class="math inline">\(x_{t-2}-\phi x_{x-1}\)</span>. That is the
correlation between <span class="math inline">\(x_{t}\)</span> and <span
class="math inline">\(x_{t-2}\)</span> with the linear dependence of
each on <span class="math inline">\(x_{t-1}\)</span> removed.</p></li>
<li><p>We have <span class="math display">\[
\textrm{cov}(x_{t}-\phi x_{t-1},x_{t-2}-\phi
x_{t-1})=\textrm{cov}(w_{t},x_{t-2}-\phi x_{t-1})=0.
\]</span></p></li>
<li><p>The tool we need is partial autocorrelation, which is the
correlation between <span class="math inline">\(x_{s}\)</span> and <span
class="math inline">\(x_{t}\)</span> with the linear effect of
everything “in the middle” removed.</p></li>
<li><p>To formally define the PACF for mean-zero stationary time series,
let <span class="math inline">\(\hat{x}_{t+h}\)</span> , for <span
class="math inline">\(h\geq2\)</span>, denote the regression of <span
class="math inline">\(x_{t+h}\)</span> on <span
class="math inline">\(\{x_{t+h-1},x_{t+h-2},\ldots,x_{t+1}\}\)</span>,
which we write as <span class="math display">\[
\hat{x}_{t+h}=\beta_{1}x_{t+h-1}+\beta_{2}x_{t+h-2}+\cdots+\beta_{h-1}x_{t+1}.
\]</span></p></li>
<li><p>No intercept term is needed in because the mean of <span
class="math inline">\(x_{t}\)</span> is zero (otherwise, replace <span
class="math inline">\(x_{t}\)</span> by <span
class="math inline">\(x_{t}-\mu_{x}\)</span>).</p></li>
<li><p>In addition, let <span class="math inline">\(\hat{x}_{t}\)</span>
denote the regression of <span class="math inline">\(x_{t}\)</span> on
<span
class="math inline">\(\{x_{t+1},x_{t+2},\ldots,x_{t+h-1}\}\)</span>,
then <span class="math display">\[
\hat{x}_{t}=\beta_{1}x_{t+1}+\beta_{2}x_{t+2}+\cdots+\beta_{h-1}x_{t+h-1}.
\]</span></p></li>
</ul>
</div>
<div id="partial-autocorrelation-function" class="section level3">
<h3>Partial Autocorrelation Function</h3>
<blockquote>
<p>The <strong>partial autocorrelation function (PACF)</strong> of a
stationary process, <span class="math inline">\(x_{t}\)</span>, denoted
<span class="math inline">\(\phi_{hh}\)</span>, for <span
class="math inline">\(h=1,2,\ldots\)</span>, is <span
class="math display">\[
\phi_{11}=\textrm{corr}(x_{t+1},x_{t})=\rho(1)
\]</span> and <span class="math display">\[
\phi_{hh}=\textrm{corr}(x_{t+h}-\hat{x}_{t+h},x_{t}-\hat{x}_{t}),\quad
h\geq2.
\]</span></p>
</blockquote>
<ul>
<li><p>The PACF, <span class="math inline">\(\phi_{hh}\)</span>, is the
correlation between <span class="math inline">\(x_{t+h}\)</span> and
<span class="math inline">\(x_{t}\)</span> with the linear dependence of
<span class="math inline">\(\{x_{t+1},\ldots,x_{t+h-1}\}\)</span> on
each, removed.</p></li>
<li><p>If the process <span class="math inline">\(x_{t}\)</span> is
Gaussian, then <span class="math display">\[
\phi_{hh}=\textrm{corr}(x_{t+h},x_{t}\mid x_{t+1},\ldots,x_{t+h-1});
\]</span> that is, <span class="math inline">\(\phi_{hh}\)</span> is the
correlation coefficient between <span
class="math inline">\(x_{t+h}\)</span> and <span
class="math inline">\(x_{t}\)</span> in the bivariate distribution of
<span class="math inline">\((x_{t+h},x_{t})\)</span> conditional on
<span
class="math inline">\(\{x_{t+1},\ldots,x_{t+h-1}\}\)</span>.</p></li>
</ul>
</div>
<div id="example-the-pacf-of-an-ar1" class="section level3">
<h3>Example: The PACF of an AR(1)</h3>
<ul>
<li><p>Consider the PACF of the AR(<span
class="math inline">\(1\)</span>) process given by <span
class="math inline">\(x_{t}=\phi x_{t-1}+w_{t}\)</span>, with <span
class="math inline">\(\left|\phi\right|&lt;1\)</span>.</p></li>
<li><p>By definition, <span
class="math inline">\(\phi_{11}=\rho(1)=\phi\)</span>.</p></li>
<li><p>To calculate <span class="math inline">\(\phi_{22}\)</span>,
consider the regression of <span class="math inline">\(x_{t+2}\)</span>
on x_{t+1}, say, <span class="math inline">\(\hat{x}_{t+2}=\beta
x_{t+1}\)</span>. We choose <span class="math inline">\(\beta\)</span>
to minimize <span class="math display">\[
\textrm{E}(x_{t+2}-\hat{x}_{t+2})^{2}=\textrm{E}(x_{t+2}-\beta
x_{t+1})^{2}=\gamma(0)-2\beta\gamma(1)+\beta^{2}\gamma(0).
\]</span></p></li>
<li><p>Taking derivatives with respect to <span
class="math inline">\(\beta\)</span> and setting the result equal to
zero, we have <span class="math display">\[
\beta=\gamma(1)/\gamma(0)=\rho(1)=\phi.
\]</span></p></li>
<li><p>Next, consider the regression of <span
class="math inline">\(x_{t}\)</span> on <span
class="math inline">\(x_{t+1}\)</span>, say <span
class="math inline">\(\hat{x}_{t}=\beta x_{t+1}\)</span>. We choose
<span class="math inline">\(\beta\)</span> to minimize <span
class="math display">\[
\textrm{E}(x_{t}-\hat{x}_{t})^{2}=\textrm{E}(x_{t}-\beta
x_{t+1})^{2}=\gamma(0)-2\beta\gamma(1)+\beta^{2}\gamma(0).
\]</span> This is the same equation as before, so <span
class="math inline">\(\beta=\phi\)</span>.</p></li>
<li><p>Hence, <span class="math display">\[
\begin{align*}
\phi_{22} &amp;
=\textrm{corr}(x_{t+2}-\hat{x}_{t+2},x_{t}-\hat{x}_{t})=\textrm{corr}(x_{t+2}-\phi
x_{t+1},x_{t}-\phi x_{t+1})\\
&amp; =\textrm{corr}(w_{t+2},x_{t}-\phi x_{t+1})=0
\end{align*}
\]</span> by causality. Thus, <span
class="math inline">\(\phi_{22}=0\)</span>. In fact, in this case, <span
class="math inline">\(\phi_{hh}=0\)</span> for all <span
class="math inline">\(h&gt;1\)</span>.</p></li>
</ul>
</div>
<div id="example-the-pacf-of-an-arp" class="section level3">
<h3>Example: The PACF of an AR(<span
class="math inline">\(p\)</span>)</h3>
<ul>
<li><p>The model implies <span
class="math display">\[x_{t+h}=\sum_{j=1}^{p}\phi_{j}x_{t+h-j}+w_{t+h},\]</span>
where the roots of <span class="math inline">\(\phi(z)\)</span> are
outside the unit circle.</p></li>
<li><p>When <span class="math inline">\(h&gt;p\)</span>, the regression
of <span class="math inline">\(x_{t+h}\)</span> on <span
class="math inline">\(\{x_{t+1},\ldots,x_{t+h-1}\}\)</span>, is (why?)
<span class="math display">\[
\hat{x}_{t+h}=\sum_{j=1}^{p}\phi_{j}x_{t+h-j}.
\]</span></p></li>
<li><p>Thus, when <span class="math inline">\(h&gt;p\)</span>, <span
class="math display">\[
\phi_{hh}=\textrm{corr}(x_{t+h}-\hat{x}_{t+h},x_{t}-\hat{x}_{t})=\textrm{corr}(w_{t+h},x_{t}-\hat{x}_{t})=0,
\]</span> because causality.</p></li>
<li><p>When <span class="math inline">\(h\leq p\)</span>, <span
class="math inline">\(\phi_{pp}\)</span> is not zero, and <span
class="math inline">\(\phi_{11},\ldots,\phi_{p-1,p-1}\)</span> are not
necessarily zero.</p></li>
<li><p>Consider <span class="math inline">\(n=144\)</span> observations
from the AR(2) model <span class="math display">\[
x_{t}=1.5x_{t-1}-.75x_{t-2}+w_{t},
\]</span> with <span
class="math inline">\(\sigma_{w}^{2}=1\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" tabindex="-1"></a>ar2.acf <span class="ot">=</span> <span class="fu">ARMAacf</span>(<span class="at">ar=</span><span class="fu">c</span>(<span class="fl">1.5</span>,<span class="sc">-</span>.<span class="dv">75</span>), <span class="at">ma=</span><span class="dv">0</span>, <span class="dv">24</span>)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb70-2"><a href="#cb70-2" tabindex="-1"></a>ar2.pacf <span class="ot">=</span> <span class="fu">ARMAacf</span>(<span class="at">ar=</span><span class="fu">c</span>(<span class="fl">1.5</span>,<span class="sc">-</span>.<span class="dv">75</span>), <span class="at">ma=</span><span class="dv">0</span>, <span class="dv">24</span>, <span class="at">pacf=</span><span class="cn">TRUE</span>)</span>
<span id="cb70-3"><a href="#cb70-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb70-4"><a href="#cb70-4" tabindex="-1"></a><span class="fu">plot</span>(ar2.acf, <span class="at">type=</span><span class="st">&quot;h&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;lag&quot;</span>)</span>
<span id="cb70-5"><a href="#cb70-5" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>)</span>
<span id="cb70-6"><a href="#cb70-6" tabindex="-1"></a><span class="fu">plot</span>(ar2.pacf, <span class="at">type=</span><span class="st">&quot;h&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;lag&quot;</span>)</span>
<span id="cb70-7"><a href="#cb70-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>)</span></code></pre></div>
<p><img src="Slides_files/figure-html/example3-16-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="example-the-pacf-of-an-invertible-maq" class="section level3">
<h3>Example: The PACF of an Invertible MA(q)</h3>
<ul>
<li><p>For an invertible MA(<span class="math inline">\(q\)</span>), we
can write <span class="math display">\[
x_{t}=-\sum_{j=1}^{\infty}\pi_{j}x_{t-j}+w_{t}.
\]</span></p></li>
<li><p>Moreover, no finite representation exists. Thus, the PACF will
never cut off, as in the case of an AR(<span
class="math inline">\(p\)</span>).</p></li>
<li><p>For an MA(<span class="math inline">\(1\)</span>), <span
class="math inline">\(x_{t}==w_{t}+\theta w_{t-1}\)</span>, with <span
class="math inline">\(\left|\theta\right|&lt;1\)</span>, we have <span
class="math display">\[
\phi_{22}=\frac{-\theta^{2}}{1+\theta^{2}+\theta^{4}}.
\]</span></p></li>
<li><p>In general, <span class="math display">\[
\phi_{hh}=-\frac{(-\theta)^{h}(1-\theta^{2})}{1-\theta^{2(h+1)}},\quad
h\geq1.
\]</span></p></li>
<li><p>Behavior of the ACF and PACF for ARMA Models</p></li>
</ul>
<table>
<colgroup>
<col width="8%" />
<col width="34%" />
<col width="34%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>AR(<span class="math inline">\(p\)</span>)</th>
<th>MA(<span class="math inline">\(q\)</span>)</th>
<th>ARMA(<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ACF</td>
<td>Tails off</td>
<td>Cuts off after lag <span class="math inline">\(q\)</span></td>
<td>Tails off</td>
</tr>
<tr class="even">
<td>PACF</td>
<td>Cuts off after lag <span class="math inline">\(p\)</span></td>
<td>Tails off</td>
<td>Tails off</td>
</tr>
</tbody>
</table>
</div>
<div id="example-preliminary-analysis-of-the-recruitment-series"
class="section level3">
<h3>Example: Preliminary Analysis of the Recruitment Series</h3>
<ul>
<li>We consider the problem of modeling the Recruitment series.</li>
</ul>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" tabindex="-1"></a><span class="fu">tsplot</span>(rec, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">main=</span><span class="st">&quot;Recruitment&quot;</span>) </span></code></pre></div>
<p><img src="Slides_files/figure-html/example1-5-recruitmentPlot-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li>There are 453 months of observed recruitment ranging over the years
1950-1987.</li>
</ul>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" tabindex="-1"></a><span class="fu">acf2</span>(rec, <span class="dv">48</span>)     <span class="co"># will produce values and a graphic </span></span></code></pre></div>
<p><img src="Slides_files/figure-html/example3-18-acf-1.png" width="768" style="display: block; margin: auto;" /></p>
<ul>
<li><p>We use the data triplets <span class="math display">\[
\{(x;z_{1},z_{2}):(x_{3};x_{2},x_{1}),(x_{4};x_{3},x_{2}),\ldots,(x_{453};x_{452},x_{451})\}
\]</span> to fit a model of the form <span class="math display">\[
x_{t}=\phi_{0}+\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+w_{t}
\]</span> for <span
class="math inline">\(t=3,\ldots,453\)</span>.</p></li>
<li><p>The estimates and standard errors (in parentheses) are <span
class="math display">\[
\hat{\phi}_{0}=6.74_{(1.11)},
\hat{\phi}_{1}=1.35_{.04},\hat{\phi}_{2}=-.46_{.04}\quad\textrm{and}\;\hat{\sigma}_{w}^{2}=89.72.
\]</span></p></li>
</ul>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" tabindex="-1"></a>(<span class="at">regr =</span> <span class="fu">ar.ols</span>(rec, <span class="at">order=</span><span class="dv">2</span>, <span class="at">demean=</span>F, <span class="at">intercept=</span><span class="cn">TRUE</span>))  <span class="co"># regression</span></span></code></pre></div>
<pre><code>## 
## Call:
## ar.ols(x = rec, order.max = 2, demean = F, intercept = TRUE)
## 
## Coefficients:
##       1        2  
##  1.3541  -0.4632  
## 
## Intercept: 6.737 (1.111) 
## 
## Order selected 2  sigma^2 estimated as  89.72</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" tabindex="-1"></a>regr<span class="sc">$</span>asy.se.coef  <span class="co"># standard errors</span></span></code></pre></div>
<pre><code>## $x.mean
## [1] 1.110599
## 
## $ar
## [1] 0.04178901 0.04187942</code></pre>
</div>
</div>
<div id="forecasting" class="section level2">
<h2>Forecasting</h2>
<ul>
<li><p>The goal is to predict future values of a time series, <span
class="math inline">\(x_{n+m}\)</span>, <span
class="math inline">\(m=1,2,\ldots\)</span>, based on the data collected
to the present, <span
class="math inline">\(x_{1:n}=\{x_{1},x_{2},\ldots,x_{n}\}\)</span>.</p></li>
<li><p>We assume <span class="math inline">\(x_{t}\)</span> stationary
and the model parameters known.</p></li>
<li><p>The minimum mean square error predictor of <span
class="math inline">\(x_{n+m}\)</span> is <span class="math display">\[
x_{n+m}^{n}=\textrm{E}(x_{n+m}\mid x_{1:n})
\]</span> because the conditional expectation minimizes the mean square
error <span class="math display">\[
\textrm{E}[(x_{n+m}-g(x_{1:n}))^{2}],
\]</span> where <span class="math inline">\(g(x_{1:n})\)</span> is a
function of the observations <span
class="math inline">\(x_{1:n}\)</span>.</p></li>
<li><p>We consider predictors that are linear functions of the data,
that is, predictors of the form <span class="math display">\[
x_{n+m}^{n}=\alpha_{0}+\sum_{k=1}^{n}\alpha_{k}x_{k},
\]</span> where <span
class="math inline">\(\alpha_{0},\alpha_{1},\ldots,\alpha_{n}\)</span>
are real numbers.</p></li>
<li><p>Note that the <span class="math inline">\(\alpha\)</span>’s
depend on <span class="math inline">\(n\)</span> and <span
class="math inline">\(m\)</span>, but for now we drop the dependence
from the notation.</p></li>
<li><p>For example</p>
<ul>
<li>if <span class="math inline">\(n=m=1\)</span>, then <span
class="math inline">\(x_{2}^{1}\)</span> is the one-step-ahead linear
forecast of <span class="math inline">\(x_{2}\)</span> given <span
class="math inline">\(x_{1}\)</span>. We have, <span
class="math display">\[
x_{2}^{1}=\alpha_{0}+\alpha_{1}x_{1}.
\]</span></li>
<li>But, if <span class="math inline">\(n=2\)</span>, <span
class="math inline">\(x_{3}^{2}\)</span> is the one-step-ahead linear
forecast of <span class="math inline">\(x_{3}\)</span> given <span
class="math inline">\(x_{1}\)</span> and <span
class="math inline">\(x_{2}\)</span>. We have, <span
class="math display">\[
x_{3}^{2}=\alpha_{0}+\alpha_{1}x_{1}+\alpha_{2}x_{2},
\]</span> and in general, the <span
class="math inline">\(\alpha\)</span>’s in <span
class="math inline">\(x_{2}^{1}\)</span> and <span
class="math inline">\(x_{3}^{2}\)</span> will be different.</li>
</ul></li>
</ul>
<div id="review-1" class="section level3">
<h3>Review</h3>
<ul>
<li>Appendix B Time Domain Theory.</li>
</ul>
</div>
</div>
</div>
</div>

   
   
            
      

  <script>
    $(document).ready(function () {

			
 		
	    });
  </script>



    <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
	var script = document.createElement("script");
	script.type = "text/javascript";
	script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
	document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
  
</body>
</html>
